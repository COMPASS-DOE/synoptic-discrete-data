---
title: "GHG processor"
author: "Bond-Lamberty / Wilson"
title-block-banner: true
params:
  html_outfile: "ghg-processor.html"
  data_folder: "Raw Data/"
  output_folder: "Processed Data/"
  metadata_folder: "Raw Data/"
  MSMT_LENGTH_SEC: 120
  DEAD_BAND_SEC: 10
  SAVE_RESULTS_FIGURES: true
date: now
date-format: "YYYY-MM-DD HH:mm:ssZ"
format: 
  html:
    toc: true
    code-fold: true
editor: visual
---

# Initializing

```{r setup}
#| include: false

library(rmarkdown)
library(readr)
library(lubridate)
library(ggplot2)
theme_set(theme_bw())
library(ggrepel)
library(readxl)
library(broom)
library(DT)
library(MASS)
library(tidyr)
library(dplyr)
library(fluxfinder)

# These columns are REQUIRED to be present in the metadata file(s)
# (NB there are can be other columns present; they will be carried through to the results)
METADATA_REQUIRED_FIELDS <- c("Date", "Time_start", "Plot", "Area", "Volume", "Temp")

# Data files must end with ".txt" and may be in sub-folders
data_files <- list.files(params$data_folder, 
                         pattern = "*txt$", 
                         full.names = TRUE, recursive = TRUE)
# Metadata files must end with ".xlsx" and may be in sub-folders
metadata_files <- list.files(params$metadata_folder, 
                             pattern = "*xlsx$", 
                             full.names = TRUE, recursive = TRUE)

#SW made the metadata file a csv to be visable on Github and there is only one 
#metadata_files <- "COMPASS_Synoptic_TreeFlux_Metadata_2023.csv"
```

I see `r length(data_files)` data files to process.

I see `r length(metadata_files)` metadata files to process.

Measurement length is `r params$MSMT_LENGTH_SEC` seconds.

Default dead band time is `r params$DEAD_BAND_SEC` seconds.

Required metafields are: `r paste(METADATA_REQUIRED_FIELDS, collapse = ", ")`.

Writing output to `r normalizePath(params$output_folder)`.

HTML outfile is `r params$html_outfile`.

Working directory is `r getwd()`.

# Data

## Read in GHG data

```{r read-data}
errors <- 0 # error count

# Function to read a data file given by 'fn', filename
# Returns NULL if there's an error reading
read_ghg_data <- function(fn) {
  basefn <- basename(fn)
  message(Sys.time(), " Processing ", basefn)
  
  dat_raw <- readLines(fn)
  # Save the serial number in case we want to look at machine differences later
  sn <- trimws(gsub("SN:\t", "", dat_raw[2], fixed = TRUE))
  # Parse the timezone from the header and use it to make a TIMESTAMP field
  tz <- trimws(gsub("Timezone:\t", "", dat_raw[5], fixed = TRUE))
  
  # These files have five header lines, then the names of the columns in line 6,
  # and then the column units in line 7. We only want the names
  dat_raw <- dat_raw[-c(1:5, 7)]
  # Irritatingly, the units line can repeat in the file (???!?). Remove these
  dat_raw <- dat_raw[grep("DATAU", dat_raw, invert = TRUE)]
  # Double irritatingly, if there's no remark, the software write \t\t, not
  # \t""\t, causing a read error. Replace these instances
  dat_raw <- gsub("\t\t", "\tnan\t", dat_raw, fixed = TRUE)
  dat <- try({
    readr::read_table(I(dat_raw), na = "nan", guess_max = 1e4)
  })
  
  # If the try() above succeeded, we haev a data frame and can process it 
  if(is.data.frame(dat)) {
    message("\tRead in ", nrow(dat), " rows of data, ", 
            min(dat$DATE), " to ", max(dat$DATE))
    message("\tInstrument serial number: ", sn)
    dat$SN <- sn
    message("\tInstrument time zone: ", tz)
    # We record the time zone but don't convert the timestamps to it as
    # that's not needed right now; metadata time is guaranteed to be same
    dat$TIMESTAMP <- lubridate::ymd_hms(paste(dat$DATE, dat$TIME))
    dat$TZ <- tz
    # Remove unneeded Licor DATE and TIME columns
    dat$DATE <- dat$TIME <- NULL
    dat$Data_file <- basefn
    return(dat)
  } else {
    warning("File read error for ", basefn)
    errors <<- errors + 1
    return(NULL)
  }
}

dat <- lapply(data_files, read_ghg_data)
dat <- do.call("rbind", dat)
```

Errors: `r errors`

```{r}
#| echo: false
#| output: asis

if(errors) cat("**<span style='color:red'>---> At least one error occurred! <---</span>**")
```

Total observations: `r format(nrow(dat), big.mark = ",")`

## Read in metadata

```{r read-metadata}
errors <- 0

# Function to read a metadata file given by 'fn', filename
# Returns NULL if there's an error reading
read_metadata <- function(fn) {
  basefn <- basename(fn)
  message(Sys.time(), " Processing ", basefn)
  
  metadat_raw <- try({
    read_excel(fn)
  })
  
  if(is.data.frame(metadat_raw)) {
    if(!all(METADATA_REQUIRED_FIELDS %in% names(metadat_raw))) {
      warning("Metadata file ", basefn, " doesn't have required fields!")
      return(NULL)
    }

    metadat_raw$Metadata_file <- basefn
    # Note that it's guaranteed that metadata time is instrument time,
    # so we don't worry about any time zones right now
    metadat <- metadat_raw
    
    # Metadata files can, optionally, have "Dead_band" and "Msmt_length" columns
    # If not present, insert those columns with NA values
    if(!"Dead_band" %in% names(metadat)) {
      metadat$Dead_band <- NA_real_
    } else {
      message("\tCustom Dead_band column exists")
    }
    if(!"Msmt_stop" %in% names(metadat)) {
      metadat$Msmt_stop <- NA_real_
    } else {
      message("\tCustom Msmt_stop column exists")
    }
    
  } else {
    warning("File read error for ", basefn)
    errors <<- errors + 1
    return(NULL)
  }

  metadat
}

metadat <- lapply(metadata_files, read_metadata)
metadat <- do.call("rbind", metadat)
if(!nrow(metadat)) {stop("No metadata read!")}

metadat$Obs <- seq_len(nrow(metadat))



```

Errors: `r errors`

Total metadata rows: `r nrow(metadat)`

## Metadata-data matching

```{r join-data}

# For each row of metadata, find corresponding observational data
zero_matches <- data.frame()
match_info <- data.frame()

matched_dat <- list()
for(i in seq_len(nrow(metadat))) {
  ts <- metadat$Time_start[i]
  start_time <- metadat$Date[i] + hour(ts) * 60 * 60 + minute(ts) * 60 + second(ts)
  end_time <- start_time + params$MSMT_LENGTH_SEC

  # Subset data following timestamp in metadata and store
  x <- subset(dat, TIMESTAMP >= start_time & TIMESTAMP < end_time)
  
#  message("Metadata row ", i, " ", metadat$Plot[i], " start = ", start_time, " matched ", nrow(x), " data rows")
  info <- data.frame(Row = i,
                     Plot = metadat$Plot[i],
                     start_time = start_time,
                     Rows_matched = nrow(x),
                     Min_timestamp = NA,
                     Max_timestamp = NA)
  if(nrow(x)) {
    # We have matched data!
    x$Obs <- i
    
    # Assign dead band: value given in Quarto params, but can be overridden in metadata    
    if(is.na(metadat$Dead_band[i])) {
      x$DEAD_BAND_SEC <- params$DEAD_BAND_SEC
    } else {
      x$DEAD_BAND_SEC <- metadat$Dead_band[i]
    }
    # Assign stop time (in seconds, not including dead band): value given in 
    # Quarto params, but can be overridden in metadata    
    if(is.na(metadat$Msmt_stop[i])) {
      x$MSMT_STOP_SEC <- params$MSMT_LENGTH_SEC
    } else {
      x$MSMT_STOP_SEC <- metadat$Msmt_stop[i]
    }
    
    # Calculate ELAPSED_SECS, which is relative time (in s) since start of measurement
    x$ELAPSED_SECS <- time_length(interval(min(x$TIMESTAMP), x$TIMESTAMP), "seconds")
    info$Min_timestamp <- min(x$TIMESTAMP)
    info$Max_timestamp <- max(x$TIMESTAMP)
  } else {
    # No data matched the date and time given in this row of the metadata
    message("\tWARNING; file: ", metadat$Metadata_file[i])
    zero_matches <- rbind(zero_matches, data.frame(File = metadat$Metadata_file[i],
                                                   Row = i,
                                                   Date = metadat$Date[i],
                                                   Time_start = metadat$Time_start[i]))
  }
  match_info <- rbind(match_info, info)
  matched_dat[[i]] <- x
}

# Combine all subsetted data together and merge with metadata 
combined_dat <- do.call("rbind", matched_dat)
combined_dat <- merge(metadat, combined_dat, by = "Obs")

# Compute a few basic stats about the combined data
n <- nrow(combined_dat)
neg_CO2 <- sum(combined_dat$CO2 < 0, na.rm = TRUE)
na_CO2 <- sum(is.na(combined_dat$CO2), na.rm = TRUE)
na_CH4 <- sum(is.na(combined_dat$CH4), na.rm = TRUE)

datatable(match_info)
```

Checking for metadata rows with no matching data...

```{r}
#| echo: false
#| output: asis

if(nrow(zero_matches)) {
  cat("**<span style='color:red'>---> At least one metadata row had no matching data! <---</span>**\n\n")
}
```

```{r}
# Table of zero-match data, if any
if(nrow(zero_matches)) {
  knitr::kable(zero_matches)
}
```

Checking for unused data files...

```{r}
#| echo: false
#| output: asis

unused_files <- setdiff(unique(dat$Data_file), unique(combined_dat$Data_file))
if(length(unused_files)) {
  cat("**<span style='color:red'>---> At least one data file was unused! <---</span>**\n\n")
}
```

```{r}
# Table of unused files, if any
if(length(unused_files)) {
  knitr::kable(data.frame(Data_file = unused_files))
}
```

Total rows of combined data: `r format(nrow(combined_dat), big.mark = ",")`

## CO2 and CH4 measurement data

Observations with negative CO2: `r neg_CO2` (`r round(neg_CO2 / n * 100, 0)`%)

Observations with missing CO2: `r na_CO2` (`r round(na_CO2 / n * 100, 0)`%)

Distribution of CO2 values:

```{r}
# Print distribution and (below) plot random subsample of CO2 data
OVERALL_PLOT_N <- 300
summary(combined_dat$CO2)
```

Plot of `r OVERALL_PLOT_N` random CO2 values (y axis 5%-95%):

```{r plot-obs}
#| fig-width: 8
#| fig-height: 4

df_split <- split(combined_dat, rep(1:3, each = nrow(combined_dat) / 3)) 

for (obs_group in df_split) {

  obs_group <- bind_rows(obs_group)
  ylim_co2 <- quantile(obs_group$CO2, probs = c(0.05, 0.95), na.rm = TRUE)
  
  dat_small <- obs_group[sample.int(n, OVERALL_PLOT_N, replace = TRUE),]
  print(ggplot(dat_small, aes(TIMESTAMP, CO2, color=Plot)) + 
    geom_point(na.rm = TRUE) +
    coord_cartesian(ylim = ylim_co2)) 
}
```

Observations with missing CH4: `r na_CH4` (`r round(na_CH4 / n * 100, 0)`%)

Distribution of CH4 values:

```{r}
# Print distribution and (below) plot random subsample of CH4 data
summary(combined_dat$CH4)
```

Plot of `r OVERALL_PLOT_N` random CH4 values (y axis 5%-95%):

```{r plot-ch4}
#| fig-width: 8
#| fig-height: 4

for (obs_group in df_split) {

  ylim_ch4 <- quantile(obs_group$CH4, probs = c(0.05, 0.95), na.rm = TRUE)
  
  dat_small <- obs_group[sample.int(n, OVERALL_PLOT_N, replace = TRUE),]
  print(ggplot(dat_small, aes(TIMESTAMP, CH4, color = Plot)) + 
    geom_point(na.rm = TRUE) +
    coord_cartesian(ylim = ylim_ch4))
}

```

## Observations by plot and day

```{r plot-co2}
obs_list <- split(combined_dat, combined_dat$Obs)
grouped_obs_list <- split(obs_list, ceiling(seq_along(obs_list)/12))

for (obs_group in grouped_obs_list) {
  for (list in obs_group) {
    data.frame(list)
  }
  obs_group <- bind_rows(obs_group)
  #| fig-width: 8
  print(ggplot(obs_group, aes(ELAPSED_SECS, CO2, group = Obs)) +
    geom_point(na.rm = TRUE, color = "orchid") +
    facet_wrap(~Plot + Date, scales = "free") +
    # Plot both linear and curvilinear fits
    #geom_smooth(method = lm, formula = 'y ~ poly(x, 2)', na.rm = TRUE, linetype = 2, color = "blue") +
    geom_smooth(method = lm, formula = 'y ~ x', na.rm = TRUE, color = "darkslateblue", linewidth = 0.5) +
    geom_vline(aes(xintercept = DEAD_BAND_SEC), linetype = 2, color = "gray40") +
    geom_vline(aes(xintercept = MSMT_STOP_SEC), linetype = 2, color = "black"))

  print(ggplot(obs_group, aes(ELAPSED_SECS, CH4, group = Obs)) +
    geom_point(na.rm = TRUE, color = "skyblue") +
    facet_wrap(~Plot + Date, scales = "free") +
    #geom_smooth(method = lm, formula = 'y ~ poly(x, 2)', na.rm = TRUE, linetype = 2, color = "blue") +
    geom_smooth(method = lm, formula = 'y ~ x', na.rm = TRUE, color = "navy", linewidth = 0.5) +
    geom_vline(aes(xintercept = DEAD_BAND_SEC), linetype = 2, color = "gray40") +
    geom_vline(aes(xintercept = MSMT_STOP_SEC), linetype = 2, color = "black"))
}
  
```

# Fluxes

## Model fitting

```{r model-fitting}

#would be great to include some code to remove lines that are at the end of the measurement...? I think we aren't 

#change units 
  #so we will use temp and volume from metadata 
  #using the fluxfinder conversion function to get nmol or umol units
combined_dat$CH4_nmol <- ffi_ppb_to_nmol(combined_dat$CH4,
                                        volume = combined_dat$Volume, #m3
                                        temp = combined_dat$Temp) #degrees C

combined_dat$CO2_umol <- ffi_ppm_to_umol(combined_dat$CO2,
                                        volume = combined_dat$Volume, #m3
                                        temp = combined_dat$Temp) #degrees C

#area from metadata used to get a concentration per area
combined_dat$CH4_nmol_m2 <- combined_dat$CH4_nmol / combined_dat$Area
combined_dat$CO2_umol_m2 <- combined_dat$CO2_umol / combined_dat$Area


#need to create a date_time column in dataframe
combined_dat$datetime <- as.POSIXct(
  paste(combined_dat$Date, format(combined_dat$Time_start, "%H:%M:%S")),
  tz = "EST"
)

combined_dat <- combined_dat %>%
  mutate(PlotID = paste0(Plot, "-", Date))

#also need to create a unique Plot_ID with the 

fluxes_co2 <- ffi_compute_fluxes(combined_dat,
                     group_column = "PlotID",
                     time_column = "TIMESTAMP",
                     gas_column = "CO2_umol_m2",
                     dead_band = "DEAD_BAND_SEC")


fluxes_ch4 <- ffi_compute_fluxes(combined_dat,
                     group_column = "PlotID",
                     time_column = "TIMESTAMP",
                     gas_column = "CH4_nmol_m2",
                     dead_band = "DEAD_BAND_SEC")


```

``` {#r}
```

## Flux summary

```{r flux-summary}

#make a plot of the flux estimates, error, and visualize with R2 as color
ggplot(fluxes_co2, aes(PlotID, lin_flux.estimate, color = lin_r.squared)) +
  geom_point(size=4) +
  geom_linerange(aes(ymin = lin_flux.estimate - lin_flux.std.error,
                     ymax = lin_flux.estimate + lin_flux.std.error)) +
  geom_hline(yintercept = 0, col="darkgray", linewidth=0.5) +
  theme_classic() +
  ylab(expression(paste( CO [2], " Flux (umol m"^-2* " s"^-1*")"))) +
  labs(color=expression(Adj.~R^{2})) +
  theme(axis.title.x = element_text(size=12), axis.text = element_text(size=12),
        axis.title.y = element_text(size=12), legend.text=element_text(size=12),
        panel.border = element_rect(colour = "black", fill=NA, linewidth =1))


ggplot(fluxes_ch4, aes(PlotID, lin_flux.estimate, color = lin_r.squared)) +
  geom_point(size=4) +
  geom_linerange(aes(ymin = lin_flux.estimate - lin_flux.std.error,
                     ymax = lin_flux.estimate + lin_flux.std.error)) +
  geom_hline(yintercept = 0, col="darkgray", linewidth=0.5) +
  theme_classic() +
  ylab(expression(paste( CH [4], "Flux (nmol m"^-2* " s"^-1*")"))) +
  labs(color=expression(Adj.~R^{2})) +
  theme(axis.title.x = element_text(size=12), axis.text = element_text(size=12),
        axis.title.y = element_text(size=12), legend.text=element_text(size=12),
        panel.border = element_rect(colour = "black", fill=NA, linewidth =1))

```

## R2 distribution

```{r qaqc-r2}
#| fig-width: 8
#| fig-height: 6


results$FLAG_R2 <- onecol_plot_and_flag(results,
                                        "adj.r.squared",
                                        right = FALSE)

subset_results_df <- subset(results, FLAG_R2=="TRUE")
n <- 6
nr <- nrow(subset_results_df)
R2_list <- split(subset_results_df, rep(1:ceiling(nr/n), each=n, length.out=nr))
for (flagged_df in R2_list) {
  plot_group(flagged_df, combined_dat)
}  
```

## y-intercept distribution

```{r qaqc-yint}
#| fig-width: 8
#| fig-height: 6

# The intercept is quite variable, so use {0.01, 0.99}
results$FLAG_INTERCEPT <- onecol_plot_and_flag(results, "int_estimate", 
                                               probs = c(0.01, 0.99))
plot_group(subset(results, FLAG_INTERCEPT), combined_dat)
```

## Robust regression divergence

This can indicate outlier problems or ebullition events.

```{r qaqc-robust}
#| fig-width: 8
#| fig-height: 6
results$FLAG_ROBUST_DIV <- twocol_plot_and_flag(results,
                                                "slope_estimate", 
                                                "slope_estimate_robust",
                                                add_1to1_line = TRUE)
plot_group(subset(results, FLAG_ROBUST_DIV), combined_dat)
```

## Polynomial regression divergence

This can indicate curvature of the gas concentrations time series due to saturation, etc.

```{r qaqc-poly}
#| fig-width: 8
#| fig-height: 6
results$FLAG_POLY_DIV <- twocol_plot_and_flag(results,
                                                "adj.r.squared", 
                                                "r.squared_poly",
                                                add_1to1_line = TRUE)

subset_results_df <- subset(results, FLAG_POLY_DIV=="TRUE")
n <- 6
nr <- nrow(subset_results_df)
R2_list <- split(subset_results_df, rep(1:ceiling(nr/n), each=n, length.out=nr))
for (flagged_df in R2_list) {
  plot_group(flagged_df, combined_dat)
}  
```

## QA/QC summary

```{r qaqc-summary}
res1 <- results["Obs"]
res2 <- results[grepl("^FLAG_", names(results))]
res <- pivot_longer(cbind(res1, res2), starts_with("FLAG"), names_to = "Flag")
res <- subset(res, value)
f <- function(x) paste(unique(x), collapse = ", ")
knitr::kable(aggregate(Obs ~ Flag, data = res, f))
```

```{r}
#| include: false

# -----------------------------------------------------------------------------
is_outlier <- function(x, devs = 5.2) {
 # See: Davies, P.L. and Gather, U. (1993).
 # "The identification of multiple outliers" (with discussion)
 # J. Amer. Statist. Assoc., 88, 782-801.

 x <- na.omit(x)
 lims <- median(x) + c(-1, 1) * devs * mad(x, constant = 1)
 return(x < lims[1] | x > lims[2])
}
```

# Output

## Data

```{r write-output}
now_string <- function() format(Sys.time(), "%Y-%m-%d")
mkfn <- function(name, extension) { # "make filename" helper function
  file.path(params$output_folder,
            paste(paste(name, now_string(), sep = "_"), extension, sep = "."))
}

flux_fn <- mkfn("results", "csv")
message("Writing final flux dataset ", flux_fn, "...")
write_csv(results, flux_fn)
obs_fn <- mkfn("observations", "csv")
message("Writing compiled observations dataset ", obs_fn, "...")
write_csv(combined_dat, obs_fn)

if(params$SAVE_RESULTS_FIGURES) {
  message("Writing plot outputs...", appendLF = FALSE)
  # Loop through each line, generating plot and filename and saving
  for(i in seq_len(nrow(results))) {
    p <- plot_without_print(results[i,], combined_dat)
    fn <- mkfn(paste("obs", results$Obs[i], results$Date[i],
                     results$Plot[i], sep = "_"), "pdf")
    ggsave(fn, plot = p, width = 8, height = 5)
  }
  message("\t", i, " written")
}
```

## Metadata

Note this doesn't include any 'extra' fields in the metadata file.

```{r write-metadata}
results_fields <- c(
  "adj.r.squared" = "Linear flux model adjusted R2",
  "AIC" = "Linear flux model AIC",
  "Area" = "Area entry from metadata file, m2",
  "BIC" = "Linear flux model BIC",
  "Data_file" = "Data file name",
  "Date" = "Date entry from metadata file",
  "DEAD_BAND_SEC" = "Dead band used for flux calculation, s",
  "Dead_band" = "Dead band entry from metadata file, if present, s",
  "deviance" = "Linear flux model deviance",
  "df.residual" = "Linear flux model degrees of freedom",
  "df" = "Linear flux model degrees of freedom",
  "FLAG_INTERCEPT" = "Flag: linear model intercept flagged as unusual",
  "FLAG_POLY_DIV" = "Flag: linear model R2 diverges from polynomial model",
  "FLAG_R2" = "Flag: linear model R2 flagged as unusual",
  "FLAG_ROBUST_DIV" = "Flag: linear model flux diverges from robust model",
  "flux_estimate" = "Gas flux computed from slope of linear flux model",
  "flux_std.error" = "Standard error of flux_estimate",
  "flux_units" = "Units of flux_estimate and flux_std.error",
  "Gas" = "Gas (CO2 or CH4)",
  "int_estimate" = "Intercept of linear flux model, ppb (CH4) or ppm (CO2)",
  "int_p.value" = "P-value of intercept of linear flux model",
  "int_statistic" = "Statistic (t-value) of intercept of linear flux model",
  "int_std.error" = "Standard error of intercept of linear flux model",
  "logLik" = "Linear flux model log-likelihood",
  "Metadata_file" = "Metadata file name",
  "MSMT_STOP_SEC" = "Measurement stop used for flux calculation, s",
  "Msmt_stop" = "Measurement stop entry from metadata file, if present, s",
  "nobs" = "Linear flux model number of observations",
  "Obs" = "Observation number, i.e. order in metadata file(s)",
  "p.value" = "Linear flux model p-value",
  "Plot" = "Plot entry from metadata file",
  "r.squared_poly" = "Polynomial flux model R2",
  "r.squared" = "Linear flux model R2",
  "sigma" = "Linear flux model sigma",
  "slope_estimate_robust" = "Slope of robust flux model, ppb (CH4) or ppm (CO2) /s",
  "slope_estimate" = "Slope of linear flux model, ppb (CH4) or ppm (CO2) /s",
  "slope_p.value" = "P-value of slope of linear flux model",
  "slope_statistic" = "Statistic (t-value) of slope of linear flux model",
  "slope_std.error" = "Standard error of slope of linear flux model",
  "SN" = "Serial number from analyzer",
  "statistic" = "Linear flux model F-statistic",
  "Temp" = "Temperature entry from metadata file, degC",
  "Time_start" = "Time_start entry from metadata file",
  "TIMESTAMP" = "Original timestamp from analyzer",
  "TZ" = "Time zone from analyzer",
  "Volume" = "Volume entry from metadata file, m3"
)

# Which metadata names above don't occur in results?
not_there <- names(results_fields)[!names(results_fields) %in% names(results)]
if(length(not_there)) {
  message("Field metadata descriptions NOT in results: ", paste(not_there, collapse = ", "))
}

results_meta <- data.frame(Field_name = names(results))
results_meta$Description <- results_fields[results_meta$Field_name]
# Mark any metadata fields that aren't required - they're extra, from user
from_metadata <- is.na(results_meta$Description) &
  results_meta$Field_name %in% names(metadat)
results_meta$Description[from_metadata] <- "<from metadata file>"
datatable(results_meta)

metadata_fn <- mkfn("results_metadata", "csv")
message("Writing metadata ", metadata_fn, "...")
write_csv(results_meta, metadata_fn)
```

All done!

# Reproducibility

```{r}
sessionInfo()
```
