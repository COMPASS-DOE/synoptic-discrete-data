## Remove once you fix the flagging chunk
#Pull in data that from the raw data file based on the sample info input above
dat_flagged <- all_dat %>%
filter(!str_detect(sample_ID, "PPR")) %>%
filter(str_detect(sample_ID, samples_pattern))
cat("Visualize Data")
#Plot samples to get a first look at concentrations (sanity check)
IDs <- data.frame(do.call('rbind', strsplit(as.character(dat_flagged$sample_name),'_',fixed=TRUE)))
colnames(IDs) <- c("Sample_Number", "Site_Code","Date" ,"Zone", "Lysimeter", "Depth", "Excess_Info")
#head(IDs)
#rejoin them to the dataframe
dat_flagged_id <- cbind(IDs, dat_flagged)
#group the data for plotting
dat_flagged_id <- dat_flagged_id %>%
group_by(Site_Code) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#Plot data and change colors based on flags to check it:
viz_cl_plot <- ggplot(dat_flagged_id, aes(x = factor(row_num), y = Cl_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
labs(x = " ", y = "Cl (mg/L)", title = "Samples: Chloride") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)
viz_so4_plot <-  ggplot(dat_flagged_id, aes(x = row_num, y = SO4_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
labs(x = " ", y = "TN (mg/L)", title = "Samples: TN") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)
#print(viz_c_plot)
#print(viz_n_plot)
ggarrange(viz_cl_plot, viz_so4_plot, nrow=2, ncol=1)
## Remove once you fix the flagging chunk
#Pull in data that from the raw data file based on the sample info input above
dat_flagged <- all_dat %>%
filter(!str_detect(sample_ID, "PPR")) %>%
filter(!str_detect(sample_ID, "_dup")) %>%
filter(!str_detect(sample_ID, "_spk")) %>%
filter(str_detect(sample_ID, samples_pattern))
cat("Visualize Data")
#Plot samples to get a first look at concentrations (sanity check)
IDs <- data.frame(do.call('rbind', strsplit(as.character(dat_flagged$sample_name),'_',fixed=TRUE)))
colnames(IDs) <- c("Sample_Number", "Site_Code","Date" ,"Zone", "Lysimeter", "Depth", "Excess_Info")
## Remove once you fix the flagging chunk
#Pull in data that from the raw data file based on the sample info input above
dat_flagged <- all_dat %>%
filter(!str_detect(sample_ID, "PPR")) %>%
filter(!str_detect(sample_ID, "_dup")) %>%
filter(!str_detect(sample_ID, "_spk")) %>%
filter(str_detect(sample_ID, samples_pattern))
cat("Visualize Data")
#Plot samples to get a first look at concentrations (sanity check)
IDs <- data.frame(do.call('rbind', strsplit(as.character(dat_flagged$sample_name),'_',fixed=TRUE)))
colnames(IDs) <- c("Sample_Number", "Site_Code","Date" ,"Zone", "Lysimeter", "Depth") #, "Excess_Info")
#head(IDs)
#rejoin them to the dataframe
dat_flagged_id <- cbind(IDs, dat_flagged)
#group the data for plotting
dat_flagged_id <- dat_flagged_id %>%
group_by(Site_Code) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#Plot data and change colors based on flags to check it:
viz_cl_plot <- ggplot(dat_flagged_id, aes(x = factor(row_num), y = Cl_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
labs(x = " ", y = "Cl (mg/L)", title = "Samples: Chloride") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)
viz_so4_plot <-  ggplot(dat_flagged_id, aes(x = row_num, y = SO4_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
labs(x = " ", y = "TN (mg/L)", title = "Samples: TN") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)
#print(viz_c_plot)
#print(viz_n_plot)
ggarrange(viz_cl_plot, viz_so4_plot, nrow=2, ncol=1)
cat("Check Sample IDs with Metadata")
#remove duplicates from the sample dataframe bc we are just taking the first dup run
all_data_flagged <- dat_flagged_id %>%
filter(!str_detect(sample_ID, "dup")) #%>%
#select(!Excess_Info)
#check to see if all samples are present in the metadata
all_present <- all(all_data_flagged$sample_ID %in% dionex_metadata$Cl_SO4_ID)
if (all_present) {
message("All sample IDs are present in metadata.")
} else {
message("Some sample IDs are missing from metadata.")
# Optional: Which ones are missing?
missing_ids <- setdiff(all_data_flagged$sample_ID, dionex_metadata$Cl_SO4_ID)
print(missing_ids)
}
dionex_metadata_selected <- dionex_metadata %>%
select(Cl_SO4_ID, Year, Month, Day, Depth_cm, Lysimeter, Time..24hr., Time.Zone_EDT.EST,  Field.Notes)
#merge metadata with sample run data
merged_data <- all_data_flagged %>%
left_join(dionex_metadata_selected, by = c("sample_ID" = "Cl_SO4_ID"))
cat("Check Sample IDs with Metadata")
#remove duplicates from the sample dataframe bc we are just taking the first dup run
all_data_flagged <- dat_flagged_id %>%
filter(!str_detect(sample_ID, "dup")) %>%
mutate(sample_ID = sub("^[^_]+_", "", sample_ID))
#select(!Excess_Info)
#check to see if all samples are present in the metadata
all_present <- all(all_data_flagged$sample_ID %in% dionex_metadata$Cl_SO4_ID)
if (all_present) {
message("All sample IDs are present in metadata.")
} else {
message("Some sample IDs are missing from metadata.")
# Optional: Which ones are missing?
missing_ids <- setdiff(all_data_flagged$sample_ID, dionex_metadata$Cl_SO4_ID)
print(missing_ids)
}
dionex_metadata_selected <- dionex_metadata %>%
select(Cl_SO4_ID, Year, Month, Day, Depth_cm, Lysimeter, Time..24hr., Time.Zone_EDT.EST,  Field.Notes)
#merge metadata with sample run data
merged_data <- all_data_flagged %>%
left_join(dionex_metadata_selected, by = c("sample_ID" = "Cl_SO4_ID"))
#let you know which section you are in
cat("Setup")
#set the run date & user name
run_date <- "20230603"
sample_year <- 2023
sample_month <- 04
user <- "Stephanie Wilson"
#identify the files you want to read in
NOx_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NOx_April2023_1.csv"
NH3_PO4_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_April2023_1.csv"
#  NH3_PO4_file_2 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_Aug2023_2.csv"
#combine files if multople
#  NH3_PO4_file <- rbind(NH3_PO4_file_1, NH3_PO4_file_2)
# Define the file path for QAQC log file - NO Need to change just check year
file_path <- "Raw Data/SEAL_COMPASS_Synoptic_QAQC_Log_2023.csv"
final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_202304.csv"
#record any notes about the run or anything other info here:
Notes <- "any info that needs recorded"
#let you know which section you are in
cat("Setup")
#a link to the Gitbook or whatever protocol you are using for this analysis
#Maybe using pacman instead?
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
dplyr,
ggplot2,
ggpubr,
stringr,
readr,
broom)
#Coefficients / constants that are needed for calculations
N_mw <- 14.0067    # molecular weight of N
P_mw <- 30.973762  # molecular weight of P
Con1 <- 1000       # conversion factor value
Con2 <- 1000000    # conversion factor value
#Detection limit and top standards for flagging
NOx_dl <- 0.025  #mgL
NH3_dl <- 0.02  #mgL
PO4_dl <- 0.003 #mgL
NOx_top <- 1  #mgL
NH3_top <- 2  #mgL
PO4_top <- 0.3 #mgL
#Check Standard concnetrations
NOx_CCV <- 0.5
NH3_CCV <- 1.0
PO4_CCV <- 0.15
#expected ranges for sample concentrations used for flags
r2_cutoff = 0.990
chk_flag = 0.10
chk_conc_flag = 15 #cutoff level for percent difference of check stds vs. expected concentration
# min_conc = int
# max_conc = int
# cv_flag = 10%
# rep_flag = 10%
#Flag that we
rep_flag = 25 #this is a 25% error between samples
#blank_flag is calculated based on samples later in this code
#check standard concentrations
# chk_std = 10
#any reference to other code
#let you know which section you are in
cat("Setup")
#set the run date & user name
run_date <- "20230603"
sample_year <- 2023
sample_month <- 04
user <- "Stephanie Wilson"
#identify the files you want to read in
NOx_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NOx_April2023_1.csv"
NH3_PO4_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_April2023_1.csv"
#  NH3_PO4_file_2 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_Aug2023_2.csv"
#combine files if multople
#  NH3_PO4_file <- rbind(NH3_PO4_file_1, NH3_PO4_file_2)
# Define the file path for QAQC log file - NO Need to change just check year
file_path <- "Raw Data/SEAL_COMPASS_Synoptic_QAQC_Log_2023.csv"
final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_202304.csv"
#record any notes about the run or anything other info here:
Notes <- "any info that needs recorded"
cat("Import Data")
#setwd("/Raw Data")
#set working directory
#could be from google drive or local drive etc.
path <- ("file path")
#Read in Raw Data
dat_NOx_1 <- readr::read_csv(NOx_file_1)
dat_NH3_PO4_1 <- readr::read_csv(NH3_PO4_file_1)
#dat_NH3_PO4_2 <- readr::read_csv(NH3_PO4_file_2)
# Combining Files
#dat_NH3_PO4 <- bind_rows(dat_NH3_PO4_1, dat_NH3_PO4_2)
df_combo <- bind_rows(dat_NOx_1, dat_NH3_PO4_1)
# Rename columns
df_combo_rename <- df_combo %>%
rename('Conc' = ...6,
'Absorbance' = ...7,
'Dilution' = ...9,
'Sample_Name' = ...4,
'Test' = ...13,
'Run_Time' = ...15,
'Unit' = ...12)
#making list to remove columns
cols_to_remove <- c("RUNSTARTED", "...5", "...8", "...10", "...11", "...14")
#Removing columns
df_all <- df_combo_rename %>%
select(-all_of(cols_to_remove)) %>%
select(!c(1,2))%>%
select(!c(8,9))
#Checking column headers
head(df_all)
View(dat_NOx_1)
View(df_combo_rename)
unique(df_combo_rename$Sample_Name)
table(df_combo_rename)
table(df_combo_rename$Sample_Name)
table(dat_NH3_PO4_1$...4)
View(dat_NH3_PO4_1)
#let you know which section you are in
cat("Setup")
#set the run date & user name
run_date <- "20230603"
sample_year <- 2023
sample_month <- 04
user <- "Stephanie Wilson"
#identify the files you want to read in
NOx_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NOx_April2023_1.csv"
NH3_PO4_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_April2023_1.csv"
#  NH3_PO4_file_2 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_Aug2023_2.csv"
#combine files if multople
#  NH3_PO4_file <- rbind(NH3_PO4_file_1, NH3_PO4_file_2)
# Define the file path for QAQC log file - NO Need to change just check year
file_path <- "Raw Data/SEAL_COMPASS_Synoptic_QAQC_Log_2023.csv"
final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_202304.csv"
#record any notes about the run or anything other info here:
Notes <- "There are two sample names we suspect were input incorrectly, they are listed below and have been checked against metadata"
#duplicate sample names to be changed
#list the sample iDs that are messed up and create a list with the cup identifier as well so that we can change them below
wrong_names <- c("GCW_202304_TR_LysC_45cm", "GCW_202304_TR_LysA_20cm")
wrong_cups <- c("20", "13")
right_names <- c("GCW_202304_TR_LysB_45cm", "GCW_202304_TR_LysA_10cm")
#can't determine from metadata - for now unsure
issue_names <- c("GCW_202304_TR_LysA_20cm")
#couldn't tell which onethis is from the metadata, no A_10cm which is what we thought marked on the sheet, need to check sample vials in            freezer to see if we have a A_10cm from GCW_TR to be sure
issue_cups <- c("13")
#let you know which section you are in
cat("Setup")
#set the run date & user name
run_date <- "20230603"
sample_year <- 2023
sample_month <- 04
user <- "Stephanie Wilson"
#identify the files you want to read in
NOx_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NOx_April2023_1.csv"
NH3_PO4_file_1 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_April2023_1.csv"
#  NH3_PO4_file_2 <- "Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_Aug2023_2.csv"
#combine files if multople
#  NH3_PO4_file <- rbind(NH3_PO4_file_1, NH3_PO4_file_2)
# Define the file path for QAQC log file - NO Need to change just check year
file_path <- "Raw Data/SEAL_COMPASS_Synoptic_QAQC_Log_2023.csv"
final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_202304.csv"
#record any notes about the run or anything other info here:
Notes <- "There are two sample names we suspect were input incorrectly, they are listed below and have been checked against metadata"
#duplicate sample names to be changed
#list the sample iDs that are messed up and create a list with the cup identifier as well so that we can change them below
wrong_names <- c("GCW_202304_TR_LysC_45cm", "GCW_202304_TR_LysA_20cm")
wrong_cups <- c("20", "13")
right_names <- c("GCW_202304_TR_LysB_45cm", "GCW_202304_TR_LysA_10cm")
#can't determine from metadata - for now unsure
issue_names <- c("GCW_202304_TR_LysA_20cm")
#couldn't tell which onethis is from the metadata, no A_10cm which is what we thought marked on the sheet, need to check sample vials in            freezer to see if we have a A_10cm from GCW_TR to be sure
issue_cups <- c("13")
cat("Import Data")
#setwd("/Raw Data")
#set working directory
#could be from google drive or local drive etc.
path <- ("file path")
#Read in Raw Data
dat_NOx_1 <- readr::read_csv(NOx_file_1)
dat_NH3_PO4_1 <- readr::read_csv(NH3_PO4_file_1)
#dat_NH3_PO4_2 <- readr::read_csv(NH3_PO4_file_2)
# Combining Files
#dat_NH3_PO4 <- bind_rows(dat_NH3_PO4_1, dat_NH3_PO4_2)
df_combo <- bind_rows(dat_NOx_1, dat_NH3_PO4_1)
# Rename columns
df_combo_rename <- df_combo %>%
rename('Conc' = ...6,
'Absorbance' = ...7,
'Dilution' = ...9,
'Sample_Name' = ...4,
'Test' = ...13,
'Run_Time' = ...15,
'Unit' = ...12)
#making list to remove columns
cols_to_remove <- c("RUNSTARTED", "...5", "...8", "...10", "...11", "...14")
#Removing columns
df_all <- df_combo_rename %>%
select(-all_of(cols_to_remove)) %>%
select(!c(1,2))%>%
select(!c(8,9))
#Checking column headers
head(df_all)
cat("Assess Standard Curves")
#CHECK SEAL METHOD FOR WHAT THE SETTING FOR THE EQUATIONS USED TO CALCULARE THE CONCENTRATIONS
#maybe nonlinear model or intercept settigns
#Pull out standards
stds <- df_all %>%
filter(str_detect(Sample_Name, "Standard"))
#Making standards dataframe based on the test
stds_NOx <- stds[stds$Test == "Vanadium NOx", ]
stds_NH3 <- stds[stds$Test == "Ammonia 2", ]
stds_PO4 <- stds[stds$Test == "o-PHOS 0.3", ]
#Inputting Standard Values Based on Protocol (LINK PROTOCOL)
stds_NOx <- stds_NOx %>%
mutate(`Conc` = case_when(
Sample_Name == "Standard 0" ~  0.0,
Sample_Name == "Standard 1" ~  0.0,
Sample_Name == "Standard 90" ~ 0.0222,
Sample_Name == "Standard 91" ~ 0.05,
Sample_Name == "Standard 92" ~ 0.1,
Sample_Name == "Standard 93" ~ 0.25,
Sample_Name == "Standard 94" ~ 0.5,
Sample_Name == "Standard 95" ~ 0.75,
Sample_Name == "Standard 96" ~ 1.0,
TRUE ~ `Conc`  # leave unchanged if no match
)) %>%
filter(!Sample_Name %in% c("Nitrate Standard" , "Nitrite Standard"))
stds_NH3 <- stds_NH3 %>%
mutate(`Conc` = case_when(
Sample_Name == "Standard 0" ~  0.0,
Sample_Name == "Standard 1" ~  0.0,
Sample_Name == "Standard .0389" ~ 0.0389,
Sample_Name == "Standard .1000" ~ 0.1,
Sample_Name == "Standard .2000" ~ 0.2,
Sample_Name == "Standard .5000" ~ 0.5,
Sample_Name == "Standard 1.0000" ~ 1.0,
Sample_Name == "Standard 1.5000" ~ 1.5,
Sample_Name == "Standard 2.0000" ~ 2,
TRUE ~ `Conc`  # leave unchanged if no match
))
stds_PO4 <- stds_PO4 %>%
mutate(`Conc` = case_when(
Sample_Name == "Standard 0" ~  0.0,
Sample_Name == "Standard 1" ~  0.0,
Sample_Name == "Standard 90" ~ 0.0060,
Sample_Name == "Standard 91" ~ 0.0150,
Sample_Name == "Standard 92" ~ 0.0300,
Sample_Name == "Standard 93" ~ 0.0750,
Sample_Name == "Standard 94" ~ 0.1500,
Sample_Name == "Standard 95" ~ 0.2250,
Sample_Name == "Standard 96" ~ 0.3000,
TRUE ~ `Conc`  # leave unchanged if no match
))
#generating line of best fit aka standard curves
#NOx
lin_reg_NOx <- lm(stds_NOx$Absorbance ~stds_NOx$Conc, stds_NOx)
NOx_coefs <- coef(lin_reg_NOx)                        # intercept and slope
NOx_r2 <- summary(lin_reg_NOx)$r.squared              # R squared
# Store in a clean dataframe
std_curve_NOx_1 <- data.frame(
Test = "NOx",
Intercept = NOx_coefs["(Intercept)"],
Slope = NOx_coefs[2],
R_squared = NOx_r2
)
#NH3
lin_reg_NH3 <- lm(stds_NH3$Absorbance ~stds_NH3$Conc, stds_NH3)
NH3_coefs <- coef(lin_reg_NH3)                        # intercept and slope
NH3_r2 <- summary(lin_reg_NH3)$r.squared              # R squared
# Store in a clean dataframe
std_curve_NH3_1 <- data.frame(
Test = "NH3",
Intercept = NH3_coefs["(Intercept)"],
Slope = NH3_coefs[2],
R_squared = NH3_r2
)
#PO4
lin_reg_PO4 <- lm(stds_PO4$Absorbance ~stds_PO4$Conc, stds_PO4)
PO4_coefs <- coef(lin_reg_PO4)                        # intercept and slope
PO4_r2 <- summary(lin_reg_PO4)$r.squared              # R squared
# Store in a clean dataframe
std_curve_PO4_1 <- data.frame(
Test = "PO4",
Intercept = PO4_coefs["(Intercept)"],
Slope = PO4_coefs[2],
R_squared = PO4_r2
)
#Combining standards & curve data frames
stds_combo <- bind_rows(stds_NOx, stds_NH3, stds_PO4)
std_curve_combo <- bind_rows(std_curve_NOx_1, std_curve_NH3_1, std_curve_PO4_1)
cat("Assess Standard Curves")
#Plot standard Curve or Curves
#v-Nox
std_curve_NOx <- ggplot(stds_NOx, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkorchid") +
theme_bw() +
labs(x="Concentration (ppm)")+
annotate("text", x = 0.75, y = max(stds_NOx$Absorbance), label = paste("r^2 =", round(NOx_r2, 4)),
color = "black", size = 4)
print(std_curve_NOx)
#NH3
std_curve_NH3 <- ggplot(stds_NH3, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkblue") +
theme_bw() +
labs(x="Concentration (ppm)")+
annotate("text", x = 1.7, y = max(stds_NH3$Absorbance), label = paste("r^2 =", round(NH3_r2, 4)),
color = "black", size = 4)
print(std_curve_NH3)
#PO4
std_curve_PO4 <- ggplot(stds_PO4, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
theme_bw() +
labs(x="Concentration (ppm)")+
annotate("text", x = 0.25, y = max(stds_PO4$Absorbance), label = paste("r^2 =", round(PO4_r2, 4)),
color = "black", size = 4)
print(std_curve_PO4)
############## Report on Cutoffs
#Report out a flag if the run has an R2 lower than appropriate
#Write out to the user whether or not the r2 is above the cutoff of 0.98
ifelse(std_curve_NOx_1$R_squared <= r2_cutoff, "NOx Curve r2 is below cutoff! - REASSESS", "NOx Curve r2 GOOD - PROCEED")
ifelse(std_curve_NH3_1$R_squared <= r2_cutoff, "NH3 Curve r2 is below cutoff! - REASSESS", "NH3 Curve r2 GOOD - PROCEED")
ifelse(std_curve_PO4_1$R_squared <= r2_cutoff, "PO4 Curve r2 is below cutoff! - REASSESS", "PO4 Curve r2 GOOD - PROCEED")
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(file_path)) {
# If it exists, read it back into R
stds_log <- read.csv(file_path)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
stds_log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(stds_log) <- c("Test", "Intercept", "Slope", "R_squared", "run_date", "user")
#maybe add here the last slopes from the previous year
# Write add_log to CSV
write.csv(stds_log, file = file_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#getting rid of first "X" column
stds_log <- stds_log %>%
select(-X)
#create a dataframe that can be appended to the QAQC log
add_log <- as.data.frame(rbind(std_curve_NOx_1, std_curve_NH3_1, std_curve_PO4_1))
add_log$run_date <- run_date
add_log$user <- user
#compare slopes to previous runs (from log) in order to assess drift
log <- rbind(stds_log, add_log)
Slopes_chk <- ggplot(log, aes(run_date, Slope, col=Test)) +
geom_point(size=4) +
geom_line() + theme_bw() +
labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
scale_color_manual(values=c("darkred", "darkorange", "darkolivegreen"))
Slopes_chk
#write out to the user if the slope is within 10% of previous slopes
#calculate an average of the slopes and then determine how far off we are?
# section where we take all the logs that will calc avg of all prev ran slopes then compare current slope
#since there wouldnt be any prev dat we will have to put a starter slope to compare to
#ifelse(C_slope_cv <= chk_flag, "NPOC Curve slope is out of range! - REASSESS", "NPOC Curve slope GOOD")
#ifelse(N_slope_cv <= chk_flag, "TN Curve slope is out of range! - REASSESS", "TN Curve slope GOOD")
#write out the log file with the added lines for this run date
write.csv(log, file_path)
View(stds_log)
