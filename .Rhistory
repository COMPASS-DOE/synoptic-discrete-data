sample_type = X.2,
Cl_ppm = IC.Cl.1,
Cl_area = IC.Cl.3
) %>%
group_by(sample_name) %>%
mutate(sample_name = if_else(
sample_name %in% c("Lab Blank", "Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
paste0(sample_name, "_", row_number()),
sample_name
)) %>%
ungroup()
#head(Cldat_clean)
## Pull Cl and SO4 data  together:
all_dat_merge <- inner_join(Sdat_clean, Cldat_clean, by = c("sample_name", "sample_ID", "sample_type"))
#Remove any lines that have no sample ID, make n.a. into NAs,
#if there are NAs in the SO4 or Cl columns, make them zeros because that means there was no peak detected
all_dat <- all_dat_merge %>%
filter(!is.na(sample_ID) & sample_ID != "") %>%
mutate(across(everything(), ~ na_if(.x, "n.a."))) %>%
mutate(across(c(SO4_ppm, Cl_ppm), as.numeric),
across(c(SO4_area, Cl_area), as.numeric)) %>%
mutate(across(c(SO4_ppm, Cl_ppm), ~ replace_na(.x, 0)),
across(c(SO4_area, Cl_area), ~ replace_na(.x, 0)))
head(all_dat)
cat("Assess the Standard Curves")
# Filter standards
stds <- all_dat %>%
filter(grepl("Calibration Standard", sample_type)) %>%
mutate(run_date = Date_Run)
#We might need to identify the concentration of the standards, rather than use what is in the file TBD
#calculate slope and r2 of cal curves
#Cl curve
lm_results_cl <- stds %>%
group_by(run_date) %>%
do({
model = lm(Cl_area ~ Cl_ppm, data = .)
tidy_model = tidy(model)             # coefficients
glance_model = glance(model)         # model metrics like R²
tibble(
slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
intercept = tidy_model$estimate[1],
r2 = glance_model$adj.r.squared
)
}) %>%
mutate(
analyte = "Cl",
curve = "Chloride (mg/L)"
)
#SO4 curve
lm_results_s <- stds %>%
group_by(run_date) %>%
do({
model = lm(SO4_area ~ SO4_ppm, data = .)
tidy_model = tidy(model)             # coefficients
glance_model = glance(model)         # model metrics like R²
tibble(
slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
intercept = tidy_model$estimate[1],
r2 = glance_model$adj.r.squared
)
}) %>%
mutate(
analyte = "SO4",
curve = "SO4 (mg/L)"
)
#put the together in one dataframe to later add to log
Slopes <- rbind(lm_results_cl, lm_results_s)
#store the r2's so they plot on the curve graphs
r2_labels_cl <- stds %>%
group_by(run_date) %>%
summarise(
x_pos = max(Cl_ppm, na.rm = TRUE) * 0.8,
y_pos = max(Cl_area, na.rm = TRUE),
r_squared = round(summary(lm(Cl_area ~ Cl_ppm))$adj.r.squared, 4),
.groups = "drop"
)
r2_labels_so4 <- stds %>%
group_by(run_date) %>%
summarise(
x_pos = max(SO4_ppm, na.rm = TRUE) * 0.8,
y_pos = max(SO4_area, na.rm = TRUE),
r_squared = round(summary(lm(SO4_area ~ SO4_ppm))$adj.r.squared, 4),
.groups = "drop"
)
##Plot standard Curve or Curves
#Cl Curve
Cl_stds_plot <- ggplot(stds, aes(x = Cl_ppm, y = Cl_area)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
facet_wrap(~ run_date) +
geom_text(
data = r2_labels_cl,
aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
inherit.aes = FALSE,
hjust = 1, vjust = 1,
size = 4
) +
labs(
title = "Chloride Std Curve by Date",
x = "Cl Standard Concentration (ppm)",
y = "Peak Area"
) +
theme_bw()
Cl_stds_plot
#SO4 Curve
SO4_stds_plot <- ggplot(stds, aes(x = SO4_ppm, y = SO4_area)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "purple") +
facet_wrap(~ run_date) +
geom_text(
data = r2_labels_so4,
aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
inherit.aes = FALSE,
hjust = 1, vjust = 1,
size = 4
) +
labs(
title = "Sulfate Std Curve by Date",
x = "SO4 Standard Concentration (ppm)",
y = "Peak Area"
) +
theme_bw()
SO4_stds_plot
#compare slopes to previous runs (from log) in order to assess drift
log <- read.csv(Log_path)
log <- log[ ,-c(1)]
#make sure they both have dates as dates
log$run_date <- as.Date(log$run_date)
log$analyte <- as.character(log$analyte)
log$curve <- as.character(log$curve)
Slopes$run_date <- as.Date(Slopes$run_date)
# Filter to only rows in Slopes that are NOT already in log (by run_date + analyte)
new_rows <- anti_join(Slopes, log, by = c("run_date", "analyte"))
# Append the new, non-duplicate rows to log
log <- bind_rows(log, new_rows)
#plot the current slops with the previous slopes
Slopes_chk <- ggplot(log, aes(run_date, slope, col=curve)) +
geom_point(size=4) +
geom_line() +
theme_bw() + labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
scale_color_manual(values=c("blue", "purple"))
Slopes_chk
#write out the log file with the added lines for this run
write.csv(log, Log_path)
#Grab the highest r2 that is available for this run
r2_Cl = max(lm_results_cl$r2)
r2_SO4 = max(lm_results_s$r2)
#Write out to the user whether or not the r2 is above the cutoff of 0.98
ifelse(r2_Cl <= r2_cutoff,
"Cl Curve r2 is below cutoff! - REASSESS", "Cl Curve r2 GOOD")
ifelse(r2_SO4 <= r2_cutoff,
"SO4 Curve r2 is below cutoff! - REASSESS", "SO4 Curve r2 GOOD")
#write out a flag to the sample dataframe if the r2 is above the cutoff of 0.98
all_dat <- all_dat %>%
mutate(
Cl_flag = if (r2_Cl <= r2_cutoff) {
"NPOC r2 low"
} else {
""
},
SO4_flag = if (r2_SO4 <= r2_cutoff) {
"TN r2 low"
} else {
""
}
)
cat("Assess the Standard Curves")
# Filter standards
stds <- all_dat %>%
filter(grepl("Calibration Standard", sample_type)) %>%
mutate(run_date = Date_Run)
#We might need to identify the concentration of the standards, rather than use what is in the file TBD
#calculate slope and r2 of cal curves
#Cl curve
lm_results_cl <- stds %>%
group_by(run_date) %>%
do({
model = lm(Cl_area ~ Cl_ppm, data = .)
tidy_model = tidy(model)             # coefficients
glance_model = glance(model)         # model metrics like R²
tibble(
slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
intercept = tidy_model$estimate[1],
r2 = glance_model$adj.r.squared
)
}) %>%
mutate(
analyte = "Cl",
curve = "Chloride (mg/L)"
)
#SO4 curve
lm_results_s <- stds %>%
group_by(run_date) %>%
do({
model = lm(SO4_area ~ SO4_ppm, data = .)
tidy_model = tidy(model)             # coefficients
glance_model = glance(model)         # model metrics like R²
tibble(
slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
intercept = tidy_model$estimate[1],
r2 = glance_model$adj.r.squared
)
}) %>%
mutate(
analyte = "SO4",
curve = "SO4 (mg/L)"
)
#put the together in one dataframe to later add to log
Slopes <- rbind(lm_results_cl, lm_results_s)
#store the r2's so they plot on the curve graphs
r2_labels_cl <- stds %>%
group_by(run_date) %>%
summarise(
x_pos = max(Cl_ppm, na.rm = TRUE) * 0.8,
y_pos = max(Cl_area, na.rm = TRUE),
r_squared = round(summary(lm(Cl_area ~ Cl_ppm))$adj.r.squared, 4),
.groups = "drop"
)
r2_labels_so4 <- stds %>%
group_by(run_date) %>%
summarise(
x_pos = max(SO4_ppm, na.rm = TRUE) * 0.8,
y_pos = max(SO4_area, na.rm = TRUE),
r_squared = round(summary(lm(SO4_area ~ SO4_ppm))$adj.r.squared, 4),
.groups = "drop"
)
##Plot standard Curve or Curves
#Cl Curve
Cl_stds_plot <- ggplot(stds, aes(x = Cl_ppm, y = Cl_area)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
facet_wrap(~ run_date) +
geom_text(
data = r2_labels_cl,
aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
inherit.aes = FALSE,
hjust = 1, vjust = 1,
size = 4
) +
labs(
title = "Chloride Std Curve by Date",
x = "Cl Standard Concentration (ppm)",
y = "Peak Area"
) +
theme_bw()
Cl_stds_plot
#SO4 Curve
SO4_stds_plot <- ggplot(stds, aes(x = SO4_ppm, y = SO4_area)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "purple") +
facet_wrap(~ run_date) +
geom_text(
data = r2_labels_so4,
aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
inherit.aes = FALSE,
hjust = 1, vjust = 1,
size = 4
) +
labs(
title = "Sulfate Std Curve by Date",
x = "SO4 Standard Concentration (ppm)",
y = "Peak Area"
) +
theme_bw()
SO4_stds_plot
#compare slopes to previous runs (from log) in order to assess drift
log <- read.csv(Log_path)
#identify section
cat("Setup Information")
###### Run information - PLEASE CHANGE
Date_Run = "09/05/23"  #Date that instrument was run
Run_by = "Stephanie J. Wilson"  #Instrument user
Script_run_by = "Stephanie J. Wilson" #Code user
run_notes = "None"  #any notes from the run
samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names
samples_pattern <- paste(samples, collapse = "|")
#samples_pattern <- "GCW" #use this instead of the line above if you have only one site code
chks_name = "Check Standard"  #what did you name your check standards?
###### File Names - PLEASE CHANGE
#file path and name for raw summary data file
raw_file_name_cl = "Raw Data/COMPASS_Synoptic_CB_MonMon_202304_Cl.txt"  #example
raw_file_name_so4 = "Raw Data/COMPASS_Synoptic_CB_MonMon_202304_SO4.txt"
#raw_file_name_cl = "Raw Data/FILE_NAME.txt"
#raw_file_name_so4 = "Raw Data/FILE_NAME.txt"
#file path and name for raw all peaks file
#raw_allpeaks_name = "Raw Data/COMPASS_SynopticCB_PW_DOC_202505_allpeaks.txt" #example
#raw_allpeaks_name = "Raw Data/FILE_NAME.txt"
#file path and name of processed data file
processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202304.csv" #example
#processed_file_name = "Processed Data/FILE_NAME.csv" #example
###### Log Files - PLEASE CHECK
#downloaded metadata csv - downloaded from Google drive as csv for this year
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#qaqc log file path for this year
Log_path = "Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
#identify section
cat("Setup")
#Link to the protocol used for analysis
#steph will add this soon
#Packages that are required
lapply(c(
"dplyr", "ggplot2", "ggpubr", "stringr",
"purrr", "tidyverse", "here", "broom", "tibble",
"googledrive", "googlesheets4", "data.table",
"matrixStats", "gridExtra", "grid"),
library, character.only = TRUE)
#any coefficients / constants that are needed for calculations
cl_mw <- 35.45     #molecular weight of Chloride: 35.45
s_mw <- 32.06      #molecular weight of sulfur: 32.06
Con1 <- 1000       # conversion factor value
Con2 <- 1000000    # conversion factor value
#Flag for Dionex ##Need to edit this!
r2_cutoff = 0.98  #this is the level below which we want to rerun or consider a curve
chk_flag = 5   #for the cv
chk_conc_flag = 5 #this is the level cutoff for percent difference of check standards vs. the concentration they are meant to be
chks_flag = 0.50 #this is the percent of chks we want to have a CV less than 10, usually 60
rep_flag = 25 #this is a 25% error between samples
#blank_flag - calculated based on samples later in this code as lower 25% quantile of sample concentrations
#standard concentrations - Update if running different checks:
standards <- tibble(
sample_ID = c("Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
SO4_std_conc = c(0.5, 1.0, 2.0, 10, 20),
Cl_std_conc = c(5, 10, 20, 100, 200)
)
#Spike concentration calc:
spk_std <- (250/s_mw)    # in mM
spkvol <- 10             # in uL
spkvol <- spkvol/1000000
#spike for these samples was 10uL of the 250mM standard
spk_Conc <- (spk_std)*spkvol
#Top standard Concentrations- Update if running different standard curve:
top_std_c1 = 200
top_std_so4 = 20
#Set time zone
common_tz = "Etc/GMT+5"
Sys.setenv(TZ = "America/New_York")
#plot indicators
site_order <- c('GCW', 'MSM', 'GWI', 'SWH')
plot_order <- c('UP', 'SWAMP', 'TR', 'WC', 'SW')
plot_colors <- c("#20063B", "darkgrey", "#FFBC42", "#419973", "#25ABE6" )
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
#read in the raw metadata file
raw_metadata <- read.csv(Raw Data/Raw_Metadata)
#read in the raw metadata file
raw_metadata <- read.csv("Raw Data/Raw_Metadata")
#identify section
cat("Setup Information")
###### Run information - PLEASE CHANGE
Date_Run = "Unknown"  #Date that instrument was run
Run_by = "Unknown"  #Instrument user
Script_run_by = "Zoe Read" #Code user
run_notes = "Samples missing from metadata: MSM_202311_UP_LYSC_45CM MSM_202311_WC_LYSB_45CM MSM_202311_WC_LYSC_45CM      MSM_202311_UP_RHZ_SF_TREE_1 MSM_202311_UP_RHZ_SF_TREE_2   MSM_202311_UP_RHZ_SF_TREE_3   MSM_202311_UP_RHZ_SF_TREE_4  MSM_202311_UP_RHZ_SF_TREE_6   MSM_202311_UP_RHZ_SF_TREE_7   MSM_202311_UP_RHZ_SF_TREE_8   MSM_202311_TR_RHZ_SF_TREE_   MSM_202311_TR_RHZ_SF_TREE_2   MSM_202311_TR_RHZ_SF_TREE_3 MSM_202311_TR_RHZ_SF_TREE_4  MSM_202311_TR_RHZ_SF_TREE_5  MSM_202311_TR_RHZ_SF_TREE_6  MSM_202311_TR_RHZ_SF_TREE_7   MSM_202311_TR_RHZ_SF_TREE_8 MSM_202311_WC_RHZ_SF_COLLAR_1 MSM_202311_WC_RHZ_SF_COLLAR_2 MSM_202311_WC_RHZ_SF_COLLAR_3 MSM_202311_WC_RHZ_SF_COLLAR_4 MSM_202311_WC_RHZ_SF_COLLAR_5 MSM_202311_WC_RHZ_LYSA
MSM_202311_WC_RHZ_LYSC, 118_MSM_202311_TR_RHZ_SF_Tree_1 value above cal curve for SO4, but only slightly" #any notes from the run
samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names
samples_pattern <- paste(samples, collapse = "|")
#samples_pattern <- "GCW" #use this instead of the line above if you have only one site code
chks_name = "Check Standard"  #what did you name your check standards?
###### File Names - PLEASE CHANGE
#file path and name for raw summary data file
raw_file_name_cl = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_Cl.txt"
raw_file_name_so4 = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_SO4.txt"
#file path and name of processed data file
processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202311.csv"
###### Log Files - PLEASE CHECK
#downloaded metadata csv - downloaded from Google drive as csv for this year
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#qaqc log file path for this year
Log_path = "Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
#identify section
cat("Setup Information")
###### Run information - PLEASE CHANGE
Date_Run = "Unknown"  #Date that instrument was run
Run_by = "Unknown"  #Instrument user
Script_run_by = "Zoe Read" #Code user
run_notes = "Samples missing from metadata: MSM_202311_UP_LYSC_45CM MSM_202311_WC_LYSB_45CM MSM_202311_WC_LYSC_45CM      MSM_202311_UP_RHZ_SF_TREE_1 MSM_202311_UP_RHZ_SF_TREE_2   MSM_202311_UP_RHZ_SF_TREE_3   MSM_202311_UP_RHZ_SF_TREE_4  MSM_202311_UP_RHZ_SF_TREE_6   MSM_202311_UP_RHZ_SF_TREE_7   MSM_202311_UP_RHZ_SF_TREE_8   MSM_202311_TR_RHZ_SF_TREE_   MSM_202311_TR_RHZ_SF_TREE_2   MSM_202311_TR_RHZ_SF_TREE_3 MSM_202311_TR_RHZ_SF_TREE_4  MSM_202311_TR_RHZ_SF_TREE_5  MSM_202311_TR_RHZ_SF_TREE_6  MSM_202311_TR_RHZ_SF_TREE_7   MSM_202311_TR_RHZ_SF_TREE_8 MSM_202311_WC_RHZ_SF_COLLAR_1 MSM_202311_WC_RHZ_SF_COLLAR_2 MSM_202311_WC_RHZ_SF_COLLAR_3 MSM_202311_WC_RHZ_SF_COLLAR_4 MSM_202311_WC_RHZ_SF_COLLAR_5 MSM_202311_WC_RHZ_LYSA
MSM_202311_WC_RHZ_LYSC, 118_MSM_202311_TR_RHZ_SF_Tree_1 value above cal curve for SO4, but only slightly" #any notes from the run
samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names
samples_pattern <- paste(samples, collapse = "|")
#samples_pattern <- "GCW" #use this instead of the line above if you have only one site code
chks_name = "Check Standard"  #what did you name your check standards?
###### File Names - PLEASE CHANGE
#file path and name for raw summary data file
raw_file_name_cl = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_Cl.txt"
raw_file_name_so4 = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_SO4.txt"
#file path and name of processed data file
processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202311.csv"
###### Log Files - PLEASE CHECK
#downloaded metadata csv - downloaded from Google drive as csv for this year
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#qaqc log file path for this year
Log_path = "Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
#Packages that are required
lapply(c(
"dplyr", "ggplot2", "ggpubr", "stringr",
"purrr", "tidyverse", "here", "broom", "tibble",
"googledrive", "googlesheets4", "data.table",
"matrixStats", "gridExtra", "grid"),
library, character.only = TRUE)
#Link to the protocol used for analysis
#steph will add this soon
#Any coefficients / constants that are needed for calculations
cl_mw <- 35.45     #molecular weight of Chloride: 35.45
s_mw <- 32.06      #molecular weight of sulfur: 32.06
#Flags for Dionex
r2_cutoff = 0.98  #this is the level below which we want to rerun or consider a curve
chk_flag_std_s = 10 #this is the maximum cv allowed for sulfate check standards
chk_flag_std_cl = 5   #this is the maximum cv allowed for chloride check standards
chk_flag_std_dups = 10 #this is the maximum cv allowed for duplicates
chks_flag = 0.20 #this is the percent of chks we want to have a CV less than the max allowable cv
#blank_flag - calculated based on samples later in this code as lower 25% quantile of sample concentrations
#Standard concentrations - Update if running different standard curve:
standards <- tibble(
sample_ID = c("Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
SO4_std_conc = c(0.5, 1.0, 2.0, 10, 20),
Cl_std_conc = c(5, 10, 20, 100, 200))
#Spike concentration calc
spk_std <- (250/s_mw)    # in mM
spkvol <- 10             # in uL
spkvol <- spkvol/1000000
#spike for these samples was 10uL of the 250mM standard
spk_Conc <- (spk_std)*spkvol
#Top standard Concentrations- Update if running different standard curve:
top_std_cl = 200
top_std_so4 = 20
#Set time zone
common_tz = "Etc/GMT+5"
Sys.setenv(TZ = "America/New_York")
#plot indicators
site_order <- c('GCW', 'MSM', 'GWI', 'SWH')
plot_order <- c('UP', 'SWAMP', 'TR', 'WC', 'SW')
plot_colors <- c("#20063B", "darkgrey", "#FFBC42", "#419973", "#25ABE6" )
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
#read in the raw metadata file
raw_metadata <- read.csv(Raw Data/Raw_Metadata)
#read in the raw metadata file
raw_metadata <- read.csv("Raw Data/Raw_Metadata")
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
#identify section
cat("Setup Information")
###### Run information - PLEASE CHANGE
Date_Run = "Unknown"  #Date that instrument was run
Run_by = "Unknown"  #Instrument user
Script_run_by = "Zoe Read" #Code user
run_notes = "Samples missing from metadata: MSM_202311_UP_LYSC_45CM MSM_202311_WC_LYSB_45CM MSM_202311_WC_LYSC_45CM      MSM_202311_UP_RHZ_SF_TREE_1 MSM_202311_UP_RHZ_SF_TREE_2   MSM_202311_UP_RHZ_SF_TREE_3   MSM_202311_UP_RHZ_SF_TREE_4  MSM_202311_UP_RHZ_SF_TREE_6   MSM_202311_UP_RHZ_SF_TREE_7   MSM_202311_UP_RHZ_SF_TREE_8   MSM_202311_TR_RHZ_SF_TREE_   MSM_202311_TR_RHZ_SF_TREE_2   MSM_202311_TR_RHZ_SF_TREE_3 MSM_202311_TR_RHZ_SF_TREE_4  MSM_202311_TR_RHZ_SF_TREE_5  MSM_202311_TR_RHZ_SF_TREE_6  MSM_202311_TR_RHZ_SF_TREE_7   MSM_202311_TR_RHZ_SF_TREE_8 MSM_202311_WC_RHZ_SF_COLLAR_1 MSM_202311_WC_RHZ_SF_COLLAR_2 MSM_202311_WC_RHZ_SF_COLLAR_3 MSM_202311_WC_RHZ_SF_COLLAR_4 MSM_202311_WC_RHZ_SF_COLLAR_5 MSM_202311_WC_RHZ_LYSA
MSM_202311_WC_RHZ_LYSC, 118_MSM_202311_TR_RHZ_SF_Tree_1 value above cal curve for SO4, but only slightly" #any notes from the run
samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names
samples_pattern <- paste(samples, collapse = "|")
#samples_pattern <- "GCW" #use this instead of the line above if you have only one site code
chks_name = "Check Standard"  #what did you name your check standards?
###### File Names - PLEASE CHANGE
#file path and name for raw summary data file
raw_file_name_cl = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_Cl.txt"
raw_file_name_so4 = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_SO4.txt"
#file path and name of processed data file
processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202311.csv"
###### Log Files - PLEASE CHECK
#downloaded metadata csv - downloaded from Google drive as csv for this year
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#qaqc log file path for this year
Log_path = "Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
#Packages that are required
lapply(c(
"dplyr", "ggplot2", "ggpubr", "stringr",
"purrr", "tidyverse", "here", "broom", "tibble",
"googledrive", "googlesheets4", "data.table",
"matrixStats", "gridExtra", "grid"),
library, character.only = TRUE)
#Link to the protocol used for analysis
#steph will add this soon
#Any coefficients / constants that are needed for calculations
cl_mw <- 35.45     #molecular weight of Chloride: 35.45
s_mw <- 32.06      #molecular weight of sulfur: 32.06
#Flags for Dionex
r2_cutoff = 0.98  #this is the level below which we want to rerun or consider a curve
chk_flag_std_s = 10 #this is the maximum cv allowed for sulfate check standards
chk_flag_std_cl = 5   #this is the maximum cv allowed for chloride check standards
chk_flag_std_dups = 10 #this is the maximum cv allowed for duplicates
chks_flag = 0.20 #this is the percent of chks we want to have a CV less than the max allowable cv
#blank_flag - calculated based on samples later in this code as lower 25% quantile of sample concentrations
#Standard concentrations - Update if running different standard curve:
standards <- tibble(
sample_ID = c("Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
SO4_std_conc = c(0.5, 1.0, 2.0, 10, 20),
Cl_std_conc = c(5, 10, 20, 100, 200))
#Spike concentration calc
spk_std <- (250/s_mw)    # in mM
spkvol <- 10             # in uL
spkvol <- spkvol/1000000
#spike for these samples was 10uL of the 250mM standard
spk_Conc <- (spk_std)*spkvol
#Top standard Concentrations- Update if running different standard curve:
top_std_cl = 200
top_std_so4 = 20
#Set time zone
common_tz = "Etc/GMT+5"
Sys.setenv(TZ = "America/New_York")
#plot indicators
site_order <- c('GCW', 'MSM', 'GWI', 'SWH')
plot_order <- c('UP', 'SWAMP', 'TR', 'WC', 'SW')
plot_colors <- c("#20063B", "darkgrey", "#FFBC42", "#419973", "#25ABE6" )
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
###### Log Files - PLEASE CHECK
#downloaded metadata csv - downloaded from Google drive as csv for this year
Raw_Metadata = "COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
