Log_path = "Raw Data/COMPASS_Synoptic_DIC_QAQClog_2025.csv"
#identify section
cat("Setup")
#Link to the protocol used for analysis
#steph will add this soon
#Packages that are required
lapply(c(
"dplyr", "ggplot2", "ggpubr", "stringr",
"purrr", "tidyverse", "here", "broom",
"googledrive", "googlesheets4"),
library, character.only = TRUE)
#any coefficients / constants that are needed for calculations
mw_c <- 12.011   #molecular weight of Carbon
mw_n <- 14.0067  #molecular weight of Nitrogen
Con1 <- 1000       # conversion factor value
Con2 <- 1000000    # conversion factor value
#Flag that we
r2_cutoff = 0.98  #this is the level below which we want to rerun or consider a curve
chk_flag = 0.10   #for the RSD (relative standard deviation)
chk_conc_flag = 15 #this is the level cutoff for percent difference of check standards vs. the concentration they are meant to be
chks_flag = 50 #this is the percent of chks we want to have a CV less than 10, usually 60
rep_flag = 25 #this is a 25% error between samples
#blank_flag - calculated based on samples later in this code as lower 25% quantile of sample concentrations
#critical reference material concentration - Update if running different checks: ** needs update
ic_crm = 22.19
#Top standard Concentrations- Update if running different standard curve:
top_std_c = 200
#Set time zone
common_tz = "Etc/GMT+5"
Sys.setenv(TZ = "America/New_York")
#plot indicators
site_order <- c('GCW', 'MSM', 'GWI', 'SWH')
plot_order <- c('UP', 'SWAMP', 'TR', 'WC', 'SW')
plot_colors <- c("#20063B", "darkgrey", "#FFBC42", "#419973", "#25ABE6" )
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
#make a new columns in the metadata with important info:
metadata <- raw_metadata %>%
mutate(Depth = paste0(Depth_cm, "cm")) %>%
mutate(LysID = paste0("Lys", Lysimeter)) %>%
mutate(YearMonth = sprintf("%d%02d", Year, Month))  %>%
mutate(Zone = case_when(
`Transect.Location` == "Transition" ~ "TR",
`Transect.Location` == "Wetland"    ~ "WC",
`Transect.Location` == "Upland"     ~ "UP",
`Transect.Location` == "Surface Water" ~ "SW",
TRUE                 ~ `Transect.Location`    # keep original value if no match
)) %>%
rename('DIC' = DIC......if..50mL.)
#Create DOC IDs from what was collected for comparison later
metadata <- metadata %>%
mutate(DIC_ID = ifelse(DIC == "x",
paste(YearMonth,
Site,
Zone,
LysID,
Depth,
sep = "_"),
NA) )
#Change the SW lines because they don't have lysimeters or a depth
metadata <- metadata %>%
mutate(
DIC_ID = if_else(
Zone == "SW",
# Modify the string:
DIC_ID %>%
str_replace("_LysA", "_A") %>%                    # Replace "_LysA" with "_A"
str_replace("_LysB", "_B") %>%                    # Replace "_LysB" with "_B"
str_replace("_LysC", "_C") %>%                    # Replace "_LysC" with "_C"
str_replace("_0cm$", ""),                          # Remove trailing "_0cm"
DIC_ID  # else keep original
)
)
#Take out the columns and rows that are not relevant
dic_metadata <- metadata %>%
select(DIC_ID, Year, Month, Day, YearMonth, Site, Zone, Lysimeter, LysID, Depth_cm,
Depth, Time..24hr., Time.Zone_EDT.EST, Field.Notes, ) %>%
# only keep specific columns
filter(!is.na(DIC_ID) & DIC_ID != "")  # remove missing/blank DIC_ID rows
#head(doc_metadata)
## Create a function to read in data from summary file:
read_data <- function(data){
# Second, read in data
read_delim(file = data, skip = 10, delim = "\t", show_col_types = FALSE) %>%
rename(sample_name = `Sample Name`,
ic_raw = `Result(IC)`,
# tdn_raw = `Result(TN)`,
run_datetime = `Date / Time`) %>%
select(sample_name, ic_raw,run_datetime)
}
## Create a function to read in data from all peaks file:
read_curve <- function(data){
# Second, read in data
read_delim(file = data, skip = 10, delim = "\t", show_col_types = FALSE) %>%
rename(sample_name = `Sample Name`,
analyte = `Analysis(Inj.)`,
concentration = `Conc.`,
area = Area,
manual_dilution = `Manual Dilution`,
excluded = Excluded,
run_datetime = `Date / Time`) %>%
filter(excluded == 0) %>% #filter to injections that are included in the analysis
select(sample_name, analyte, concentration, area, run_datetime) %>%
pivot_wider(names_from= analyte, values_from = concentration) %>%
rename(dic_raw = IC)
}
cat("Import Sample Data")
#Pull in data that from the raw data file based on the sample info input above
dat_raw <- raw_file_name %>%
map_df(read_data) %>%
filter(str_detect(sample_name, samples_pattern))
head(dat_raw)
cat("Assess the Standard Curves")
#filter standards out of the raw data
stds_all <- raw_allpeaks_name %>%
map_df(read_curve) %>%
filter(grepl("CalCurve", sample_name)) %>%
dplyr::rename(
standard_C_ppm = dic_raw) %>%
select(run_datetime,standard_C_ppm, area) %>%
bind_rows()
#separate by analyte
stds_C <- stds_all %>%
filter(!is.na(standard_C_ppm)) %>%
mutate(run_date = as.Date(strptime(run_datetime, format = "%m/%d/%Y %I:%M:%S %p")))
#calculate slope and r2 of cal curves
#ic curve
lm_results_c <- stds_C %>%
group_by(run_date) %>%
do({
model = lm(area ~ standard_C_ppm, data = .)
tidy_model = tidy(model)             # coefficients
glance_model = glance(model)         # model metrics like R²
tibble(
slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
intercept = tidy_model$estimate[1],
r2 = glance_model$adj.r.squared
)
}) %>%
mutate(
analyte = "C",
curve = "IC (mg/L)"
)
#put the together in one dataframe to later add to log
Slopes <- rbind(lm_results_c)
#store the r2's so they plot on the curve graphs
r2_labels_c <- stds_C %>%
group_by(run_date) %>%
summarise(
x_pos = max(standard_C_ppm, na.rm = TRUE) * 0.8,
y_pos = max(area, na.rm = TRUE),
r_squared = round(summary(lm(area ~ standard_C_ppm))$adj.r.squared, 4),
.groups = "drop"
)
##Plot standard Curve or Curves
#C Curve
C_stds_plot <- ggplot(stds_C, aes(x = standard_C_ppm, y = area)) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "darkred") +
facet_wrap(~ run_date) +
geom_text(
data = r2_labels_c,
aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
inherit.aes = FALSE,
hjust = 1, vjust = 1,
size = 4
) +
labs(
title = "IC Std Curve by Date",
x = "Carbon Standard Concentration (ppm)",
y = "Peak Area"
) +
theme_bw()
C_stds_plot
#compare slopes to previous runs (from log) in order to assess drift
log <- read.csv(Log_path)
log <- log[ ,-c(1)]
#make sure they both have dates as dates
log$run_date <- as.Date(log$run_date)
log$analyte <- as.character(log$analyte)
log$curve <- as.character(log$curve)
Slopes$run_date <- as.Date(Slopes$run_date)
# Filter to only rows in Slopes that are NOT already in log (by run_date + analyte)
new_rows <- anti_join(Slopes, log, by = c("run_date", "analyte"))
# Append the new, non-duplicate rows to log
log <- bind_rows(log, new_rows)
#plot the current slops with teh previous slopes
Slopes_chk <- ggplot(log, aes(run_date, slope, col=curve)) +
geom_point(size=4) +
geom_line() +
theme_bw() + labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
scale_color_manual(values=c("darkred"))
Slopes_chk
#write out the log file with the added lines for this run
write.csv(log, Log_path)
#Grab the highest r2 that is available for this run
r2_C = max(lm_results_c$r2)
#Write out to the user whether or not the r2 is above the cutoff of 0.98
ifelse(r2_C <= r2_cutoff,
"IC Curve r2 is below cutoff! - REASSESS", "IC Curve r2 GOOD")
#write out a flag to the sample dataframe if the r2 is above the cutoff of 0.98
dat_raw <- dat_raw %>%
mutate(
ic_flag = if (r2_C <= r2_cutoff) {
"IC r2 low"
} else {
""
}
)
##Plot standard Curve or Curves
#C Curve
C_stds_plot <- ggplot(stds_C, aes(x = standard_C_ppm, y = as.numeric(area))) +
geom_point(size = 3) +
geom_smooth(method = "lm", se = FALSE, color = "darkred") +
facet_wrap(~ run_date) +
geom_text(
data = r2_labels_c,
aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
inherit.aes = FALSE,
hjust = 1, vjust = 1,
size = 4
) +
labs(
title = "IC Std Curve by Date",
x = "Carbon Standard Concentration (ppm)",
y = "Peak Area"
) +
theme_bw()
C_stds_plot
library(dplyr)
library(ggplot2)
library(ggpubr)
library(stringr)
library(readxl)
library(tidyr)
library(tibble)
files <- list.files(path = "Processed Data", pattern = "\\.csv$", full.names = TRUE)
# Read and combine all CSVs
all_data <- files %>%
lapply(read.csv) %>%    # or read.csv if you prefer base R
bind_rows()             # combine into one data frame
all_data <- all_data[ ,-1]
all_data <- all_data %>%
mutate(Site = recode(Site, "GCrew" = "GCW")) %>%
mutate(Sample_ID = str_replace(Sample_ID, "GCrew", "GCW"))
gcw_up <- read.csv("COMPASS_SynopticCB_GCW_UP_TGAS_2022.csv")
View(gcw_up)
gcw_up <- read.csv("COMPASS_SynopticCB_GCW_UP_TGAS_2022.csv")
#pull out what we need
gcw_up1 <- gcw_up %>%
select( "Project", "Year","Month", "Day", "Sapflux_Tree_ID" ,
"Sample_ID","CH4_ppm", "CH4_Flag", "CO2_ppm", "CO2_Flag")
head(gcw_up1)
#clean it up and add necessary columns
gcw_up1 <- gcw_up1 %>%
mutate(Project = "COMPASS: Synoptic",
Region = "CB",
Site = "GCW",
Gas_Sample = "TGAS",
Zone = "UP",
Tree_Code = "SF") %>%
mutate(Replicate = str_extract(Sapflux_Tree_ID, "\\d+")) %>%
mutate(Tree_Info = case_when(
Tree_Code == "DS" ~ "Dead Standing",
Tree_Code == "SF" ~ "Sapflow Monitoring",
TRUE ~ "Other"  # Optional: handles any values that aren't DS or SF
)) %>%
mutate(Status = case_when(
Tree_Code == "DS" ~ "Dead Standing",
Tree_Code == "SF" ~ "Living",
TRUE ~ "Other"
)) %>%
mutate(CH4_Flag = case_when(
CH4_Flag == "Needs_Dilution" ~ "Over Std Curve Range",
TRUE ~ "Within Std Curve Range"
)) %>%
mutate(CO2_Flag = case_when(
CO2_Flag == "Needs_Dilution" ~ "Over Std Curve Range",
TRUE ~ "Within Std Curve Range"
))
#Put them in order and grab what we need
final_gcw_up <- gcw_up1 %>%
select( "Project", "Region" , "Year","Month" , "Day","Site", "Zone", "Gas_Sample",
"Sample_ID", "Tree_Code", "Replicate", "Status", "Tree_Info",
"CH4_ppm", "CH4_Flag", "CO2_ppm", "CO2_Flag")
#combine the TMP data with the rest of the data:
all_data_gcw <- rbind(all_data, final_gcw_up)
View(all_data)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(stringr)
library(readxl)
library(tidyr)
library(tibble)
library(openxlsx)
#read in 2022 TGAS Data:
dat22 <- read.csv("2022/COMPASS_SynopticCB_TGAS_2022.csv")
#need to deal with some of the tempest data that has the wrong replicate info
#a rep of 1 = C7
#a rep of 2 = C11
#a rep of 4 = C19
dat22$Replicate <- as.numeric(dat22$Replicate)
dat22 <- dat22 %>%
mutate(Replicate = case_when(
Site == "GCW" & Zone == "UP" & Replicate == 1 ~ 7,
Site == "GCW" & Zone == "UP" & Replicate == 2 ~ 11,
Site == "GCW" & Zone == "UP" & Replicate == 4 ~ 19,
TRUE ~ Replicate  # keep existing value otherwise
))
#read in 2023 TGAS Data:
dat23 <- read.csv("2023/COMPASS_SynopticCB_TGAS_2023.csv")
#read in the tree information about species and dbh
tree_dat <- read.csv("TGAS Tree Info/COMPASS_SynopticCB_TGAS_TreeInfo.csv")
head(tree_dat)
#combine the tree data with the 2022 File
tree_dat22 <- tree_dat %>%
select(!"DBH_2023_cm")
dat22_wtree <- left_join(dat22, tree_dat22, by = c("Site", "Zone", "Tree_Code", "Replicate"))
dat22_wtree <- dat22_wtree %>%
rename(DBH_cm = DBH_2022_cm)
#combine tree data with 2023 File
tree_dat23 <- tree_dat %>%
select(!"DBH_2022_cm")
dat23_wtree <- left_join(dat23, tree_dat23, by = c("Site", "Zone", "Tree_Code", "Replicate"))
dat23_wtree <- dat23_wtree %>%
rename(DBH_cm = DBH_2023_cm)
#read in the sampling date information
dates <- read.csv("TGAS Tree Info/COMPASS_Synoptic_TGAS_CollectionDates.csv")
head(dates)
#add sampling date and the month # to dat22
dat22_wtree_dates <- dat22_wtree %>%
left_join(dates %>% select(Site, Year, Month, Day),
by = c("Site", "Year", "Month")) %>%
mutate(Month_num = match(Month, month.name))
#add sampling date and the month # to dat23
dat23_wtree_dates <- dat23_wtree %>%
left_join(dates %>% select(Site, Year, Month, Day),
by = c("Site", "Year", "Month")) %>%
mutate(Month_num = match(Month, month.name))
#combine all the data together in one dataframe
all_dat <- rbind(dat22_wtree_dates, dat23_wtree_dates)
all_dat <- all_dat %>%
mutate(CO2_ppm = if_else(CO2_ppm < 0, 0, CO2_ppm),
CO2_ppm = round(CO2_ppm, 3)) %>%
mutate(CH4_ppm = if_else(CH4_ppm < 0, 0, CH4_ppm),
CH4_ppm = round(CH4_ppm, 3))
all_dat <- all_dat %>%
rename( Sapflux_ID = Replicate,
Month_name = Month,
Month = Month_num) %>%
mutate(CH4_Flag = recode(CH4_Flag, "Within Std Curve Range" = "NA")) %>%
mutate(CO2_Flag = recode(CO2_Flag, "Within Std Curve Range" = "NA")) %>%
select("Project", "Region", "Year", "Month", "Day", "Site", "Zone",
"Sapflux_ID", "Species",
"DBH_cm" ,  "Status",  "Tree_Info", "Sample_ID","CH4_ppm",
"CH4_Flag", "CO2_ppm","CO2_Flag")
#pull the column names from the all_dat dataframe and create a new dataframe with them
metadat <- data.frame(Column_Names = colnames(all_dat))
metadat$Description <- ""
metadat$Units <- ""
metadat$Instrument_ifapplicable <- ""
#add information about each line:
metadat$Description <- c("Project name: Project component",
"Region samples originated from: CB = Chesapeake Bay",
"Year (YYYY) of sample collection",
"Month (#) of sample collection",
"Day (#) of sample collection",
"Name of sampling transect location (GCW, SWH, MSM, GWI), see dataset locations for coordinates",
"Zone within space-for-time transect (UP=upland, TR=transition)",
"Sap flow (SF) id (1-8, 11, 19). The # ID for the sap flow sensor associated with the tree (only live trees)",
"USDA Species code for tree sampled",
"Diameter at breast height (DBH) for tree sampled in same year",
"Tree status indicates if the tree was Living or Dead Standing",
"Tree Info indicates if the tree had sap flow monitoring during sampling, dead trees were not monitored for sap flow",
"Sample identifier used in laboratory and field",
"Methane concentration of sample from tree gas well",
"Methane concentration flag: NA (data within instrument and std curve range) or Out of Std Curve Range.",
"Carbon Dioxide concentration of sample from tree gas well",
"Carbon Dioxide concentration flag: NA (data within instrument and std curve range) or Out of Std Curve Range." )
metadat$Units <-      c("NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"cm",
"NA",
"NA",
"NA",
"ppm",
"NA",
"ppm",
"NA" )
metadat$Instrument_ifapplicable <-  c("NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"NA",
"DBH tape",
"NA",
"NA",
"NA",
"Varian 450 GC (Agilent Technologies)",
"NA",
"Varian 450 GC (Agilent Technologies)",
"NA" )
#write out a csv of all the data to the main folder:
write.csv(all_dat, "COMPASS_SynopticCB_TGAS_AllData.csv")
#write out a csv of the metadata associated with the data:
write.csv(metadat, "COMPASS_SynopticCB_TGAS_Metadata.csv")
#just plot all the data
ggplot(data=all_dat, aes(x=Sapflux_ID, y=CH4_ppm, fill=Zone, color=Zone))+
geom_point()+
facet_grid(Site~Month, scales="free_y")+
theme_bw()
#Let's pull out dead trees and only plot the live ones
live_dat <- subset(all_dat, Status == "Living")
ggplot(data=live_dat, aes(x=Sapflux_ID, y=CH4_ppm, fill=Zone, color=Zone))+
geom_point()+
facet_grid(Site~Month, scales="free_y")+
theme_bw()
TGAS_avg <- live_dat %>%
group_by(Month, Site, Zone) %>%
dplyr::summarize(mean_CH4 = mean(CH4_ppm, na.rm=TRUE),
sd_CH4 = sd(CH4_ppm, na.rm=TRUE),
mean_CO2 = mean(CO2_ppm, na.rm=TRUE),
sd_CO2 = sd(CO2_ppm, na.rm=TRUE))
ggplot(data=TGAS_avg, aes(x=Month, y=mean_CH4, fill=Zone, color=Zone))+
geom_boxplot()+
facet_grid(~Site, scales="free_y")+
theme_bw()
ggplot(data=TGAS_avg, aes(x=Zone, y=mean_CH4, fill=Zone, color=Zone))+
geom_boxplot()+
facet_grid(~Site, scales="free_y")+
theme_bw()
#| include: false
library(rmarkdown)
library(readr)
library(lubridate)
library(ggplot2)
theme_set(theme_bw())
library(ggrepel)
library(readxl)
library(broom)
library(DT)
library(MASS)
library(tidyr)
library(dplyr)
# These columns are REQUIRED to be present in the metadata file(s)
# (NB there are can be other columns present; they will be carried through to the results)
METADATA_REQUIRED_FIELDS <- c("Date", "Time_start", "Plot", "Area", "Volume", "Temp")
# Data files must end with ".txt" and may be in sub-folders
data_files <- list.files(params$data_folder,
pattern = "*txt$",
full.names = TRUE, recursive = TRUE)
# Metadata files must end with ".xlsx" and may be in sub-folders
metadata_files <- list.files(params$metadata_folder,
pattern = "*xlsx$",
full.names = TRUE, recursive = TRUE)
errors <- 0 # error count
# Function to read a data file given by 'fn', filename
# Returns NULL if there's an error reading
read_ghg_data <- function(fn) {
basefn <- basename(fn)
message(Sys.time(), " Processing ", basefn)
dat_raw <- readLines(fn)
# Save the serial number in case we want to look at machine differences later
sn <- trimws(gsub("SN:\t", "", dat_raw[2], fixed = TRUE))
# Parse the timezone from the header and use it to make a TIMESTAMP field
tz <- trimws(gsub("Timezone:\t", "", dat_raw[5], fixed = TRUE))
# These files have five header lines, then the names of the columns in line 6,
# and then the column units in line 7. We only want the names
dat_raw <- dat_raw[-c(1:5, 7)]
# Irritatingly, the units line can repeat in the file (???!?). Remove these
dat_raw <- dat_raw[grep("DATAU", dat_raw, invert = TRUE)]
# Double irritatingly, if there's no remark, the software write \t\t, not
# \t""\t, causing a read error. Replace these instances
dat_raw <- gsub("\t\t", "\tnan\t", dat_raw, fixed = TRUE)
dat <- try({
readr::read_table(I(dat_raw), na = "nan", guess_max = 1e4)
})
# If the try() above succeeded, we haev a data frame and can process it
if(is.data.frame(dat)) {
message("\tRead in ", nrow(dat), " rows of data, ",
min(dat$DATE), " to ", max(dat$DATE))
message("\tInstrument serial number: ", sn)
dat$SN <- sn
message("\tInstrument time zone: ", tz)
# We record the time zone but don't convert the timestamps to it as
# that's not needed right now; metadata time is guaranteed to be same
dat$TIMESTAMP <- lubridate::ymd_hms(paste(dat$DATE, dat$TIME))
dat$TZ <- tz
# Remove unneeded Licor DATE and TIME columns
dat$DATE <- dat$TIME <- NULL
dat$Data_file <- basefn
return(dat)
} else {
warning("File read error for ", basefn)
errors <<- errors + 1
return(NULL)
}
}
dat <- lapply(data_files, read_ghg_data)
dat <- do.call("rbind", dat)
