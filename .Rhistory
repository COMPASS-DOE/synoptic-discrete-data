View(dionex_metadata)
View(metadata)
View(log)
View(merged_data)
View(final_data)
View(dionex_metadata)
############### THIS IS ONLY FOR 2023 when SWAMP was called UP and UPLAND was called UPCON ##############
df_all_clean <- merged_data %>%
#filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
#mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = Sample_Name,
sep = "_",
into = c("Sample_Number", "Site_Code", "Date" ,"Zone", "LysID", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(Samp_Time))
############### THIS IS ONLY FOR 2023 when SWAMP was called UP and UPLAND was called UPCON ##############
df_all_clean <- merged_data %>%
#filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
#mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = sample_name,
sep = "_",
into = c("Sample_Number", "Site_Code", "Date" ,"Zone", "LysID", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(Samp_Time))
############### THIS IS ONLY FOR 2023 when SWAMP was called UP and UPLAND was called UPCON ##############
df_all_clean <- merged_data %>%
#filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
#mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = sample_name,
sep = "_",
into = c("Sample_Number", "Site_Code", "Date" ,"Zone", "LysID", "Depth"),
remove = FALSE)
View(df_all_clean)
df_all_clean <- df_all_clean %>%
mutate(
Zone = case_when(
Site == "SWH" & Zone == "UP" ~ "SWAMP",
Site == "SWH" & Zone == "UPCON" ~ "UP",
TRUE ~ `Zone`
))
df_all_clean <- df_all_clean %>%
mutate(
Zone = case_when(
Site_Code == "SWH" & Zone == "UP" ~ "SWAMP",
Site_Code == "SWH" & Zone == "UPCON" ~ "UP",
TRUE ~ `Zone`
))
#Plot samples to get a first look at concentrations (sanity check)
data_plotting <- df_all_clean
#group the data for plotting
data_plotting <- data_plotting %>%
group_by(Site_Code) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#Plot data and change colors based on Zone:
viz_cl_plot <- ggplot(data_plotting, aes(x = row_num, y = Cl_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
theme(axis.text.x = element_blank()) +
labs(x = " ", y = "Cl (mg/L)", title = "Samples: Chloride") +
scale_x_discrete(drop = TRUE)
viz_so4_plot <-  ggplot(data_plotting, aes(x = row_num, y = SO4_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
theme(axis.text.x = element_blank()) +
labs(x = " ", y = "SO4 (mg/L)", title = "Samples: SO4") +
scale_x_discrete(drop = TRUE)
ggarrange(viz_cl_plot, viz_so4_plot, nrow=2, ncol=1)
#group the data for plotting
data_plotting <- data_plotting %>%
group_by(Site_Code) %>%
Zone = factor(Zone, levels = c("UP", "TR", "SWAMP", "WC", "SW")) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
View(data_plotting)
#group the data for plotting
data_plotting <- data_plotting %>%
group_by(Site_Code) %>%
data_plotting$Zone = factor(data_plotting$Zone, levels = c("UP", "TR", "SWAMP", "WC", "SW")) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#group the data for plotting
data_plotting <- data_plotting %>%
group_by(Site_Code) %>%
data_plotting$Zone = factor(data_plotting$Zone, levels = c("UP", "TR", "SWAMP", "WC", "SW")) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#group the data for plotting
data_plotting$Zone = factor(data_plotting$Zone, levels = c("UP", "TR", "SWAMP", "WC", "SW")) %>%
data_plotting <- data_plotting %>%
group_by(Site_Code) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#group the data for plotting
data_plotting$Zone = factor(data_plotting$Zone, levels = c("UP", "TR", "SWAMP", "WC", "SW"))
data_plotting <- data_plotting[order(data_plotting$Zone), ]
data_plotting <- data_plotting %>%
group_by(Site_Code) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
#Plot data and change colors based on Zone:
viz_cl_plot <- ggplot(data_plotting, aes(x = row_num, y = Cl_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
theme(axis.text.x = element_blank()) +
labs(x = " ", y = "Cl (mg/L)", title = "Samples: Chloride") +
scale_x_discrete(drop = TRUE)
viz_so4_plot <-  ggplot(data_plotting, aes(x = row_num, y = SO4_ppm, fill = Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
facet_grid(~ Site_Code, scales="free_x") +
scale_fill_manual(values = c(
"UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6"
)) +
theme_classic() +
theme(axis.text.x = element_blank()) +
labs(x = " ", y = "SO4 (mg/L)", title = "Samples: SO4") +
scale_x_discrete(drop = TRUE)
ggarrange(viz_cl_plot, viz_so4_plot, nrow=2, ncol=1)
#Prepare data to be exported - if there is anything else to add
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
#example read in sample IDs list and merge
#create required ID columns in R, etc.
final_data <- df_all_clean %>%
mutate(
Project = "COMPASS: Synoptic",   # new column with same value on every row
Region = "CB",
Run_notes = run_notes     # new column with notes about the run
)
colnames(final_data)
final_data <- final_data %>%
rename(
Time = Time..24hr.,
Time_Zone = Time.Zone_EDT.EST,
Replicate = Lysimeter,
Sample_ID = sample_ID,
Field_notes = Field.Notes
# add more rename pairs as needed
) %>%
select(
Sample_ID, SO4_mM, Cl_mM, salinity, Cl_flag, SO4_flag, Year, Month, Day, Depth_cm, Replicate, Time, Time_Zone, Run_notes, Field_notes
# list columns in the order you want them
)
head(final_data)
#will put final data in processed data folder
#Write out data frame
write.csv(final_data, processed_file_name)
#Prepare data to be exported - if there is anything else to add
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
#example read in sample IDs list and merge
#create required ID columns in R, etc.
final_data <- df_all_clean %>%
mutate(
Project = "COMPASS: Synoptic",   # new column with same value on every row
Region = "CB",
Run_notes = run_notes     # new column with notes about the run
)
colnames(final_data)
View(final_data)
Analysis_r
cat("Export Processed Data")
#Prepare data to be exported - if there is anything else to add
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
#example read in sample IDs list and merge
#create required ID columns in R, etc.
final_data <- df_all_clean %>%
mutate(
Project = "COMPASS: Synoptic",   # new column with same value on every row
Region = "CB",
Run_notes = run_notes,     # new column with notes about the run
Analysis_rundate = print(run_date)
)
#Prepare data to be exported - if there is anything else to add
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
#example read in sample IDs list and merge
#create required ID columns in R, etc.
final_data <- df_all_clean %>%
mutate(
Project = "COMPASS: Synoptic",   # new column with same value on every row
Region = "CB",
Run_notes = run_notes,     # new column with notes about the run
Analysis_rundate = print(Date_Run)
)
colnames(final_data)
final_data <- final_data %>%
rename(
Time = Time..24hr.,
Time_Zone = Time.Zone_EDT.EST,
Replicate = Lysimeter,
Sample_ID = sample_ID,
Field_notes = Field.Notes,
Site = Site_Code
# add more rename pairs as needed
) %>%
select(
Project, Region, Site, Zone, Replicate, Depth_cm,
Sample_ID, Year, Month, Day, Time, Time_Zone,
SO4_mM, Cl_mM, salinity, Cl_flag, SO4_flag,
Analysis_rundate,  Run_notes, Field_notes
# list columns in the order you want them
)
head(final_data)
View(final_data)
View(Perc_spks)
#report out if the dups are within range
ifelse(Perc_spks$Percent >= chks_flag, ">80% of SO4 spikes have a recovery between the high and low cutoff - PROCEED",
">80% of SO4 spikes have a recovery between the high and low cutoff - REASSESS")
chks_flag
#report out if the dups are within range
ifelse(Perc_spks$Percent/100 >= chks_flag, ">80% of SO4 spikes have a recovery between the high and low cutoff - PROCEED",
">80% of SO4 spikes have a recovery between the high and low cutoff - REASSESS")
Perc_spks$Percent/100
#write out a flag to the sample dataframe if less than 80% of the SO4 spikes are within range
if (Perc_spks$Percent/100 <= chks_flag) {
all_dat$SO4_flag <- ifelse(
all_dat$SO4_flag != "",
paste0(all_dat$Cl_flag, "; SO4 spikes out of range"),
"SO4 spikes out of range"
)
}
View(all_dat)
#write out a flag to the sample dataframe if less than 80% of the SO4 spikes are within range
if (Perc_spks$Percent/100 <= chks_flag) {
all_dat$SO4_flag <- ifelse(
all_dat$SO4_flag != "",
paste0(all_dat$SO4_flag, "; SO4 spikes out of range"),
"SO4 spikes out of range"
)
}
cat("Sample Flagging")
#Undilute samples
all_dat$Cl_ppm_undil <- all_dat$Cl_ppm/all_dat$Dilution
all_dat$SO4_ppm_undil <- all_dat$SO4_ppm/all_dat$Dilution
#Remove standards
dat_flagged <- all_dat %>%
filter(!str_detect(sample_ID, "Standard"))
#Flagging data if the concentration is outside the standards range and based on blanks
dat_flagged <- dat_flagged %>%
mutate(
Cl_flag = if_else(Cl_ppm_undil > top_std_cl,
if_else(Cl_flag != "" & !is.na(Cl_flag),
paste0(Cl_flag, "; value above cal curve"), "value above cal curve"), Cl_flag),
SO4_flag = if_else(SO4_ppm_undil > top_std_so4,
if_else(SO4_flag != "" & !is.na(SO4_flag),
paste0(SO4_flag, "; value above cal curve"), "value above cal curve"), SO4_flag) )
#Plot data and change colors based on flags to check it:
cl_samples_flag <-  ggplot(data = dat_flagged, aes(x = sample_name, y = Cl_ppm_undil, fill=Cl_flag)) +
geom_bar(stat = 'identity') +
scale_fill_manual(values=c("darkgreen", "red"))+
theme_classic() + labs(x= " ", y="C (mg/L)", title="Cl: Green = Within Range of Curve") +
theme(legend.position="none") +
theme(axis.text.x = element_blank())
so4_samples_flag <-  ggplot(data = dat_flagged, aes(x = sample_name, y = SO4_ppm_undil, fill=SO4_flag)) +
geom_bar(stat = 'identity') +
scale_fill_manual(values=c("darkgreen", "red"))+
theme_classic() + labs(x= " ", y="N (mg/L)", title="SO4: Green = Within Range of Curve") +
theme(legend.position="none") +
theme(axis.text.x = element_blank())
ggarrange(cl_samples_flag, so4_samples_flag, nrow=1, ncol=2)
View(log)
log
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read.csv(Log_path)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
log <- log[ ,-c(1)]
log
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read.csv(Log_path)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read.csv(Log_path, row.names = FALSE)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read.csv(Log_path, row.names = NULL)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read_csv(Log_path, row.names = NULL)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read_csv(Log_path, row.names = FALSE)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read.csv(Log_path, row.names = FALSE)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(log) <- c( "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#compare slopes to previous runs (from log) in order to assess drift
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(Log_path)) {
# If it exists, read it back into R
log <- read.csv(Log_path)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
log <- as.data.frame(matrix(ncol = 7, nrow = 0))
colnames(log) <- c( "X", "r2", "analyte", "curve", "run_date", "slope", "intercept")
# Write add_log to CSV
write.csv(log, file = Log_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
log <- log[ ,-c(1)]
#Packages that are required
lapply(c(
"dplyr", "ggplot2", "ggpubr", "stringr",
"purrr", "tidyverse", "here", "broom", "tibble",
"googledrive", "googlesheets4", "data.table",
"matrixStats", "gridExtra", "grid", "tidyverse", "formatR"),
library, character.only = TRUE)
#Packages that are required
lapply(c(
"dplyr", "ggplot2", "ggpubr", "stringr",
"purrr", "tidyverse", "here", "broom", "tibble",
"googledrive", "googlesheets4", "data.table",
"matrixStats", "gridExtra", "grid", "tidyverse", "knitr"),
library, character.only = TRUE)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
#identify section
cat("Setup Information")
###### Run information - PLEASE CHANGE
Date_Run = "2023-09-05"  #Date that instrument was run
Run_by = "Unknown"  #Instrument user
Script_run_by = "Zoe Read" #Code user
run_notes = "Samples missing from metadata: MSM_202311_UP_LYSC_45CM MSM_202311_WC_LYSB_45CM MSM_202311_WC_LYSC_45CM      MSM_202311_UP_RHZ_SF_TREE_1 MSM_202311_UP_RHZ_SF_TREE_2   MSM_202311_UP_RHZ_SF_TREE_3   MSM_202311_UP_RHZ_SF_TREE_4  MSM_202311_UP_RHZ_SF_TREE_6   MSM_202311_UP_RHZ_SF_TREE_7   MSM_202311_UP_RHZ_SF_TREE_8   MSM_202311_TR_RHZ_SF_TREE_   MSM_202311_TR_RHZ_SF_TREE_2   MSM_202311_TR_RHZ_SF_TREE_3 MSM_202311_TR_RHZ_SF_TREE_4  MSM_202311_TR_RHZ_SF_TREE_5  MSM_202311_TR_RHZ_SF_TREE_6  MSM_202311_TR_RHZ_SF_TREE_7   MSM_202311_TR_RHZ_SF_TREE_8 MSM_202311_WC_RHZ_SF_COLLAR_1 MSM_202311_WC_RHZ_SF_COLLAR_2 MSM_202311_WC_RHZ_SF_COLLAR_3 MSM_202311_WC_RHZ_SF_COLLAR_4 MSM_202311_WC_RHZ_SF_COLLAR_5 MSM_202311_WC_RHZ_LYSA
MSM_202311_WC_RHZ_LYSC. 118_MSM_202311_TR_RHZ_SF_Tree_1 value above cal curve for SO4, but only slightly." #any notes from the run
samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names
samples_pattern <- paste(samples, collapse = "|")
#samples_pattern <- "GCW" #use this instead of the line above if you have only one site code
chks_name = "Check Standard"  #what did you name your check standards?
###### File Names - PLEASE CHANGE
#file path and name for raw summary data file
# raw_file_name_cl = "Porewater/Sulfate_Chloride/Synoptic_CB/2023/Raw Data/COMPASS_Synoptic_CB_MonMon_202311_Cl.txt"
# raw_file_name_so4 = "Porewater/Sulfate_Chloride/Synoptic_CB/2023/Raw Data/COMPASS_Synoptic_CB_MonMon_202311_SO4.txt"
raw_file_name_cl = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_Cl.txt"
raw_file_name_so4 = "Raw Data/COMPASS_Synoptic_CB_MonMon_202311_SO4.txt"
#file path and name of processed data file
# processed_file_name = "Porewater/Sulfate_Chloride/Synoptic_CB/2023/Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202311.csv"
processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202311.csv"
###### Log Files - PLEASE CHECK
#downloaded metadata csv - downloaded from Google drive as csv for this year
# Raw_Metadata = "Porewater/Sulfate_Chloride/Synoptic_CB/2023/Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#qaqc log file path for this year
# Log_path = "Porewater/Sulfate_Chloride/Synoptic_CB/2023/Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
Log_path = "Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
install.packages(formatR)
install.packages("formatR")
cat("Sample Flagging")
#Undilute samples
all_dat$Cl_ppm_undil <- all_dat$Cl_ppm/all_dat$Dilution
all_dat$SO4_ppm_undil <- all_dat$SO4_ppm/all_dat$Dilution
#Remove standards
dat_flagged <- all_dat %>%
filter(!str_detect(sample_ID, "Standard"))
#Plot data and change colors based on flags to check it:
cl_samples_flag <-  ggplot(data = dat_flagged, aes(x = sample_name, y = Cl_ppm_undil, fill=Cl_flag)) +
geom_bar(stat = 'identity') +
scale_fill_manual(values=c("darkgreen", "red"))+
theme_classic() + labs(x= " ", y="C (mg/L)", title="Cl: Green = Within Range of Curve") +
theme(legend.position="none") +
theme(axis.text.x = element_blank())
so4_samples_flag <-  ggplot(data = dat_flagged, aes(x = sample_name, y = SO4_ppm_undil, fill=SO4_flag)) +
geom_bar(stat = 'identity') +
scale_fill_manual(values=c("darkgreen", "red"))+
theme_classic() + labs(x= " ", y="N (mg/L)", title="SO4: Green = Within Range of Curve") +
theme(legend.position="none") +
theme(axis.text.x = element_blank())
ggarrange(cl_samples_flag, so4_samples_flag, nrow=1, ncol=2)
#Flagging data if the concentration is outside the standards range and based on blanks
dat_flagged <- dat_flagged %>%
mutate(
Cl_flag = if_else(Cl_ppm_undil > top_std_cl,
if_else(Cl_flag != "" & !is.na(Cl_flag),
paste0(Cl_flag, "; value above cal curve"), "value above cal curve"), Cl_flag),
SO4_flag = if_else(SO4_ppm_undil > top_std_so4,
if_else(SO4_flag != "" & !is.na(SO4_flag),
paste0(SO4_flag, "; value above cal curve"), "value above cal curve"), SO4_flag) )
cat("Sample Flagging")
#Undilute samples
all_dat$Cl_ppm_undil <- all_dat$Cl_ppm/all_dat$Dilution
all_dat$SO4_ppm_undil <- all_dat$SO4_ppm/all_dat$Dilution
#Remove standards
dat_flagged <- all_dat %>%
filter(!str_detect(sample_ID, "Standard"))
#Flagging data if the concentration is outside the standards range and based on blanks
dat_flagged <- dat_flagged %>%
mutate(
Cl_flag = if_else(Cl_ppm_undil > top_std_cl,
if_else(Cl_flag != "" & !is.na(Cl_flag),
paste0(Cl_flag, "; value above cal curve"), "value above cal curve"), Cl_flag),
SO4_flag = if_else(SO4_ppm_undil > top_std_so4,
if_else(SO4_flag != "" & !is.na(SO4_flag),
paste0(SO4_flag, "; value above cal curve"), "value above cal curve"), SO4_flag) )
#Plot data and change colors based on flags to check it:
cl_samples_flag <-  ggplot(data = dat_flagged, aes(x = sample_name, y = Cl_ppm_undil, fill=Cl_flag)) +
geom_bar(stat = 'identity') +
scale_fill_manual(values=c("darkgreen", "red"))+
theme_classic() + labs(x= " ", y="C (mg/L)", title="Cl: Green = Within Range of Curve") +
theme(legend.position="none") +
theme(axis.text.x = element_blank())
so4_samples_flag <-  ggplot(data = dat_flagged, aes(x = sample_name, y = SO4_ppm_undil, fill=SO4_flag)) +
geom_bar(stat = 'identity') +
scale_fill_manual(values=c("darkgreen", "red"))+
theme_classic() + labs(x= " ", y="N (mg/L)", title="SO4: Green = Within Range of Curve") +
theme(legend.position="none") +
theme(axis.text.x = element_blank())
ggarrange(cl_samples_flag, so4_samples_flag, nrow=1, ncol=2)
