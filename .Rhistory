!is.na(Sample_Name),                                         # Remove NA
!grepl("^\\d+$", Sample_Name),                               # removes all numbers
!grepl("^\\d{1,2}/\\d{1,2}/\\d{2,4}", Sample_Name)) %>%      # removes all date/time
mutate(`Duplicate?` = if_else(                                  # adds duplicate column
grepl("Duplicate", Sample_Name, ignore.case = TRUE),         # keeps sample ID if there
lag(Sample_Name),                                            # get previous row's SampleID
Sample_Name)) %>%                                            # keeps sample ID if not duplicate
group_by(`Duplicate?`) %>%                                      # groups by duplicate
filter(n() > 1) %>%                                             # filters anything that appears more than once
ungroup() %>%
transmute(
Pair_ID = `Duplicate?`,                                      # changing heading to pair_id
Sample_Name,                                                 #keeping sample name and conc
Conc,                                                        #if sample name duplicate then put duplicate
Type = if_else(str_detect(Sample_Name, "Duplicate"), "Duplicate", "Original"))  #, if not put original to ID
df_PO4_dup <- df_all_cor %>%
filter(Test == "o-PHOS 0.3") %>%
filter(
!grepl("CCV|CCB|Standard|Auto Spike|Blank|1mg/L ammonia|Abs_Chk_20ppt|Abs_Chk_10ppt|
0.15 mg/L", Sample_Name, ignore.case = TRUE),
#^removes autospice, ccv, ccb, standards
!is.na(Sample_Name),                                         # Remove NA
!grepl("^\\d+$", Sample_Name),                               # removes all numbers
!grepl("^\\d{1,2}/\\d{1,2}/\\d{2,4}", Sample_Name)) %>%      # removes all date/time
mutate(`Duplicate?` = if_else(                                  # adds duplicate column
grepl("Duplicate", Sample_Name, ignore.case = TRUE),         # keeps sample ID if there
lag(Sample_Name),                                            # get previous row's SampleID
Sample_Name)) %>%                                            # keeps sample ID if not duplicate
group_by(`Duplicate?`) %>%                                      # groups by duplicate
filter(n() > 1) %>%                                             # filters anything that appears more than once
ungroup() %>%
transmute(
Pair_ID = `Duplicate?`,                                      # changing heading to pair_id
Sample_Name,                                                 #keeping sample name and conc
Conc,                                                        #if sample name duplicate then put duplicate
Type = if_else(str_detect(Sample_Name, "Duplicate"), "Duplicate", "Original"))
# Duplicate Stats
Nox_dup_chk <- df_NOx_dup %>%
group_by(Pair_ID) %>%  # Use Pair_ID as it groups Originals and Duplicates
filter(n_distinct(Type) == 2) %>%  # Keep only groups that have both Original and Duplicate
summarise(
OG_Conc = Conc[Type == "Original"],
Dup_Conc = Conc[Type == "Duplicate"],
Mean_Conc = mean(c(OG_Conc, Dup_Conc)),
SD_Conc = sd(c(OG_Conc, Dup_Conc)),
CV = (SD_Conc / Mean_Conc) * 100,
pct_diff = abs(Dup_Conc - OG_Conc) / Mean_Conc * 100,
NOx_diff_flag = ifelse(abs(CV) < 10, "YES", "NO, rerun"),
.groups = "drop")
NH3_dup_chk <- df_NH3_dup %>%
group_by(Pair_ID) %>%  # Use Pair_ID as it groups Originals and Duplicates
filter(n_distinct(Type) == 2) %>%  # Keep only groups that have both Original and Duplicate
summarise(
OG_Conc = Conc[Type == "Original"],
Dup_Conc = Conc[Type == "Duplicate"],
Mean_Conc = mean(c(OG_Conc, Dup_Conc)),
SD_Conc = sd(c(OG_Conc, Dup_Conc)),
CV = (SD_Conc / Mean_Conc) * 100,
pct_diff = abs(Dup_Conc - OG_Conc) / Mean_Conc * 100,
NH3_diff_flag = ifelse(abs(CV) < 10, "YES", "NO, rerun"),
.groups = "drop")
PO4_dup_chk <- df_PO4_dup %>%
group_by(Pair_ID) %>%  # Use Pair_ID as it groups Originals and Duplicates
filter(n_distinct(Type) == 2) %>%  # Keep only groups that have both Original and Duplicate
summarise(
OG_Conc = Conc[Type == "Original"],
Dup_Conc = Conc[Type == "Duplicate"],
Mean_Conc = mean(c(OG_Conc, Dup_Conc)),
SD_Conc = sd(c(OG_Conc, Dup_Conc)),
CV = (SD_Conc / Mean_Conc) * 100,
pct_diff = abs(Dup_Conc - OG_Conc) / Mean_Conc * 100,
PO4_diff_flag = ifelse(abs(CV) < 10, "YES", "NO, rerun"),
.groups = "drop")
#check to see if NOx dups are below the detection limit, this is very common for NOx and leads to
# very high cvs between dups, so we will accept them if the samples are below detection
Nox_dup_chk <- Nox_dup_chk %>%
mutate(NOx_diff_flag = ifelse(OG_Conc < NOx_dl, "bdl", NOx_diff_flag))
#calculate the percent of check standards that are within the range based on the flag
NOx_dup_percent <- (sum(Nox_dup_chk$NOx_diff_flag %in% c("YES", "bdl")) / nrow(Nox_dup_chk)) * 100
NH3_dup_percent <- (sum(NH3_dup_chk$NH3_diff_flag == "YES")/nrow(NH3_dup_chk))*100
PO4_dup_percent <- (sum(PO4_dup_chk$PO4_diff_flag == "YES")/nrow(PO4_dup_chk))*100
#dup flag report out
ifelse(NOx_dup_percent >= chks_flag,
">60% of NOx Duplicates have a CV <10% - PROCEED",
"<60% of NOx Duplicates have a CV <10% - REASSESS")
ifelse(NH3_dup_percent >= chks_flag,
">60% of NH3 Duplicates have a CV <10% - PROCEED",
"<60% of NH3 Duplicates have a CV <10% - REASSESS")
ifelse(PO4_dup_percent >= chks_flag,
">60% of PO4 Duplicates have a CV <10% - PROCEED",
"<60% of PO4 Duplicates have a CV <10% - REASSESS")
#Create row numbers for plotting
Nox_dup_chk <- Nox_dup_chk %>%
mutate(row_num = row_number())
NH3_dup_chk <- NH3_dup_chk %>%
mutate(row_num = row_number())
PO4_dup_chk <- PO4_dup_chk %>%
mutate(row_num = row_number())
#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this
NOx_dups_plot <- ggplot(data =Nox_dup_chk, aes(x =Pair_ID, y =CV, fill=NOx_diff_flag)) +
geom_bar(stat = 'identity') +
theme_classic() +
labs(x= "Sample ID", y="CV between dups", title = "NOx Duplicates") +
scale_fill_manual(values = c("bdl" = "darkslateblue","YES" = "darkslateblue",  "NO, rerun" = "darkred")) +
theme(legend.position="none") +
geom_hline(yintercept=10, linetype="dashed",
color = "black", size=1)  +
guides(fill=guide_legend(title="<10%:"))+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
NH3_dups_plot <- ggplot(data = NH3_dup_chk, aes(x =Pair_ID, y =CV, fill=NH3_diff_flag)) +
geom_bar(stat = 'identity') +
theme_classic() +
labs(x= "Sample ID", y="CV between dups", title = "NH3 Duplicates") +
scale_fill_manual(values = c("YES" = "darkslateblue", "NO, rerun" = "darkred")) +
theme(legend.position="none") +
geom_hline(yintercept=10, linetype="dashed",
color = "black", size=1) +
guides(fill=guide_legend(title="<10%:"))+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
PO4_dups_plot <- ggplot(data =PO4_dup_chk, aes(x =Pair_ID, y =CV, fill=PO4_diff_flag)) +
geom_bar(stat = 'identity') +
theme_classic() +
labs(x= "Sample ID", y="CV between dups", title = "PO4 Duplicates") +
scale_fill_manual(values = c("YES" = "darkslateblue", "NO, rerun" = "darkred")) +
theme(legend.position="none") +
geom_hline(yintercept=10, linetype="dashed",
color = "black", size=1) +
guides(fill=guide_legend(title="<10%:"))+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
#Combined Plot
Dups_Plots <- ggarrange(
NOx_dups_plot,
NH3_dups_plot,
PO4_dups_plot,
nrow = 1,
ncol = 3,
common.legend = FALSE,
legend = "bottom")
print(Dups_Plots)
#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range
if (NOx_dup_percent <= chks_flag) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; NOx dups out of range"),
"NOx dups out of range"
)
}
if (NH3_dup_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
df_all_cor$NH3_flag <- ifelse(
df_all_cor$NH3_flag != "",
paste0(df_all_cor$NH3_flag, "; NH3 dups out of range"),
"NH3 dups out of range"
)
}
if (PO4_dup_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
df_all_cor$PO4_flag <- ifelse(
df_all_cor$PO4_flag != "",
paste0(df_all_cor$PO4_flag, "; PO4 dups out of range"),
"PO4 dups out of range"
)
}
#getting spikes and samples pulled
df_NOx_spk <- df_all_cor %>%
filter(Test == "Vanadium NOx") %>%
filter(
!grepl("CCV|CCB|Standard|Duplicate", Sample_Name, ignore.case = TRUE),
#^removes dup, ccv, ccb, standards
!is.na(Sample_Name),                                     # Remove NA
!grepl("^\\d+$", Sample_Name),                           # removes all numbers
!grepl("^\\d{1,2}/\\d{1,2}/\\d{2,4}", Sample_Name)) %>%  # removes all date/time
mutate(`Spike?` = if_else(                                  # adds duplicate column
grepl("Auto Spike", Sample_Name, ignore.case = TRUE),    # keeps sample ID if there
lag(Sample_Name),                                        # get previous row's SampleID
Sample_Name)) %>%                                        # keeps sample ID if not duplicate
group_by(`Spike?`) %>%                                      # groups by duplicate
filter(n() > 1) %>%                                         # filters anything that appears more than once
ungroup()  %>%
transmute(
Pair_ID = `Spike?`,
Sample_Name,
Conc,
Type = if_else(str_detect(Sample_Name, "Auto Spike"), "Auto_Spike", "Original"))
df_NH3_spk <- df_all_cor %>%
filter(Test == "Ammonia 2") %>%
filter(
!grepl("CCV|CCB|Blank|1mg/L ammonia|Standard|Duplicate", Sample_Name, ignore.case = TRUE),
#^removes dup, ccv, ccb, standards
!is.na(Sample_Name),                                         # Remove NA
!grepl("^\\d+$", Sample_Name),                               # removes all numbers
!grepl("^\\d{1,2}/\\d{1,2}/\\d{2,4}", Sample_Name)) %>%      # removes all date/time
mutate(`Spike?` = if_else(                                      # adds duplicate column
grepl("Auto Spike", Sample_Name, ignore.case = TRUE),        # keeps sample ID if there
lag(Sample_Name),                                            # get previous row's SampleID
Sample_Name)) %>%                                            # keeps sample ID if not duplicate
group_by(`Spike?`) %>%                                          # groups by duplicate
filter(n() > 1) %>%                                             # filters anything that appears more than once
ungroup()  %>%
transmute(
Pair_ID = `Spike?`,
Sample_Name,
Conc,
Type = if_else(str_detect(Sample_Name, "Auto Spike"), "Auto_Spike", "Original"))
df_PO4_spk <- df_all_cor %>%
filter(Test == "o-PHOS 0.3") %>%
filter(
!grepl("CCV|CCB|Standard|Duplicate", Sample_Name, ignore.case = TRUE),
#^removes dup, ccv, ccb, standards
!is.na(Sample_Name),                                         # Remove NA
!grepl("^\\d+$", Sample_Name),                               # removes all numbers
!grepl("^\\d{1,2}/\\d{1,2}/\\d{2,4}", Sample_Name)) %>%      # removes all date/time
mutate(`Spike?` = if_else(                                     # adds duplicate column
grepl("Auto Spike", Sample_Name, ignore.case = TRUE),        # keeps sample ID if there
lag(Sample_Name),                                            # get previous row's SampleID
Sample_Name)) %>%                                            # keeps sample ID if not duplicate
group_by(`Spike?`) %>%                                          # groups by duplicate
filter(n() > 1) %>%                                             # filters anything that appears more than once
ungroup()  %>%
transmute(
Pair_ID = `Spike?`,
Sample_Name,
Conc,
Type = if_else(str_detect(Sample_Name, "Auto Spike"), "Auto_Spike", "Original"))
#checking and flagging against the expected spike conc
df_spk_NOx_chk <- df_NOx_spk %>%
select(Pair_ID, Conc, Type) %>%
pivot_wider(
names_from = Type,
values_from = Conc,
values_fn = mean,  # in case there are duplicates
names_prefix = "" ) %>%
rename(
Spike_Conc = Auto_Spike,
OG_Conc = Original) %>%
filter(!is.na(Spike_Conc), !is.na(OG_Conc)) %>%
rowwise() %>%# keep only rows with both
mutate(
Exp_Conc = OG_Conc + NOx_Spk,
Mean_Conc = mean(c(Spike_Conc, Exp_Conc), na.rm = TRUE),
Pct_Diff = abs(Spike_Conc - Exp_Conc) / Mean_Conc * 100,
SD_Conc = sd(c(Spike_Conc, Exp_Conc)),
CV = (SD_Conc / Mean_Conc) * 100,
Spike_diff_flag = ifelse(CV < 50, "YES", "NO, check"),
Test = "Vanadium NOx") %>%
ungroup()
df_spk_NH3_chk <- df_NH3_spk %>%
select(Pair_ID, Conc, Type) %>%
pivot_wider(
names_from = Type,
values_from = Conc,
values_fn = mean,  # in case there are duplicates
names_prefix = "" ) %>%
rename(
Spike_Conc = Auto_Spike,
OG_Conc = Original) %>%
filter(!is.na(Spike_Conc), !is.na(OG_Conc)) %>%
rowwise() %>%# keep only rows with both
mutate(
Exp_Conc = OG_Conc + NH3_Spk,
Mean_Conc = mean(c(Spike_Conc, Exp_Conc), na.rm = TRUE),
Pct_Diff = abs(Spike_Conc - Exp_Conc) / Mean_Conc * 100,
SD_Conc = sd(c(Spike_Conc, Exp_Conc)),
CV = (SD_Conc / Mean_Conc) * 100,
Spike_diff_flag = ifelse(CV < 50, "YES", "NO, check"),
Test = "Ammonia 2") %>%
ungroup()
df_spk_PO4_chk <- df_PO4_spk %>%
select(Pair_ID, Conc, Type) %>%
pivot_wider(
names_from = Type,
values_from = Conc,
values_fn = mean,  # in case there are duplicates
names_prefix = "" ) %>%
rename(
Spike_Conc = Auto_Spike,
OG_Conc = Original) %>%
filter(!is.na(Spike_Conc), !is.na(OG_Conc)) %>%
rowwise() %>%# keep only rows with both
mutate(
Exp_Conc = OG_Conc + PO4_Spk,
Mean_Conc = mean(c(Spike_Conc, Exp_Conc), na.rm = TRUE),
Pct_Diff = abs(Spike_Conc - Exp_Conc) / Mean_Conc * 100,
SD_Conc = sd(c(Spike_Conc, Exp_Conc)),
CV = (SD_Conc / Mean_Conc) * 100,
Spike_diff_flag = ifelse(CV < 50, "YES", "NO, check"),
Test = "o-PHOS 0.3") %>%
ungroup()
# Combining Auto Spike data frames
df_spk_combo <- bind_rows(df_spk_NOx_chk, df_spk_NH3_chk, df_spk_PO4_chk)
#add column for test ^^ aand then combine
#calculate the percent of check standards that are within the range based on the flag
NOx_spk_percent <- (sum(df_spk_NOx_chk$Spike_diff_flag == "YES")/nrow(df_spk_NOx_chk))*100
NH3_spk_percent <- (sum(df_spk_NH3_chk$Spike_diff_flag == "YES")/nrow(df_spk_NH3_chk))*100
PO4_spk_percent <- (sum(df_spk_PO4_chk$Spike_diff_flag == "YES")/nrow(df_spk_PO4_chk))*100
#spk flag report out
ifelse(NOx_spk_percent >= chks_flag,
">60% of Spikes have a CV <50% - PROCEED",
"<60% of Carbon Spikes have a CV <50% - REASSESS")
ifelse(NH3_spk_percent >= chks_flag,
">60% of Spikes have a CV <50% - PROCEED",
"<60% of Carbon Spikes have a CV <50% - REASSESS")
ifelse(PO4_spk_percent >= chks_flag,
">60% of Spikes have a CV <50% - PROCEED",
"<60% of Carbon Spikes have a CV <50% - REASSESS")
#plot spikes output as a bar graph to easily check - want any over 10% to be red need to work on this
NOx_spk_plot <- ggplot(data =df_spk_NOx_chk, aes(x =Pair_ID, y =CV, fill=Spike_diff_flag)) +
geom_bar(stat = 'identity') +
theme_classic() +
labs(x= "Sample ID", y="NOx N (mg/L)", title = "NOx Auto Spike CV") +
scale_fill_manual(values = c("YES" = "darkcyan", "NO, check" = "darkred")) +
theme(legend.position="none") +
geom_hline(yintercept=50, linetype="dashed",
color = "black", size=1)  +
guides(fill=guide_legend(title="CV Between Spks <50%"))+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
NH3_spk_plot <- ggplot(data = df_spk_NH3_chk, aes(x =Pair_ID, y =CV, fill=Spike_diff_flag)) +
geom_bar(stat = 'identity') +
theme_classic() +
labs(x= "Sample ID", y="NH3 N(mg/L)", title = "NH3 Auto Spike CV") +
scale_fill_manual(values = c("YES" = "darkcyan", "NO, check" = "darkred")) +
theme(legend.position="none")  +
geom_hline(yintercept=50, linetype="dashed",
color = "black", size=1)  +
guides(fill=guide_legend(title="CV Between Spks <50%"))+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
PO4_spk_plot <- ggplot(data =df_spk_PO4_chk, aes(x =Pair_ID, y =CV, fill=Spike_diff_flag)) +
geom_bar(stat = 'identity') +
theme_classic() +
labs(x= "Sample ID", y="PO4 P(mg/L)", title = "PO4 Auto Spike CV") +
scale_fill_manual(values = c("YES" = "darkcyan", "NO, check" = "darkred")) +
theme(legend.position="none")  +
geom_hline(yintercept=50, linetype="dashed",
color = "black", size=1)  +
guides(fill=guide_legend(title="CV Between Spks <50%"))+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
Spk_Plots <- ggarrange(
NOx_spk_plot,
NH3_spk_plot,
PO4_spk_plot,   # (you had NH3 twice!)
nrow = 1,
ncol = 3,
common.legend = TRUE,
legend = "bottom")
print(Spk_Plots)
#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range
if (NOx_spk_percent <= chks_flag) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; NOx spikes out of range"),
"NOx spikes out of range"
)
}
if (NH3_spk_percent <= chks_flag) {
df_all_cor$NH3_flag <- ifelse(
df_all_cor$NH3_flag != "",
paste0(df_all_cor$NH3_flag, "; NH3 spikes out of range"),
"NH3 spikes out of range"
)
}
if (PO4_spk_percent <= chks_flag) {
df_all_cor$PO4_flag <- ifelse(
df_all_cor$PO4_flag != "",
paste0(df_all_cor$PO4_flag, "; PO4 spikes out of range"),
"PO4 spikes out of range"
)
}
df_matrix <- df_spk_combo%>%
filter(str_detect(Pair_ID, "Abs_Chk"))
df_matrix_NOx <- df_matrix %>%
filter(str_detect(Test, "Vanadium"))
ifelse(
all(df_matrix_NOx$Spike_diff_flag == "YES"),
"NO NOx Matrix Effect, PROCEED",
">20% CV in ASW NOx matrix effect checks - REASSESS"
)
df_matrix_NH3 <- df_matrix %>%
filter(str_detect(Test, "Ammonia"))
ifelse(
all(df_matrix_NH3$Spike_diff_flag == "YES"),
"NO NH3 Matrix Effect, PROCEED",
">20% CV in ASW NH3 matrix effect checks - REASSESS"
)
df_matrix_PO4 <- df_matrix %>%
filter(str_detect(Test, "PHOS"))
ifelse(
all(df_matrix_PO4$Spike_diff_flag == "YES"),
"NO PO4 Matrix Effect, PROCEED",
">20% CV in ASW PO4 matrix effect checks - REASSESS"
)
#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range
if (any(df_matrix$Spike_diff_flag[df_matrix$Test == "Vanadium NOx"] != "YES")) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; NOx matrix check out of range"),
"NOx matrix check out of range"
)
}
if (any(df_matrix$Spike_diff_flag[df_matrix$Test == "Ammonia 2"] != "YES")) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; NH3 matrix check out of range"),
"NH3 matrix check out of range"
)
}
if (any(df_matrix$Spike_diff_flag[df_matrix$Test == "o-PHOS 0.3"] != "YES")) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; PO4 matrix check out of range"),
"PO4 matrix check out of range"
)
}
#Pull out samples
samples <- df_all_cor %>%
filter(str_detect(Sample_Name, c("GCW|GWI|MSM|SWH")))
#Convert values based on the Test ID
samples <- samples %>%
mutate(
Conc_uM = case_when(
Test == "Vanadium NOx" ~ (((as.numeric(samples$Conc))/Con1)/N_mw)*Con2,
Test == "Ammonia 2" ~ (((as.numeric(samples$Conc))/Con1)/N_mw)*Con2,
Test == "o-PHOS 0.3" ~ (((as.numeric(samples$Conc))/Con1)/P_mw)*Con2,
TRUE ~ as.numeric(NA)  # fallback in case of unexpected value
),
# Replace negatives with 0 in 'value' and 'value_converted'
Conc = pmax(Conc, 0),
Conc_uM = pmax(Conc_uM, 0)
)
head(samples)
cat("Sample Flagging")
#Flagging data if the concentration is outside the standards range
samples_flagged <- samples %>%
mutate(
Conc_flag = case_when(
Test == "Vanadium NOx" & Conc < NOx_dl ~ "bdl",
Test == "Vanadium NOx" & Conc > NOx_top    ~ "adl",
Test == "Vanadium NOx"               ~ "Within_Range",
Test == "Ammonia 2" & Conc < NH3_dl ~ "bdl",
Test == "Ammonia 2" & Conc > NH3_top    ~ "adl",
Test == "Ammonia 2"                ~ "Within_Range",
Test == "o-PHOS 0.3" & Conc < PO4_dl  ~ "bdl",
Test == "o-PHOS 0.3" & Conc > PO4_top   ~ "adl",
Test == "o-PHOS 0.3"                ~ "Within_Range",
TRUE ~ NA_character_  # fallback for unexpected values
)
)
cat("Sample Processing")
samples_flagged <- samples_flagged %>%
filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
filter(!str_detect(Sample_Name, "RHZ|PPR")) %>%
mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = Sample_Name,
sep = "_",
into = c("Site", "Zone", "Replicate", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(202206))
# Filter rhizon and peeper samples
df_rhizon <- df_all %>%
filter(str_detect(Sample_Name, "RHZ"))
df_peep <- df_all %>%
filter(str_detect(Sample_Name, "PPR"))
# Timestamp for backups
timestamp <- format(Sys.time(), "%Y-%m-%d_%H%M")
# Paths
folder_path <- file.path("Raw Data", "Rhizon+Peeper")
dir.create(folder_path, recursive = TRUE, showWarnings = FALSE)
rhizon_main <- file.path(folder_path, "rhizon_data.csv")
peeper_main <- file.path(folder_path, "peeper_data.csv")
rhizon_backup <- file.path(folder_path, paste0("rhizon_data_", timestamp, ".csv"))
peeper_backup <- file.path(folder_path, paste0("peeper_data_", timestamp, ".csv"))
# Write timestamped backups
write.csv(df_rhizon, rhizon_backup, row.names = FALSE)
write.csv(df_peep, peeper_backup, row.names = FALSE)
# Overwrite the main files with latest data
write.csv(df_rhizon, rhizon_main, row.names = FALSE)
write.csv(df_peep, peeper_main, row.names = FALSE)
## ^^ I think there is a cleaner way to write this out, but this should work for now ^^
cat("Check Sample IDs with Metadata")
samples_flagged <- samples_flagged %>%
mutate(Sample_Name = str_replace(Sample_Name, "MSM_", "MSM_202206_")) %>%
mutate(Sample_Name = str_replace(Sample_Name, "GWI_", "GWI_202206_"))
#check to see if all samples are present in the metadata
all_present <- all(samples_flagged$Sample_Name %in% nutr_metadata$NUTR_ID)
if (all_present) {
message("All sample IDs are present in metadata.")
} else {
message("Some sample IDs are missing from metadata.")
# Optional: Which ones are missing?
missing_ids <- setdiff(samples_flagged$Sample_Name, nutr_metadata$NUTR_ID)
print(missing_ids)
}
nutr_metadata_selected <- nutr_metadata %>%
select(NUTR_ID, Year, Month, Day, Depth_cm, Lysimeter, Time..24hr., Time.Zone_EDT.EST,  Field.Notes)
#merge metadata with sample run data
merged_data <- samples_flagged %>%
left_join(nutr_metadata_selected, by = c("Sample_Name" = "NUTR_ID"))
df_all_clean <- merged_data %>%
#filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
#mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = Sample_Name,
sep = "_",
into = c("Site", "Samp_Time", "Zone", "Replicate", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(Samp_Time))
############### THIS IS ONLY FOR 2023 when SWAMP was called UP and UPLAND was called UPCON ##############
df_all_clean <- df_all_clean %>%
mutate(
Zone = case_when(
Site == "SWH" & Zone == "UP" ~ "SWAMP",
Site == "SWH" & Zone == "UPCON" ~ "UP",
TRUE ~ `Zone`
))
