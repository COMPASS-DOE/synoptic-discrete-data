---
title: "COMPASS_SynopticCB_PW_SO4_Cl_2023_Template"
author: "Stephanie J. Wilson"
date: "`r Sys.Date()`"
output: html_document
---

##Setup - Change things here & write any notes
```{r setup info}

#identify section 
cat("Setup Information")

###### Run information - PLEASE CHANGE
  Date_Run = "09/05/23"  #Date that instrument was run
  Run_by = "Stephanie J. Wilson"  #Instrument user 
  Script_run_by = "Stephanie J. Wilson" #Code user 
  run_notes = "A few samples (MSM PPRs) were run multiple times in this run"  #any notes from the run
  samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names 
  samples_pattern <- paste(samples, collapse = "|") 
    #samples_pattern <- "GCW" #use this instead of the line above if you have only one site code 
  chks_name = "Chk_Std_50C_2N"  #what did you name your check standards? 
  
###### File Names - PLEASE CHANGE 
#file path and name for raw summary data file 
    raw_file_name_cl = "Raw Data/COMPASS_Synoptic_CB_MonMon_202304_Cl.txt"  #example
    raw_file_name_so4 = "Raw Data/COMPASS_Synoptic_CB_MonMon_202304_SO4.txt"
    
    #raw_file_name_cl = "Raw Data/FILE_NAME.txt"
    #raw_file_name_so4 = "Raw Data/FILE_NAME.txt"
    
#file path and name for raw all peaks file 
    #raw_allpeaks_name = "Raw Data/COMPASS_SynopticCB_PW_DOC_202505_allpeaks.txt" #example
    #raw_allpeaks_name = "Raw Data/FILE_NAME.txt" 

#file path and name of processed data file 
    processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_Cl_SO4_202304.csv" #example
    #processed_file_name = "Processed Data/FILE_NAME.csv" #example


###### Log Files - PLEASE CHECK 
#downloaded metadata csv - downloaded from Google drive as csv for this year
  Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
  
#qaqc log file path for this year 
  Log_path = "Raw Data/COMPASS_Synoptic_Cl_SO4_QAQClog_2023.csv"
  
  cat(run_notes)

```

##Set Up Code
```{r setup, include=FALSE}

#identify section 
cat("Setup")

#Link to the protocol used for analysis 
  #steph will add this soon 

#Packages that are required 
lapply(c(
  "dplyr", "ggplot2", "ggpubr", "stringr",
  "purrr", "tidyverse", "here", "broom", "tibble",
  "googledrive", "googlesheets4", "data.table", 
  "matrixStats", "gridExtra", "grid"), 
  library, character.only = TRUE)

#any coefficients / constants that are needed for calculations 
  cl_mw <- 35.45     #molecular weight of Chloride: 35.45 
  s_mw <- 32.06      #molecular weight of sulfur: 32.06
  Con1 <- 1000       # conversion factor value
  Con2 <- 1000000    # conversion factor value 

#Flag that we 
  r2_cutoff = 0.98  #this is the level below which we want to rerun or consider a curve 
  chk_flag = 0.15   #for the RSD (relative standard deviation) 
  chk_conc_flag = 15 #this is the level cutoff for percent difference of check standards vs. the concentration they are meant to be 
  chks_flag = 0.50 #this is the percent of chks we want to have a CV less than 10, usually 60
  rep_flag = 25 #this is a 25% error between samples
  #blank_flag - calculated based on samples later in this code as lower 25% quantile of sample concentrations

#standard concentrations - Update if running different checks: 
  standards <- tibble(
    sample_ID = c("Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
    SO4_std_conc = c(0.5, 1.0, 2.0, 10, 20),
    Cl_std_conc = c(5, 10, 20, 100, 200)
)
  
#Spike concentration calc: 
  spike = 2.5
  spk_std <- (250/s_mw)    # in mM
  spkvol <- 10             # in uL
  spkvol <- spkvol/1000000 
  #spike for these samples was 10uL of the 250mM standard
  spk_Conc <- (spk_std)*spkvol 
   
#Top standard Concentrations- Update if running different standard curve: 
   top_std_cl = 200
   top_std_so4 = 20

#Set time zone 
  common_tz = "Etc/GMT+5"
  Sys.setenv(TZ = "America/New_York")
  
#plot indicators 
  site_order <- c('GCW', 'MSM', 'GWI', 'SWH')
  plot_order <- c('UP', 'SWAMP', 'TR', 'WC', 'SW')
  plot_colors <- c("#20063B", "darkgrey", "#FFBC42", "#419973", "#25ABE6" )


```

##Read in metadata and create similar sample IDs for matching to samples 
```{r pull in metadata for later, include=FALSE}

#read in the raw metadata file 
raw_metadata <- read.csv(Raw_Metadata)

#make a new columns in the metadata with important info:
metadata <- raw_metadata %>%
  mutate(Depth = paste0(Depth_cm, "cm")) %>%
  mutate(LysID = paste0("Lys", Lysimeter)) %>%
  mutate(YearMonth = sprintf("%d%02d", Year, Month))  %>%
  mutate(Zone = case_when(
    `Transect.Location` == "Transition" ~ "TR",
    `Transect.Location` == "Wetland"    ~ "WC",
    `Transect.Location` == "Upland"     ~ "UP",   
    `Transect.Location` == "Surface Water" ~ "SW",
    TRUE                 ~ `Transect.Location`    # keep original value if no match
  ))

#Create DOC IDs from what was collected for comparison later
metadata <- metadata %>%
  mutate(Cl_SO4_ID = ifelse(SO4 == "x",
                                     paste(Site,
                                           YearMonth,
                                           Zone,
                                           LysID,
                                           Depth,
                                           sep = "_"),
                                     NA),  )

#Change the SW lines because they don't have lysimeters or a depth  
metadata <- metadata %>%
  mutate(
    Cl_SO4_ID = if_else(
      Zone == "SW",
      # Modify the string:
      Cl_SO4_ID %>%
        str_replace("_LysA", "_A") %>%                    # Replace "_LysA" with "_A"
        str_replace("_LysB", "_B") %>%                    # Replace "_LysB" with "_B"
        str_replace("_LysC", "_C") %>%                    # Replace "_LysC" with "_C"
        str_replace("_0cm$", ""),                          # Remove trailing "_0cm"
      Cl_SO4_ID  # else keep original
    )
  )

#Take out the columns and rows that are not relevant
dionex_metadata <- metadata %>%
  select(Cl_SO4_ID, Year, Month, Day, YearMonth, Site, Zone, Lysimeter, LysID, Depth_cm, 
         Depth, Time..24hr., Time.Zone_EDT.EST, Field.Notes, ) %>%        
  # only keep specific columns
  filter(!is.na(Cl_SO4_ID) & Cl_SO4_ID != "")  # remove missing/blank Cl_SO4_ID rows

head(dionex_metadata)

```

## Import Sample Data     
```{r Import Data, echo=FALSE}

cat("Import Sample Data")

## Read in raw data file from Dionex - copied and saved as a txt
#Sulfate
Sdat <- read.table(raw_file_name_so4, sep='\t' , header=T, skip=3)

Sdat_clean <- Sdat %>%
  select(
    sample_name = X.1,
    sample_ID = X.1,
    sample_type = X.2,
    SO4_ppm = IC.SO4.1, 
    SO4_area = IC.SO4.3
  ) %>%
  group_by(sample_name) %>%
   mutate(sample_name = if_else(
    sample_name %in% c("Lab Blank", "Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
    paste0(sample_name, "_", row_number()),
    sample_name
  )) %>%
  ungroup()
  #head(Sdat_clean)

#Chloride
Cldat  <- read.table(raw_file_name_cl, sep='\t' , header=T, skip=3)

Cldat_clean <- Cldat %>%
  select(
    sample_name = X.1,
    sample_ID = X.1,
    sample_type = X.2,
    Cl_ppm = IC.Cl.1,
    Cl_area = IC.Cl.3
  ) %>%
  group_by(sample_name) %>%
   mutate(sample_name = if_else(
    sample_name %in% c("Lab Blank", "Standard 1", "Standard 2", "Standard 3", "Standard 4", "Standard 5"),
    paste0(sample_name, "_", row_number()),
    sample_name
  )) %>%
  ungroup()
  #head(Cldat_clean)


## Pull Cl and SO4 data  together: 
all_dat_merge <- inner_join(Sdat_clean, Cldat_clean, by = c("sample_name", "sample_ID", "sample_type"))

#Remove any lines that have no sample ID, make n.a. into NAs,
  #if there are NAs in the SO4 or Cl columns, make them zeros because that means there was no peak detected
all_dat <- all_dat_merge %>%
  filter(!is.na(sample_ID) & sample_ID != "") %>%
  mutate(across(everything(), ~ na_if(.x, "n.a."))) %>%
  mutate(across(c(SO4_ppm, Cl_ppm), as.numeric), 
         across(c(SO4_area, Cl_area), as.numeric)) %>%
  mutate(across(c(SO4_ppm, Cl_ppm), ~ replace_na(.x, 0)), 
         across(c(SO4_area, Cl_area), ~ replace_na(.x, 0)))
head(all_dat)

all_dat <- all_dat %>%
  mutate(dilution = case_when(
    str_detect(sample_ID, "GCW") & str_detect(sample_ID, "UP") ~50,
    str_detect(sample_ID, "GCW") & str_detect(sample_ID, "TR") ~ 50,
    str_detect(sample_ID, "GCW") & str_detect(sample_ID, "WC") ~ 100,
    str_detect(sample_ID, "MSM") & str_detect(sample_ID, "UP") ~ 50,
    str_detect(sample_ID, "MSM") & str_detect(sample_ID, "TR") ~ 50,
    str_detect(sample_ID, "MSM") & str_detect(sample_ID, "WC") ~ 100,
    str_detect(sample_ID, "GWI") & str_detect(sample_ID, "UP") ~ 100,
    str_detect(sample_ID, "GWI") & str_detect(sample_ID, "TR") ~ 100,
    str_detect(sample_ID, "GWI") & str_detect(sample_ID, "WC") ~ 200,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "UP") ~ 50,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "UPCON") ~ 50,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "SWAMP") ~ 50,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "TR") ~ 50,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "WC") ~ 50,
    TRUE ~ NA_real_
  )) %>%
  mutate(sample_vol = case_when(
    str_detect(sample_ID, "GCW") & str_detect(sample_ID, "UP") ~ 1501,
    str_detect(sample_ID, "GCW") & str_detect(sample_ID, "TR") ~ 1501,
    str_detect(sample_ID, "GCW") & str_detect(sample_ID, "WC") ~ 1475,
    str_detect(sample_ID, "MSM") & str_detect(sample_ID, "UP") ~ 1501,
    str_detect(sample_ID, "MSM") & str_detect(sample_ID, "TR") ~ 1501,
    str_detect(sample_ID, "MSM") & str_detect(sample_ID, "WC") ~ 1475,
    str_detect(sample_ID, "GWI") & str_detect(sample_ID, "UP") ~ 1475,
    str_detect(sample_ID, "GWI") & str_detect(sample_ID, "TR") ~ 1475,
    str_detect(sample_ID, "GWI") & str_detect(sample_ID, "WC") ~ 1462,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "UP") ~ 1501,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "UPCON") ~ 1501,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "SWAMP") ~ 1501,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "TR") ~ 1501,
    str_detect(sample_ID, "SWH") & str_detect(sample_ID, "WC") ~ 1501,
    TRUE ~ NA_real_
  )) 


```

## Assessing Standard Curves 
```{r Assess Standard Curves, echo=FALSE}

cat("Assess the Standard Curves")

# Filter standards
stds <- all_dat %>%
  filter(grepl("Calibration Standard", sample_type)) %>%
  mutate(run_date = Date_Run)

#We might need to identify the concentration of the standards, rather than use what is in the file TBD 

#calculate slope and r2 of cal curves
#npoc curve
lm_results_cl <- stds %>%
  group_by(run_date) %>%
  do({
    model = lm(Cl_area ~ Cl_ppm, data = .)
    tidy_model = tidy(model)             # coefficients
    glance_model = glance(model)         # model metrics like R²
    tibble(
      slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
      intercept = tidy_model$estimate[1],
      r2 = glance_model$adj.r.squared
    )
  }) %>%
  mutate(
    analyte = "Cl", 
    curve = "Chloride (mg/L)"
  )

#tn curve
lm_results_s <- stds %>%
  group_by(run_date) %>%
  do({
    model = lm(SO4_area ~ SO4_ppm, data = .)
    tidy_model = tidy(model)             # coefficients
    glance_model = glance(model)         # model metrics like R²
    tibble(
      slope = tidy_model$estimate[2],    # coefficient for standard_C_ppm
      intercept = tidy_model$estimate[1],
      r2 = glance_model$adj.r.squared
    )
  }) %>%
  mutate(
    analyte = "SO4", 
    curve = "SO4 (mg/L)"
  )

#put the together in one dataframe to later add to log 
Slopes <- rbind(lm_results_cl, lm_results_s)

#store the r2's so they plot on the curve graphs
r2_labels_cl <- stds %>%
  group_by(run_date) %>%
  summarise(
    x_pos = max(Cl_ppm, na.rm = TRUE) * 0.8,
    y_pos = max(Cl_area, na.rm = TRUE),
    r_squared = round(summary(lm(Cl_area ~ Cl_ppm))$adj.r.squared, 4),
    .groups = "drop"
  )

r2_labels_so4 <- stds %>%
  group_by(run_date) %>%
  summarise(
    x_pos = max(SO4_ppm, na.rm = TRUE) * 0.8,
    y_pos = max(SO4_area, na.rm = TRUE),
    r_squared = round(summary(lm(SO4_area ~ SO4_ppm))$adj.r.squared, 4),
    .groups = "drop"
  )


##Plot standard Curve or Curves 
#Cl Curve
Cl_stds_plot <- ggplot(stds, aes(x = Cl_ppm, y = Cl_area)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  facet_wrap(~ run_date) +
  geom_text(
    data = r2_labels_cl,
    aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
    inherit.aes = FALSE,
    hjust = 1, vjust = 1,
    size = 4
  ) +
  labs(
    title = "Chloride Std Curve by Date",
    x = "Cl Standard Concentration (ppm)",
    y = "Peak Area"
  ) +
  theme_bw()

Cl_stds_plot

#SO4 Curve
SO4_stds_plot <- ggplot(stds, aes(x = SO4_ppm, y = SO4_area)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "purple") +
  facet_wrap(~ run_date) +
    geom_text(
    data = r2_labels_so4,
    aes(x = x_pos, y = y_pos, label = paste0("R² = ", r_squared)),
    inherit.aes = FALSE,
    hjust = 1, vjust = 1,
    size = 4
  ) +
  labs(
    title = "Sulfate Std Curve by Date",
    x = "SO4 Standard Concentration (ppm)",
    y = "Peak Area"
  ) +
  theme_bw()

SO4_stds_plot

#compare slopes to previous runs (from log) in order to assess drift 
log <- read.csv(Log_path)
log <- log[ ,-c(1)]

#make sure they both have dates as dates 
log$run_date <- as.Date(log$run_date)
log$analyte <- as.character(log$analyte)
log$curve <- as.character(log$curve)
Slopes$run_date <- as.Date(Slopes$run_date)

# Filter to only rows in Slopes that are NOT already in log (by run_date + analyte)
new_rows <- anti_join(Slopes, log, by = c("run_date", "analyte"))

# Append the new, non-duplicate rows to log
log <- bind_rows(log, new_rows)

#plot the current slops with teh previous slopes 
Slopes_chk <- ggplot(log, aes(run_date, slope, col=curve)) +
  geom_point(size=4) + 
  geom_line() + 
  theme_bw() + labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
  scale_color_manual(values=c("blue", "purple"))
Slopes_chk

#write out the log file with the added lines for this run  
write.csv(log, Log_path)

#Grab the highest r2 that is available for this run 
r2_Cl = max(lm_results_cl$r2)
r2_SO4 = max(lm_results_s$r2)

#Write out to the user whether or not the r2 is above the cutoff of 0.98
  ifelse(r2_Cl <= r2_cutoff, 
         "Cl Curve r2 is below cutoff! - REASSESS", "Cl Curve r2 GOOD")
  ifelse(r2_SO4 <= r2_cutoff, 
         "SO4 Curve r2 is below cutoff! - REASSESS", "SO4 Curve r2 GOOD")
  
#write out a flag to the sample dataframe if the r2 is above the cutoff of 0.98
all_dat <- all_dat %>%
  mutate(
    Cl_flag = if (r2_Cl <= r2_cutoff) {
      "NPOC r2 low"
    } else {
      ""
    },
    SO4_flag = if (r2_SO4 <= r2_cutoff) {
      "TN r2 low"
    } else {
      ""
    }
  )

```

## Assess Check Standards 
```{r Check Standards, echo=FALSE}

cat("Assess the Check Standards")

# Pull out check standards
chks_raw <- all_dat %>%
  filter(grepl("Check Standard", sample_type))

chks_raw <- chks_raw %>% 
  mutate(rep = row_number()) %>%
  filter(!(sample_ID == "Standard 1" & SO4_ppm > 20)) %>% #some of the checks got labeled incorrectly
  filter(!(sample_ID == "Standard 1" & Cl_ppm > 300)) #some of the checks got labeled incorrectly

#we don't always the same checks so we need to pull the concentration out of the name 
chks_raw <- chks_raw %>%
    mutate(Cl_chk_conc = case_when(
    sample_ID == "Standard 1" ~ 5,
    sample_ID == "Standard 2" ~ 10,
    sample_ID == "Standard 3" ~ 20,
    sample_ID == "Standard 4" ~ 100,
    sample_ID == "Standard 5" ~ 200,
    TRUE ~ NA_real_  # For samples that aren't standards
  )) %>%  # extract the number
  mutate(Cl_chk_conc = as.numeric(Cl_chk_conc)) %>%
    mutate(SO4_chk_conc = case_when(
    sample_ID == "Standard 1" ~ 0.5,
    sample_ID == "Standard 2" ~ 1.0,
    sample_ID == "Standard 3" ~ 2.0,
    sample_ID == "Standard 4" ~ 10,
    sample_ID == "Standard 5" ~ 20,
    TRUE ~ NA_real_  # For samples that aren't standards
  )) %>%  # extract the number
  mutate(SO4_chk_conc = as.numeric(SO4_chk_conc))  

#we won't be calculating rsd because we run checks of different concentrations

#calculate percent difference between check standards & expected concentration 
chks_raw$Cl_diff <- ((chks_raw$Cl_ppm - chks_raw$Cl_chk_conc)/((chks_raw$Cl_ppm + chks_raw$Cl_chk_conc)/2)) * 100
chks_raw$Cl_diff_flag <-  ifelse(abs(chks_raw$Cl_diff) <= chk_conc_flag, 'YES', 'NO, rerun')

chks_raw$SO4_diff <- ((chks_raw$SO4_ppm - chks_raw$SO4_chk_conc)/((chks_raw$SO4_ppm + chks_raw$SO4_chk_conc)/2)) * 100
chks_raw$SO4_diff_flag <-  ifelse(abs(chks_raw$SO4_diff) <= chk_conc_flag, 'YES', 'NO, rerun')

#Plot the check standardsvs. the expected concentration 
cl_chks <-  ggplot(data = chks_raw, aes(x = rep, y = Cl_ppm, fill=Cl_diff_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="Cl (ppm)", title="Check Stds: Chloride") + 
        theme(legend.position="bottom") +  
        #geom_hline(yintercept=chk_std_high_c, linetype="dashed", color = "black", linewidth=1) + 
              guides(fill=guide_legend(title="% Difference <10%"))


so4_chks <-  ggplot(data = chks_raw, aes(x = rep, y = SO4_ppm, fill=SO4_diff_flag)) +
       geom_bar(stat = 'identity') + 
       scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="SO4  (ppm)", title="Check Stds: Sulfate") + 
        theme(legend.position="bottom") + 
        #geom_hline(yintercept=chk_std_high_n,linetype="dashed",  color = "black", linewidth=1) + 
              guides(fill=guide_legend(title="% Difference <10%"))

ggarrange(cl_chks, so4_chks, nrow=1, ncol=2)

#calculate the percent of check standards that are within the range based on the flag 
cl_chks_percent <- (sum(chks_raw$Cl_diff_flag == "YES")/nrow(chks_raw))*100
so4_chks_percent <- (sum(chks_raw$SO4_diff_flag == "YES")/nrow(chks_raw))*100

#report out if flags indicate need for rerun
ifelse(cl_chks_percent >= chk_flag, ">60% of Chloride Check Standards are within range of expected concentration",
       "<60% of Chloride Check Standards are within range of expected concentration - REASSESS")
ifelse(so4_chks_percent >= chk_flag,">60% of Sulfate Check Standards are within range of expected concentration",
       "<60% of Sulfate Check Standards are within range of expected concentration - REASSESS")

#write out a flag to the sample dataframe if less than 60% of the checks are within the expected CV
if (cl_chks_percent <= chks_flag) {
    all_dat$Cl_flag <- ifelse(
    all_dat$Cl_flag != "",
    paste0(all_dat$cl_flag, "; Cl checks out of range"),
    "Cl checks out of range"
  )
}

if (so4_chks_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
    all_dat$SO4_flag <- ifelse(
    all_dat$SO4_flag != "",
    paste0(all_dat$SO4_flag, "; SO4 checks out of range"),
    "SO4 checks out of range"
  )
}

```

## Assess Blanks 
```{r Check Blanks, echo=FALSE}

cat("Assess Blanks")

#Pull out the blanks from raw file 
blks_raw <- all_dat %>%
  filter(grepl("Lab Blank", sample_ID))

blks_raw <- blks_raw %>% 
  mutate(rep = row_number())

#Check if the blanks are above the lower 25% quantile of your data 
blk_flag_cl <- quantile(all_dat$Cl_ppm, prob=c(.25))   #this gives you the lower 25% quantile of the data 
blks_raw$Cl_diff_flag <-  ifelse(blks_raw$Cl_ppm <= blk_flag_cl, 'YES', 'NO, rerun')

blk_flag_so4 <- quantile(all_dat$SO4_ppm, prob=c(.25))   #this gives you the lower 25% quantile of the data 
blks_raw$so4_diff_flag <-  ifelse(blks_raw$SO4_ppm <= blk_flag_so4, 'YES', 'NO, rerun')

#calculate the percent of check standards that are within the range based on the flag 
cl_blks_percent <- (sum(blks_raw$Cl_diff_flag == "YES")/nrow(blks_raw))*100
so4_blks_percent <- (sum(blks_raw$so4_diff_flag == "YES")/nrow(blks_raw))*100

#report out if flags indicate need for rerun
ifelse(cl_blks_percent >= chks_flag, ">60% of Carbon Blank concentrations are lower 25% quartile of samples",
       "<60% of Carbon blanks are lower 25% quartile of samples - REASSESS")
ifelse(so4_blks_percent >= chks_flag, ">60% of Nitrogen Blank concentrations are lower 25% quartile of samples",
       "<60% of Nitrogen blanks are lower 25% quartile of samples - REASSESS")

#Plot the blanks vs. the lower 25% quantile of your data in this run (black line)
cl_blks <-  ggplot(data = blks_raw, aes(x = rep, y = Cl_ppm, fill=Cl_diff_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkblue", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="Cl  (mg/L)", title="Blanks: Chloride") + 
        theme(legend.position="bottom") +  geom_hline(yintercept=blk_flag_cl, linetype="dashed", 
                color = "black", linewidth=1)  + 
                guides(fill=guide_legend(title="Blank Conc <25% Quartile Samples"))

so4_blks <-  ggplot(data = blks_raw, aes(x = rep, y = SO4_ppm, fill=so4_diff_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkblue", "NO, rerun" = "darkgrey")) +
        theme_classic() + labs(x= " ", y="SO4  (mg/L)", title="Blanks: Sulfate") + 
        theme(legend.position="bottom") +  geom_hline(yintercept=blk_flag_so4, linetype="dashed", 
                color = "black", linewidth=1)  + 
                guides(fill=guide_legend(title="Blank Conc <25% Quartile Samples"))

ggarrange(cl_blks, so4_blks, nrow=1, ncol=2)

#print out the average blank concentrations 
blk_avg_cl <- mean(blks_raw$Cl_ppm)
cat("Chloride blanks:")
print(blk_avg_cl)

blk_avg_so4 <- mean(blks_raw$SO4_ppm)
cat("Sulfate blanks:")
print(blk_avg_so4)

#write out a flag to the sample dataframe if more than 60% of the blanks are above the lower 25% quantile of samples
if (cl_blks_percent <= chks_flag) {
  all_dat$Cl_flag <- ifelse(
    all_dat$Cl_flag != "",
    paste0(all_dat$Cl_flag, "; Cl blanks out of range"),
    "Cl blanks out of range"
  )
}

if (so4_blks_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
  all_dat$SO4_flag <- ifelse(
    all_dat$SO4_flag != "",
    paste0(all_dat$SO4_flag, "; SO4 blanks out of range"),
    "SO4 blanks out of range"
  )
}

```

## Assess Duplicates 
```{r Check Duplicates, echo=FALSE}

cat("Assess Duplicates")

#Take a look at the raw data 
  #head(all_dat)

#pull out any rows that have "dup" in the sample_name column
dups <- all_dat %>%  
  select(!c(sample_name, sample_type, SO4_area, Cl_area, Cl_flag, SO4_flag)) %>%
  filter(str_detect(sample_ID, "dup"))      #have to change this to match data

#create a new dataframe and remove dups from sample dataframe 
dat_raw2 <- all_dat %>%  
  filter(!str_detect(sample_ID, "dup")) %>%  
  select(!c(sample_name, sample_type, SO4_area, Cl_area, Cl_flag, SO4_flag))

#remove the dup from these IDs so we will have duplicate sample names
dups <- dups %>%
  mutate(sample_ID = gsub("_dup", "", as.character(sample_ID))) %>%
  rename(
    sample_ID = sample_ID,
    SO4_ppm_dup = SO4_ppm, 
    Cl_ppm_dup = Cl_ppm
  )

#merge with the dataframe so we have a column for the conc and the dup
QAdups <- merge(dat_raw2, dups)

#create a dataframe to compare npoc dups 
df2 <- as.data.frame(QAdups$Cl_ppm)
df2$dups <- QAdups$Cl_ppm_dup

#calculate the cv of the duplicates
df2$sds <- apply(df2,1,sd)
df2$mean <- apply(df2, 1, mean)

QAdups$Cl_dups_cv <- (df2$sds/df2$mean) * 100
QAdups$Cl_dups_cv_flag <-  ifelse(QAdups$Cl_dups_cv <10, 'YES', 'NO, rerun')

#create a dataframe to compare tdn dups 
df3 <- as.data.frame(QAdups$SO4_ppm)
df3$dups <- QAdups$SO4_ppm_dup

#calculate the cv of the duplicates
df3$sds <- apply(df3,1,sd)
df3$mean <- apply(df3, 1, mean)

QAdups$SO4_dups_cv <- (df3$sds/df3$mean) * 100
QAdups$SO4_dups_cv_flag <-  ifelse(QAdups$SO4_dups_cv <10, 'YES', 'NO, rerun')

#Put all the dups together and create row numbers for plotting
QAdups <- QAdups %>%
  filter(sample_ID != "100_MSM_202304_PPR_TR_4") %>%  #this run has a weird thing going on 
  mutate(row_num = row_number()) 
#head(QAdups)



#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this 
Cl_dups <- ggplot(data =QAdups, aes(x =row_num, y =Cl_dups_cv, fill=Cl_dups_cv_flag)) +
       geom_bar(stat = 'identity') + 
        theme_classic() + labs(x= " ", y="CV of Chloride Duplicates") + 
        scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "red")) +
        theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", 
                color = "black", size=1)  + 
              guides(fill=guide_legend(title="CV Between Dups <10%"))


SO4_dups <- ggplot(data =QAdups, aes(x =row_num, y =SO4_dups_cv, fill=SO4_dups_cv_flag)) +
       geom_bar(stat = 'identity') + 
        theme_classic() + labs(x= " ", y="CV of Sulfate Duplicates") + 
          scale_fill_manual(values = c("YES" = "darkgreen", "NO, rerun" = "red")) +
        theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", 
                color = "black", size=1) + 
              guides(fill=guide_legend(title="CV Between Dups <10%"))

ggarrange(Cl_dups, SO4_dups,ncol=2, nrow=1)

#Zeros will create NAs in the flag column, so we need to switch those to Yes's 
QAdups <- QAdups %>%
  mutate(Cl_dups_cv_flag = if_else(is.na(Cl_dups_cv), "YES", Cl_dups_cv_flag)) %>%
  mutate(SO4_dups_cv_flag = if_else(is.na(SO4_dups_cv), "YES", SO4_dups_cv_flag))

#calculate the percent of check standards that are within the range based on the flag 
cl_dups_percent <- (sum(QAdups$Cl_dups_cv_flag == "YES")/nrow(QAdups))*100
so4_dups_percent <- (sum(QAdups$SO4_dups_cv_flag == "YES")/nrow(QAdups))*100

#report out if the dups are within range
ifelse(cl_dups_percent >= chks_flag, ">60% of Chloride Duplicates have a CV <10%",
       "<60% of Chloride Duplicates have a CB <10% - REASSESS")
ifelse(so4_dups_percent >= chks_flag, ">60% of Sulfate Duplicates have a CV <10%",
       "<60% of Sulfate Duplicates have a CB <10% - REASSESS")

#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range 
if (cl_dups_percent <= chks_flag) {
    all_dat$Cl_flag <- ifelse(
    all_dat$Cl_flag != "",
    paste0(all_dat$Cl_flag, "; Cl dups out of range"),
    "Cl dups out of range"
  )
}

if (so4_dups_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
    all_dat$SO4_flag <- ifelse(
    all_dat$SO4_flag != "",
    paste0(all_dat$SO4_flag, "; SO4 dups out of range"),
    "SO4 dups out of range"
  )
}

```


## Calculate mmol/L concentrations & salinity
```{r}
cat("Unit Conversion and Salinity Calculation")

# Convert ppm to mmol/L
all_dat$SO4_mM <- (all_dat$SO4_ppm / s_mw)
all_dat$Cl_mM <- (all_dat$Cl_ppm / cl_mw)

# Calculate Salinity 
# calculated using the Knudsen equation 
# Salinity = 0.03 + 1.8050 * Chlorinity
# Ref: A Practical Handbook of Seawater Analysis by Strickland & Parsons (P. 11)
# =((1.807*Cl_ppm)+0.026)/1000
all_dat$salinity <- ((1.8070 * all_dat$Cl_ppm) + 0.026) / 1000

head(all_dat)
```


## Assess Analytical Spikes 
```{r}
cat("Assess Spikes")

#pull out any rows that have "spk" in the SampleID column
spks <- all_dat %>%  
  select(!c(sample_name, sample_type, SO4_area, SO4_ppm, Cl_ppm, Cl_area, Cl_flag, Cl_mM, SO4_flag, salinity)) %>%
  filter(str_detect(sample_ID, "spk"))      #have to change this to match data
head(spks)

#create a new dataframe and remove dups from sample dataframe 
dat_spks <- all_dat %>%  
  filter(!str_detect(sample_ID, "spk")) %>%  
  select(!c(sample_name, sample_type, SO4_area, SO4_ppm, Cl_ppm, Cl_area, Cl_flag, Cl_mM, SO4_flag, salinity))

#remove the dup from these IDs so we will have duplicate sample names
spks <- spks %>%
  mutate(sample_ID = gsub("_spk", "", as.character(sample_ID))) %>%
  rename(
    sample_ID = sample_ID,
    SO4_mM_spk = SO4_mM
  )

#put it back together with the old data set and look for duplicates 
QAspks <- merge(dat_spks, spks)

QAspks <- QAspks %>%
  filter(sample_ID != "101_MSM_202304_PPR_TR_5") 

#now we need to calculate the spike concentration and calculate the spike recovery
spkconc <- (250/s_mw)    # in mM
spkvol <- 10             # in uL
spkvol <- spkvol/1000000 
#spike for these samples was 10uL of the 250mM standard
QAspks$SO4_spk_Conc <- spk_Conc     # mmoles of SO4 

#change sample volume to L 
QAspks$sample_vol <- QAspks$sample_vol/1000000

#gives us the total SO4 in the sample in mmoles
QAspks$SO4_Total_unspkd <- (QAspks$SO4_mM/QAspks$dilution)*(QAspks$sample_vol) 

##total SO4 in spiked sample in mmoles
QAspks$SO4_Total_spkd <- (QAspks$SO4_mM_spk/QAspks$dilution)*(QAspks$sample_vol+spkvol) 

QAspks$SO4_expctd_spkd <-  (QAspks$SO4_Total_unspkd + QAspks$SO4_spk_Conc)
QAspks$spk_recovery <-    (QAspks$SO4_Total_spkd/QAspks$SO4_expctd_spkd)*100
QAspks$SO4_spks_flag <-  ifelse(QAspks$spk_recovery <=120 & QAspks$spk_recovery >=80 , 'YES', 'NO, rerun')  #fix 

#plot spk recoveries output as a bar graph to easily check - want any over 10% to be red need to work on this 
spksbar <- ggplot(data = QAspks, aes(x = sample_ID, y = spk_recovery, fill=SO4_spks_flag)) +
        geom_bar(stat = 'identity') + 
        scale_fill_manual(values = c("YES" = "darkcyan", "NO, rerun" = "darkred")) +
        theme_classic() + labs(x= "Sample ID", y="Spike Recovery (%)") + 
        theme(legend.position="none") +  
        geom_hline(yintercept=80, linetype="dashed", color = "black", size=1) + 
        geom_hline(yintercept=120, linetype="dashed", color = "black", size=1)+
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))

spksbar

#calculate the percent of check standards that are within the range based on the flag
spk_percent <- (sum(QAspks$SO4_spks_flag == "YES")/nrow(QAspks))*100
 
#spk flag report out
ifelse(spk_percent >= chks_flag, 
       ">60% of Spikes are within the expected range - PROCEED",
       "<60% of Spikes are outside of the expected range - REASSESS")

#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range
if (spk_percent <= chks_flag) {  # assuming you have tn_chks_percent similarly
    all_dat$SO4_flag <- ifelse(
    all_dat$SO4_flag != "",
    paste0(all_dat$SO4_flag, "; SO4 spikes out of range"),
    "SO4 spikes out of range"
  )
}

```


## Sample Flagging - Are samples Within the range of the curve? 
```{r Sample Flagging, echo=FALSE}

cat("Sample Flagging")

dat_flagged <- all_dat %>%
  filter(str_detect(sample_name, "GCW|GWI|MSM|SWH")) %>%
  mutate(Cl_ppm_undiluted = Cl_ppm / dilution) %>%
  mutate(SO4_ppm_undiluted = SO4_ppm / dilution)

#Flagging data if the concentration is outside the standards range and based on blanks
dat_flagged <- dat_flagged %>%
  mutate(
    Cl_flag = if_else(
      Cl_ppm_undiluted > top_std_cl,
      if_else(
        Cl_flag != "" & !is.na(Cl_flag),
        paste0(Cl_flag, "; value above cal curve"),
        "value above cal curve"
      ),
      Cl_flag
    ),
    Cl_flag = if_else(
      blk_avg_cl > 0.25 * Cl_ppm_undiluted,
      if_else(
        Cl_flag != "" & !is.na(Cl_flag),
        paste0(Cl_flag, "; blank is ≥ 25% of sample value"),
        "blank is ≥ 25% of sample value"
      ),
      Cl_flag
    ),

    SO4_flag = if_else(
      SO4_ppm_undiluted > top_std_so4,
      if_else(
        SO4_flag != "" & !is.na(SO4_flag),
        paste0(SO4_flag, "; value above cal curve"),
        "value above cal curve"
      ),
      SO4_flag
    ),
    SO4_flag = if_else(
      blk_avg_so4 > 0.25 * SO4_ppm,
      if_else(
        SO4_flag != "" & !is.na(SO4_flag),
        paste0(SO4_flag, "; blank is ≥ 25% of sample value"),
        "blank is ≥ 25% of sample value"
      ),
      SO4_flag
    )
  )

#lets make a dataframe with just the concentration flag for plotting 
dat_flag_viz <- dat_flagged %>% 
  mutate(Cl_flag = case_when(Cl_ppm_undiluted > top_std_cl ~ "value above cal curve",
            blk_avg_cl > 0.25*Cl_ppm_undiluted ~ "blank is ≥ 25% of sample value"), # flagging if blank concentration is > 20% of the sample concentration
            #sample_name == "TMP_SW_F4_T3" ~ "incorrect sample naming, cannot resolve"), # if needed and there are issues with sample names
    
        SO4_flag = case_when(SO4_ppm_undiluted > top_std_so4 ~ "value above cal curve",
            blk_avg_so4 > 0.25*SO4_ppm_undiluted ~ "blank is ≥ 25% of sample value") #, # flagging if blank concentration is > 25% of the sample concentration
          #sample_name == "TMP_SW_F4_T3" ~ "incorrect sample naming, cannot resolve"), # if needed and there are issues with sample names
 )

#Plot data and change colors based on flags to check it: 
c_samples_flag <-  ggplot(data = dat_flag_viz, aes(x = sample_name, y = Cl_ppm, fill=Cl_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("red", "orange"))+
        theme_classic() + labs(x= " ", y="Cl (mg/L)", title="C: Grey = Within Range of Curve") + 
        theme(legend.position="none") +
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))


n_samples_flag <-  ggplot(data = dat_flag_viz, aes(x = sample_name, y = SO4_ppm, fill=SO4_flag)) +
       geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("red", "orange"))+
        theme_classic() + labs(x= " ", y="SO4 (mg/L)", title="N: Grey = Within Range of Curve") + 
        theme(legend.position="none") +
        theme(axis.text.x = element_text(angle = 90, hjust = 0.5))

ggarrange(c_samples_flag, n_samples_flag, nrow=1, ncol=2)


```

## Pull out sample id information 
```{r Sample Identification, echo=FALSE}

cat("Sample Processing")

samples_flagged <- dat_flagged %>%
  filter(str_detect(sample_name, "GCW|GWI|MSM|SWH")) %>%
  filter(!str_detect(sample_name, "RHZ|PPR")) %>% 
  separate(
    col = sample_name,
    sep = "_",
    into = c("Number", "Site", "Samp_Time", "Zone", "Replicate", "Depth"),
    remove = FALSE) 

```

## Pulling Rhizon Samples
```{#r Rhizon Sample Wrangle}

# Filter rhizon and peeper samples
df_rhizon <- dat_flagged %>%
  filter(str_detect(Sample_Name, "RHZ"))
df_peep <- dat_flagged %>%
  filter(str_detect(Sample_Name, "PPR"))

# Timestamp for backups
timestamp <- format(Sys.time(), "%Y-%m-%d_%H%M")

# Paths
folder_path <- file.path("Raw Data", "Rhizon+Peeper")
dir.create(folder_path, recursive = TRUE, showWarnings = FALSE)

rhizon_main <- file.path(folder_path, "rhizon_data.csv")
peeper_main <- file.path(folder_path, "peeper_data.csv")

rhizon_backup <- file.path(folder_path, paste0("rhizon_data_", timestamp, ".csv"))
peeper_backup <- file.path(folder_path, paste0("peeper_data_", timestamp, ".csv"))

# Write timestamped backups
write.csv(df_rhizon, rhizon_backup, row.names = FALSE)
write.csv(df_peep, peeper_backup, row.names = FALSE)

# Overwrite the main files with latest data
write.csv(df_rhizon, rhizon_main, row.names = FALSE)
write.csv(df_peep, peeper_main, row.names = FALSE)

## ^^ I think there is a cleaner way to write this out, but this should work for now ^^

```

## Check to see if samples run match metadata & merge info
```{r, check sample ids with metadata, echo=FALSE}

cat("Check Sample IDs with Metadata")

#remove duplicates from the sample dataframe bc we are just taking the first dup run
all_data_flagged <- samples_flagged %>%
  filter(!str_detect(sample_ID, "dup")) %>%
  filter(!str_detect(sample_ID, "spk")) %>%
  mutate(sample_ID = sub("^[^_]+_", "", sample_ID))
  #select(!Excess_Info)

#check to see if all samples are present in the metadata 
all_present <- all(all_data_flagged$sample_ID %in% dionex_metadata$Cl_SO4_ID)

if (all_present) {
  message("All sample IDs are present in metadata.")
} else {
  message("Some sample IDs are missing from metadata.")
  
  # Optional: Which ones are missing?
  missing_ids <- setdiff(all_data_flagged$sample_ID, dionex_metadata$Cl_SO4_ID)
  print(missing_ids)
}

dionex_metadata_selected <- dionex_metadata %>%
  select(Cl_SO4_ID, Year, Month, Day, Depth_cm, Lysimeter, Time..24hr., Time.Zone_EDT.EST,  Field.Notes) 

#merge metadata with sample run data 
merged_data <- all_data_flagged %>%
  left_join(dionex_metadata_selected, by = c("sample_ID" = "Cl_SO4_ID"))

```


## Visualize Data by Plot   
```{r Visualize Data, echo=FALSE}

## Remove once you fix the flagging chunk 
#Pull in data that from the raw data file based on the sample info input above 
dat_flagged <- all_dat %>%
  filter(!str_detect(sample_ID, "PPR")) %>%
  filter(!str_detect(sample_ID, "_dup")) %>%
  filter(!str_detect(sample_ID, "_spk")) %>%
  filter(str_detect(sample_ID, samples_pattern))

cat("Visualize Data")

#Plot samples to get a first look at concentrations (sanity check)
IDs <- data.frame(do.call('rbind', strsplit(as.character(dat_flagged$sample_name),'_',fixed=TRUE)))
colnames(IDs) <- c("Sample_Number", "Site_Code","Date" ,"Zone", "Lysimeter", "Depth") #, "Excess_Info")
#head(IDs)

#rejoin them to the dataframe
dat_flagged_id <- cbind(IDs, dat_flagged)

#group the data for plotting
dat_flagged_id <- dat_flagged_id %>%
  group_by(Site_Code) %>%
  mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
  ungroup()

#Plot data and change colors based on flags to check it: 
viz_cl_plot <- ggplot(dat_flagged_id, aes(x = factor(row_num), y = Cl_ppm, fill = Zone)) +
  geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
  facet_grid(~ Site_Code, scales="free_x") +
  scale_fill_manual(values = c(
    "UP" = "#20063B",
    "TR" = "#FFBC42",
    "SWAMP" = "darkgrey",
    "WC" = "#419973",
    "SW" = "#25ABE6"
  )) +
  theme_classic() +
  labs(x = " ", y = "Cl (mg/L)", title = "Samples: Chloride") +
  theme(legend.position = "none") +
  scale_x_discrete(drop = TRUE)


viz_so4_plot <-  ggplot(dat_flagged_id, aes(x = row_num, y = SO4_ppm, fill = Zone)) +
  geom_bar(stat = "identity", position = position_dodge2(preserve = "single")) +
  facet_grid(~ Site_Code, scales="free_x") +
  scale_fill_manual(values = c(
    "UP" = "#20063B",
    "TR" = "#FFBC42",
    "SWAMP" = "darkgrey",
    "WC" = "#419973",
    "SW" = "#25ABE6"
  )) +
  theme_classic() +
  labs(x = " ", y = "SO4 (mg/L)", title = "Samples: Sulfate") +
  theme(legend.position = "none") +
  scale_x_discrete(drop = TRUE)

ggarrange(viz_cl_plot, viz_so4_plot, nrow=2, ncol=1)

```



## Export Processed Data  
```{r, Export Processed Data, echo=FALSE}

cat("Export Processed Data")

#Prepare data to be exported - if there is anything else to add 
#Add any necessary identifiers to the samples  ### VERY IMPORTANT AND STANDARD FOR PROJECT ####
  #example read in sample IDs list and merge 
  #create required ID columns in R, etc. 
final_data <- merged_data %>% 
  select(!row_num) %>%
  mutate(
    Project = "COMPASS: Synoptic",   # new column with same value on every row
    Region = "CB",
    Run_notes = run_notes     # new column with notes about the run
  ) 

final_data <- final_data %>%
  rename(
    Site = Site_Code,
    Time = Time..24hr., 
    Time_Zone = Time.Zone_EDT.EST,
    Replicate = Lysimeter.y,
    npoc_mgL = npoc_raw, 
    tdn_mgL = tdn_raw,
    Sample_ID = sample_name,
    Analysis_runtime = run_datetime, 
    Field_notes = Field.Notes
    # add more rename pairs as needed
  ) %>%
  select(
    Project, Region, Site, Zone, Replicate, Depth_cm, Sample_ID, Year, Month, Day, 
    Time, Time_Zone, npoc_mgL, npoc_uM, npoc_flag, tdn_mgL, tdn_uM, tdn_flag, Analysis_runtime,
    Run_notes, Field_notes, 
    # list columns in the order you want them
  )

head(final_data)

#will put final data in processed data folder 
#Write out data frame 
  write.csv(final_data, processed_file_name)

```


#end
