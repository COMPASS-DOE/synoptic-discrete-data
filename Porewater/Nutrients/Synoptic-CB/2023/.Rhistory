}
if (NH3_spk_percent <= chks_flag) {
df_all_cor$NH3_flag <- ifelse(
df_all_cor$NH3_flag != "",
paste0(df_all_cor$NH3_flag, "; NH3 spikes out of range"),
"NH3 spikes out of range"
)
}
if (PO4_spk_percent <= chks_flag) {
df_all_cor$PO4_flag <- ifelse(
df_all_cor$PO4_flag != "",
paste0(df_all_cor$PO4_flag, "; PO4 spikes out of range"),
"PO4 spikes out of range"
)
}
##IVB can you build this out a bit so that it reports per test?
df_matrix <- df_spk_combo%>%
filter(str_detect(Pair_ID, "Abs_Chk"))
df_matrix_NOx <- df_matrix %>%
filter(str_detect(Test, "Vanadium"))
ifelse(
all(df_matrix_NOx$Spike_diff_flag == "YES"),
"NO NOx Matrix Effect, PROCEED",
">20% CV in ASW NOx matrix effect checks - REASSESS"
)
df_matrix_NH3 <- df_matrix %>%
filter(str_detect(Test, "Vanadium"))
ifelse(
all(df_matrix_NH3$Spike_diff_flag == "YES"),
"NO NH3 Matrix Effect, PROCEED",
">20% CV in ASW NH3 matrix effect checks - REASSESS"
)
df_matrix_PO4 <- df_matrix %>%
filter(str_detect(Test, "Vanadium"))
ifelse(
all(df_matrix_PO4$Spike_diff_flag == "YES"),
"NO PO4 Matrix Effect, PROCEED",
">20% CV in ASW PO4 matrix effect checks - REASSESS"
)
#write out a flag to the sample dataframe if more than 60% of the dups have CVs out of range
if (any(df_matrix$Spike_diff_flag[df_matrix$Test == "Vanadium NOx"] != "YES")) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; NOx matrix check out of range"),
"NOx matrix check out of range"
)
}
if (any(df_matrix$Spike_diff_flag[df_matrix$Test == "Ammonia 2"] != "YES")) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; NH3 matrix check out of range"),
"NH3 matrix check out of range"
)
}
if (any(df_matrix$Spike_diff_flag[df_matrix$Test == "o-PHOS 0.3"] != "YES")) {
df_all_cor$NOx_flag <- ifelse(
df_all_cor$NOx_flag != "",
paste0(df_all_cor$NOx_flag, "; PO4 matrix check out of range"),
"PO4 matrix check out of range"
)
}
#Pull out samples
samples <- df_all_cor %>%
filter(str_detect(Sample_Name, c("GCW|GWI|MSM|SWH")))
#Convert values based on the Test ID
samples <- samples %>%
mutate(
Conc_uM = case_when(
Test == "Vanadium NOx" ~ (((as.numeric(samples$Conc))/Con1)/N_mw)*Con2,
Test == "Ammonia 2" ~ (((as.numeric(samples$Conc))/Con1)/N_mw)*Con2,
Test == "o-PHOS 0.3" ~ (((as.numeric(samples$Conc))/Con1)/P_mw)*Con2,
TRUE ~ as.numeric(NA)  # fallback in case of unexpected value
),
# Replace negatives with 0 in 'value' and 'value_converted'
Conc = pmax(Conc, 0),
Conc_uM = pmax(Conc_uM, 0)
)
head(samples)
cat("Sample Flagging")
#Flagging data if the concentration is outside the standards range
samples_flagged <- samples %>%
mutate(
Conc_flag = case_when(
Test == "Vanadium NOx" & Conc < NOx_dl ~ "bdl",
Test == "Vanadium NOx" & Conc > NOx_top    ~ "adl",
Test == "Vanadium NOx"               ~ "Within_Range",
Test == "Ammonia 2" & Conc < NH3_dl ~ "bdl",
Test == "Ammonia 2" & Conc > NH3_top    ~ "adl",
Test == "Ammonia 2"                ~ "Within_Range",
Test == "o-PHOS 0.3" & Conc < PO4_dl  ~ "bdl",
Test == "o-PHOS 0.3" & Conc > PO4_top   ~ "adl",
Test == "o-PHOS 0.3"                ~ "Within_Range",
TRUE ~ NA_character_  # fallback for unexpected values
)
)
cat("Sample Processing")
df_all_clean <- samples %>%
filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = Sample_Name,
sep = "_",
into = c("Site", "Samp_Time", "Zone", "Replicate", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(Samp_Time)) %>%
mutate(
Site = case_when(
Site == "GCW" ~ "GCReW",
Site == "GWI" ~ "Goodwin Island",
Site == "MSM" ~ "Moneystump",
Site == "SWH" ~ "Sweethall",
TRUE ~ Site),
`Zone` = case_when(
`Zone` == "UP" ~ "Upland",
`Zone` == "TR" ~ "Transition",
`Zone` == "WC" ~ "Wetland",
`Zone` == "SWAMP" ~ "Swamp",
`Zone` == "SW" ~ "Surface Water",
TRUE ~ `Zone`
))
cat("Check Sample IDs with Metadata")
#check to see if all samples are present in the metadata
all_present <- all(samples_flagged$Sample_Name %in% nutr_metadata$NUTR_ID)
if (all_present) {
message("All sample IDs are present in metadata.")
} else {
message("Some sample IDs are missing from metadata.")
# Optional: Which ones are missing?
missing_ids <- setdiff(samples_flagged$Sample_Name, nutr_metadata$NUTR_ID)
print(missing_ids)
}
nutr_metadata_selected <- nutr_metadata %>%
select(NUTR_ID, Year, Month, Day, Depth_cm, Lysimeter, Time..24hr., Time.Zone_EDT.EST,  Field.Notes)
#merge metadata with sample run data
merged_data <- samples_flagged %>%
left_join(nutr_metadata_selected, by = c("Sample_Name" = "NUTR_ID"))
df_all_clean <- merged_data %>%
#filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
#mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = Sample_Name,
sep = "_",
into = c("Site", "Samp_Time", "Zone", "Replicate", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(Samp_Time))
cat("Visualize Data")
### Nitrite + Nitrate
NOx_forplot <- df_all_clean %>%
filter(Test == "Vanadium NOx")
#group the data for plotting
NOx_forplot <- NOx_forplot %>%
group_by(Site) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
viz_NOx_plot <-  ggplot(data = NOx_forplot, aes(x = factor(row_num), y = Conc, fill=Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"),
color="black") +
facet_grid(~ Site, scales="free_x") +
scale_fill_manual(values = c("UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6")) +
theme_classic() +
labs(x= " ", y="NOx (mg/L)", title="Porewater NOx") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
### Ammonia
NH3_forplot <- df_all_clean %>%
filter(Test == "Ammonia 2")
#group the data for plotting
NH3_forplot <- NH3_forplot %>%
group_by(Site) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
viz_NH3_plot <- ggplot(data = NH3_forplot, aes(x = factor(row_num), y = Conc, fill=Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"),
color="black") +
facet_grid(~ Site, scales="free_x") +
scale_fill_manual(values = c("UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6")) +
theme_classic() +
labs(x= " ", y="H3 (mg/L)", title="Porewater NH3") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
### Phosphate
PO4_forplot <- df_all_clean %>%
filter(Test == "o-PHOS 0.3")
#group the data for plotting
PO4_forplot <- PO4_forplot %>%
group_by(Site) %>%
mutate(row_num = factor(row_number())) %>%  # create row_num as factor per group
ungroup()
viz_PO4_plot <- ggplot(data = PO4_forplot, aes(x = factor(row_num), y = Conc, fill=Zone)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"),
color="black") +
facet_grid(~ Site, scales="free_x") +
scale_fill_manual(values = c("UP" = "#20063B",
"TR" = "#FFBC42",
"SWAMP" = "darkgrey",
"WC" = "#419973",
"SW" = "#25ABE6")) +
theme_classic() +
labs(x= " ", y="PO4 (mg/L)", title="Porewater PO4") +
theme(legend.position = "none") +
scale_x_discrete(drop = TRUE)+
theme(axis.text.x = element_text(angle = 90, hjust = 0.5))
print(viz_NOx_plot)
print(viz_NH3_plot)
print(viz_PO4_plot)
cat("Export Processed Data")
#pivot the data set wider to make it wide format
final_data1 <- df_all_clean %>%
pivot_wider(
id_cols = Sample_Name,
names_from = Test,
values_from = Conc,
names_glue = "{Test}_Conc"
)
final_data2 <- df_all_clean %>%
pivot_wider(
id_cols = Sample_Name,
names_from = Test,
values_from = Conc_uM,
names_glue = "{Test}_Conc_uM"
)
final_data3 <- df_all_clean %>%
pivot_wider(
id_cols = Sample_Name,
names_from = Test,
values_from = Conc_flag,
names_glue = "{Test}_Conc_flag"
)
#take out only the columns we want to merge
df_all_clean_cols <- df_all_clean %>%
select(Sample_Name, Site, Zone, Depth, Depth_cm, Lysimeter,
Year, Month, Day, Time..24hr., Time.Zone_EDT.EST,
NOx_flag, NH3_flag, PO4_flag,
Field.Notes)
df_all_clean_cols_one_row <- df_all_clean_cols %>%
group_by(Sample_Name) %>%
slice(1) %>%  # or use summarise() if you want to aggregate
ungroup()
#merge these together
data_list <- list(df_all_clean_cols_one_row, final_data1, final_data2, final_data3)
final_data4 <- reduce(data_list, full_join, by = c("Sample_Name"))
#Add project information
final_data_labeled <- final_data4 %>%
mutate(
Project = "COMPASS: Synoptic",   # new column with same value on every row
Region = "CB",
Run_notes = run_notes,
Analysis_rundate = print(run_date)# new column with notes about the run
)
#Prepare data to be exported
final_data <- final_data_labeled %>%
rename(
Site = Site,
Sample_ID = Sample_Name,
Time = Time..24hr.,
Time_Zone = Time.Zone_EDT.EST,
Replicate = Lysimeter,
NOx_Conc_mgL = `Vanadium NOx_Conc`,
NOx_Conc_uM = `Vanadium NOx_Conc_uM`,
NOx_Conc_Flag = `Vanadium NOx_Conc_flag`,
NOx_QAQC_Flag = NOx_flag,
NH3_Conc_mgL = `Ammonia 2_Conc`,
NH3_Conc_uM = `Ammonia 2_Conc_uM`,
NH3_Conc_Flag =`Ammonia 2_Conc_flag`,
NH3_QAQC_Flag = NH3_flag,
PO4_Conc_mgL = `o-PHOS 0.3_Conc`,
PO4_Conc_uM = `o-PHOS 0.3_Conc_uM`,
PO4_Conc_Flag =`o-PHOS 0.3_Conc_flag`,
PO4_QAQC_Flag = PO4_flag,
Field_notes = Field.Notes
# add more rename pairs as needed
) %>%
select(Project, Region, Site, Zone, Replicate, Depth_cm,
Sample_ID, Year, Month, Day, Time, Time_Zone,
NOx_Conc_mgL, NOx_Conc_uM, NOx_Conc_Flag, NOx_QAQC_Flag,
NH3_Conc_mgL, NH3_Conc_uM, NH3_Conc_Flag, NH3_QAQC_Flag,
PO4_Conc_mgL, PO4_Conc_uM, PO4_Conc_Flag, PO4_QAQC_Flag,
Analysis_rundate,  Run_notes, Field_notes)
#Write out data frame
write.csv(final_data, final_path)
View(df_all_clean)
View(samples)
View(df_all)
cat("Rhizon Samples")
#we can make a new dataframe here that has rhizon & peeper samples
#if they exist pulled into a new dataframe and exported to a folder in processed data called Rhizons
# Filter rhizon and peeper data
df_rhizon <- df_all %>%
filter(str_detect(Sample_Name), "RHZ")
df_rhizon <- df_all %>%
filter(str_detect(Sample_Name, "RHZ"))
df_peep <- df_all %>%
filter(str_detect(Sample_Name, "PPR"))
cat("Rhizon Samples")
#we can make a new dataframe here that has rhizon & peeper samples
#if they exist pulled into a new dataframe and exported to a folder in processed data called Rhizons
# Filter rhizon and peeper data
df_rhizon <- df_all %>%
filter(str_detect(Sample_Name, "RHZ"))
df_peep <- df_all %>%
filter(str_detect(Sample_Name, "PPR"))
# Timestamp for backups
timestamp <- format(Sys.time(), "%Y-%m-%d_%H%M")
# Paths
folder_path <- file.path("Raw Data", "Rhizon+Peeper")
dir.create(folder_path, recursive = TRUE, showWarnings = FALSE)
rhizon_main <- file.path(folder_path, "rhizon_data.csv")
peeper_main <- file.path(folder_path, "peeper_data.csv")
rhizon_backup <- file.path(folder_path, paste0("rhizon_data_", timestamp, ".csv"))
peeper_backup <- file.path(folder_path, paste0("peeper_data_", timestamp, ".csv"))
# Write timestamped backups
write.csv(df_rhizon, rhizon_backup, row.names = FALSE)
write.csv(df_peep, peeper_backup, row.names = FALSE)
# Overwrite the main files with latest data
write.csv(df_rhizon, rhizon_main, row.names = FALSE)
write.csv(df_peep, peeper_main, row.names = FALSE)
cat("Check Sample IDs with Metadata")
#check to see if all samples are present in the metadata
all_present <- all(samples_flagged$Sample_Name %in% nutr_metadata$NUTR_ID)
if (all_present) {
message("All sample IDs are present in metadata.")
} else {
message("Some sample IDs are missing from metadata.")
# Optional: Which ones are missing?
missing_ids <- setdiff(samples_flagged$Sample_Name, nutr_metadata$NUTR_ID)
print(missing_ids)
}
nutr_metadata_selected <- nutr_metadata %>%
select(NUTR_ID, Year, Month, Day, Depth_cm, Lysimeter, Time..24hr., Time.Zone_EDT.EST,  Field.Notes)
#merge metadata with sample run data
merged_data <- samples_flagged %>%
left_join(nutr_metadata_selected, by = c("Sample_Name" = "NUTR_ID"))
df_all_clean <- merged_data %>%
#filter(str_detect(Sample_Name, "GCW|GWI|MSM|SWH")) %>%
#mutate(Run_Time = lubridate::mdy_hm(Run_Time)) %>%
separate(
col = Sample_Name,
sep = "_",
into = c("Site", "Samp_Time", "Zone", "Replicate", "Depth"),
remove = FALSE) %>%
mutate(Samp_Time = ym(Samp_Time))
############### THIS IS ONLY FOR 2023 when SWAMP was called UP and UPLAND was called UPCON ##############
df_all_clean <- df_all_clean %>%
mutate(
Zone = case_when(
Site == "SWH" & Zone == "UP" ~ "SWAMP",
Site == "SWH" & Zone == "UPCON" ~ "UP",
TRUE ~ `Zone`
))
#let you know which section you are in
cat("Run Information: Input by User")
#set the run date & user name
run_date <- "20240114"
sample_year <- 2023
sample_month <- 06
user <- "Stephanie Wilson"
#identify the files you want to read in
#read in as a list to accommadate ultiple runs in a month
NOx_files <- c("Raw Data/SEAL_COMPASS_Synoptic_NOx_June2023_1.csv",
"Raw Data/SEAL_COMPASS_Synoptic_NOx_June2023_2.csv",
"Raw Data/SEAL_COMPASS_Synoptic_NOx_June2023_3.csv")
NH3_PO4_files <- c("Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_June2023_1.csv",
"Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_June2023_2.csv",
"Raw Data/SEAL_COMPASS_Synoptic_NH3_PO4_June2023_3.csv")
# Define the file path for QAQC log file - NO Need to change just check year
file_path <- "Raw Data/SEAL_COMPASS_Synoptic_QAQC_Log_2023.csv"
final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_202304.csv"
#record any notes about the run or anything other info here:
run_notes <- "There are two sample names we suspect were input incorrectly,
they are listed below and have been checked against metadata. The metadata from Goodwin samples is not present."
#duplicate sample names to be changed
#list the sample iDs that are messed up and create a list
#with run number as well so that we can change them below
wrong_names <- c("GCW_202304_TR_LysC_45cm", "GCW_202304_TR_LysA_20cm_8",
"GWI_202304_UP_LysA_20cm", "GWI_202304_UP_LysA_20cm")
wrong_nums <- c(20, 16, 46, 44)
correct_names <- c("GCW_202304_TR_LysB_45cm", "GCW_202304_TR_LysA_20cm",
"GWI_202304_UP_LysA_10cm", "GWI_202304_UP_LysA_10cm")
#can't determine from metadata - for now unsure
remove_names <- c("GCW_202304_TR_LysA_20cm", "GCW_202304_TR_LysA_20cm",
"GCW_202304_TR_LysB_20cm_13", "GCW_202304_TR_LysB_20cm_13")
#couldn't tell which onethis is from the metadata, no A_10cm which is what we thought
#marked on the sheet, need to check sample vials in freezer
#to see if we have a A_10cm from GCW_TR to be sure
remove_nums <- c(15, 13, 21, 19 )
#Set up file path for metadata
#downloaded metadata csv - downloaded from Google drive as csv for this year
#https://docs.google.com/spreadsheets/d/1HCAN0_q6y17x0RUXVzID09hVal-RfwWc/edit?usp=sharing&ouid=108994740386869376571&rtpof=true&sd=true
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
cat("Assess Standard Curves")
#Plot standard Curve or Curves
#v-Nox
std_curve_NOx <- ggplot(stds_NOx, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", formula = y ~ poly(x, 2, raw = TRUE), color = "coral") +
theme_bw() +
facet_wrap(~Run_Date) +
labs(x="Concentration (ppm)",
y="Absorbance",
title = "NOx Standard Curve")+
annotate("text", x = 0.75, y = max(stds_NOx$Absorbance),
label = paste("r^2 =", round(NOx_r2, 4)),
color = "black", size = 4)
print(std_curve_NOx)
#NH3
std_curve_NH3 <- ggplot(stds_NH3, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkgoldenrod") +
theme_bw() +
facet_wrap(~Run_Date) +
labs(x="Concentration (ppm)",
y="Absorbance",
title = "NH3 Standard Curve")+
annotate("text", x = 1.7, y = max(stds_NH3$Absorbance), label = paste("r^2 =", round(NH3_r2, 4)),
color = "black", size = 4)
print(std_curve_NH3)
#PO4
std_curve_PO4 <- ggplot(stds_PO4, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkkhaki") +
facet_wrap(~Run_Date) +
theme_bw() +
labs(x="Concentration (ppm)",
y="Absorbance",
title = "PO4 Standard Curve")+
annotate("text", x = 0.25, y = max(stds_PO4$Absorbance), label = paste("r^2 =", round(PO4_r2, 4)),
color = "black", size = 4)
print(std_curve_PO4)
############## Report on Cutoffs
#Report out a flag if the run has an R2 lower than appropriate
#Write out to the user whether or not the r2 is above the cutoff of 0.98
ifelse(std_curve_NOx_1$R_squared <= r2_cutoff,
"NOx Curve r2 is below cutoff! - REASSESS",
"NOx Curve r2 GOOD - PROCEED")
ifelse(std_curve_NH3_1$R_squared <= r2_cutoff,
"NH3 Curve r2 is below cutoff! - REASSESS",
"NH3 Curve r2 GOOD - PROCEED")
ifelse(std_curve_PO4_1$R_squared <= r2_cutoff,
"PO4 Curve r2 is below cutoff! - REASSESS",
"PO4 Curve r2 GOOD - PROCEED")
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(file_path)) {
# If it exists, read it back into R
stds_log <- read.csv(file_path)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
stds_log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(stds_log) <- c("Test", "Intercept", "Slope", "R_squared", "run_date", "user")
#maybe add here the last slopes from the previous year
# Write add_log to CSV
write.csv(stds_log, file = file_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#getting rid of first "X" column
stds_log <- stds_log %>%
select(-1)
#create a dataframe that can be appended to the QAQC log
add_log <- as.data.frame(bind_rows(std_curve_NOx_1, std_curve_NH3_1, std_curve_PO4_1))
add_log$run_date <- run_date
add_log$user <- user
#compare slopes to previous runs (from log) in order to assess drift
stds_log$run_date <- as.character(stds_log$run_date)
add_log$run_date  <- as.character(add_log$run_date)     #getting the run_date column in the same type to merge
# Filter to only rows in Slopes that are NOT already in log (by run_date + analyte)
new_rows <- anti_join(add_log, stds_log, by = c("run_date", "Test"))
# Append the new, non-duplicate rows to log
log <- bind_rows(stds_log, new_rows)
#Plot the slopes through time
Slopes_chk <- ggplot(log, aes(run_date, Slope, col=Test)) +
geom_point(size=4) +
geom_line(aes(group = Test)) +
theme_bw() +
labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
scale_color_manual(values=c("coral", "darkgoldenrod", "darkkhaki"))
Slopes_chk
#averaging the slopes from the log and printing
avg_slopes <- log %>%
group_by(Test) %>%
summarise(avg_slope = mean(Slope, na.rm = TRUE))
print(avg_slopes)
#write out the log file with the added lines for this run date
write.csv(log, file_path)
#write out a flag to the sample dataframe if the r2 is above the cutoff of 0.98
df_all <- df_all %>%
mutate(
NOx_flag = if (NOx_r2 <= r2_cutoff) {
"NOx r2 low"
} else {
""
},
NH3_flag = if (NH3_r2 <= r2_cutoff) {
"NH3 r2 low"
} else {
""
},
PO4_flag = if (PO4_r2 <= r2_cutoff) {
"PO4 r2 low"
} else {
""
}
)
