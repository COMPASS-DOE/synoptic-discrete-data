PO4_Conc_mgL, PO4_Conc_uM, PO4_Conc_Flag, PO4_QAQC_Flag,
Analysis_rundate,  Run_notes, Field_notes)
#Write out data frame
write.csv(final_data, final_path)
cat("Run Information: Isabelle Van Benschoten") #lets you know what section you're in
#set the run date & user name
run_date <- "20230603/20230111"
sample_year <- 2023
sample_month <- 05
user <- "Stephanie Wilson"
#identify the files you want to read in
#read in as a list to accommodate multiple runs in a month
files <- c("Raw Data/COMPASS_Synoptic_Nutrients_clean_2023_May.csv")
file_stds <- ("Raw Data/COMPASS_Synoptic_Nutrients_df_all_2023.csv")
# Define the file path for QAQC log file - NO Need to change just check year
file_path <- "Raw Data/SEAL_COMPASS_Synoptic_QAQC_Log_2023.csv"
final_path <- "Processed Data/COMPASS_Synoptic_Nutrients_May2023.xlsx"
#record any notes about the run or anything other info here:
run_notes <- "Multiple months of data were mixed together so they were parsed
and cleaned prior to read in.
Samples ran on 01/11/24 were not included because of boor standard curve.
Autospikes for PO4 not included in this run.
Sample TEMPEST_20230513_CON_C6 was input twice.
NH3 and PO4 have low number of duplicates due to the bad standard curve run"
#duplicate sample names to be changed
#list the sample iDs that are messed up and create a list
#with run number as well so that we can change them below
# wrong_names <- c("GCW_202304_TR_LysC_45cm", "GCW_202304_TR_LysA_20cm_8",
#                  "GWI_202304_UP_LysA_20cm", "GWI_202304_UP_LysA_20cm")
#  wrong_nums <- c(20, 16, 46, 44)
#  correct_names <- c("GCW_202304_TR_LysB_45cm", "GCW_202304_TR_LysA_20cm",
#                     "GWI_202304_UP_LysA_10cm", "GWI_202304_UP_LysA_10cm")
#can't determine from metadata - for now unsure
# remove_names <- c("GCW_202304_TR_LysA_20cm", "GCW_202304_TR_LysA_20cm",
#                  "GCW_202304_TR_LysB_20cm_13", "GCW_202304_TR_LysB_20cm_13")
#     #couldn't tell which one this is from the metadata, no A_10cm which is what we thought
#marked on the sheet, need to check sample vials in freezer
#to see if we have a A_10cm from GCW_TR to be sure
#remove_nums <- c(15, 13, 21, 19 )
#Set up file path for metadata
#downloaded metadata csv - downloaded from Google drive as csv for this year
#https://docs.google.com/spreadsheets/d/1HCAN0_q6y17x0RUXVzID09hVal-RfwWc/edit?usp=sharing&ouid=108994740386869376571&rtpof=true&sd=true
Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2023.csv"
#let you know which section you are in
cat("Setup")
#a link to the Gitbook or whatever protocol you are using for this analysis
#load/install packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
dplyr,
purrr,
ggplot2,
ggpubr,
tidyr,
tidyverse,
stringr,
readr,
LaTeX,
writexl,
tinytex,
write_excel,
broom)
#Coefficients / constants that are needed for calculations
N_mw <- 14.0067    # molecular weight of N
P_mw <- 30.973762  # molecular weight of P
Con1 <- 1000       # conversion factor value
Con2 <- 1000000    # conversion factor value
#Detection limit and top standards for flagging
NOx_dl <- 0.025  #mgL
NH3_dl <- 0.02  #mgL
PO4_dl <- 0.003 #mgL
NOx_top <- 1  #mgL
NH3_top <- 2  #mgL
PO4_top <- 0.3 #mgL
#Check Standard concnetrations
NOx_CCV <- 0.5
NH3_CCV <- 1.0
PO4_CCV <- 0.15
#Spike Concentrations
NOx_Spk <- 0.5 #ppm or mg/L
NH3_Spk <- 1
PO4_Spk <- 0.152
#expected ranges for sample concentrations used for flags
r2_cutoff = 0.990
chk_flag = 0.10
chk_conc_flag = 15 #cutoff level for percent difference of check stds vs. expected concentration
chks_flag = 60
rep_flag = 25
#^this is a 25% error between samples
#blank_flag is calculated based on samples later in this code
#pe check Concentrations
NH3_pe <- 0.948
NOx_pe <- 0.706
PO4_pe <- 0.818
#read in the raw metadata file
raw_metadata <- read.csv(Raw_Metadata)
#make a new columns in the metadata with important info:
metadata <- raw_metadata %>%
mutate(Depth = paste0(Depth_cm, "cm")) %>%
mutate(LysID = paste0("Lys", Lysimeter)) %>%
mutate(YearMonth = sprintf("%d%02d", Year, Month))  %>%
mutate(Zone = case_when(
`Transect.Location` == "Transition" ~ "TR",
`Transect.Location` == "Wetland"    ~ "WC",
`Transect.Location` == "Upland"     ~ "UP",
`Transect.Location` == "Surface Water" ~ "SW",
TRUE                 ~ `Transect.Location`    # keep original value if no match
))
#Create NUTR IDs from what was collected for comparison later
metadata <- metadata %>%
mutate(NUTR_ID = ifelse(NUTR == "x",
paste(Site,
YearMonth,
Zone,
LysID,
Depth,
sep = "_"),
NA) )
#Change the SW lines because they don't have lysimeters or a depth
metadata <- metadata %>%
mutate(
NUTR_ID = if_else(
Zone == "SW",
# Modify the string:
NUTR_ID %>%
str_replace("_LysA", "_A") %>%                    # Replace "_LysA" with "_A"
str_replace("_LysB", "_B") %>%                    # Replace "_LysB" with "_B"
str_replace("_LysC", "_C") %>%                    # Replace "_LysC" with "_C"
str_replace("_0cm$", ""),                         # Remove trailing "_0cm"
NUTR_ID  # else keep original
)
)
#Take out the columns and rows that are not relevant
nutr_metadata <- metadata %>%
select(NUTR_ID, Year, Month, Day, YearMonth, Site, Zone, Lysimeter, LysID, Depth_cm,
Depth, Time..24hr., Time.Zone_EDT.EST, Field.Notes, ) %>%
# only keep specific columns
filter(!is.na(NUTR_ID) & NUTR_ID != "")  # remove missing/blank NUTR_ID rows
cat("Import Data")
#set file path for data - in run info chunk
path <- ("file path")
#Read in Raw Data
df_all <- read.csv(files)
df_all_stds <- read.csv(file_stds)
cat("Assess Standard Curves")
#Pull out standards
stds <- df_all_stds %>%
filter(str_detect(Sample_Name, "Standard"))
#Making standards dataframe based on the test
stds_NOx <- stds[stds$Test == "Vanadium NOx", ]
stds_NH3 <- stds[stds$Test == "Ammonia 2", ]
stds_PO4 <- stds[stds$Test == "o-PHOS 0.3", ]
#Inputting Standard Values Based on Protocol (LINK PROTOCOL)
stds_NOx <- stds_NOx %>%
mutate(`Conc` = case_when(
Sample_Name == "Standard 0" ~  0.0,
Sample_Name == "Standard 1" ~  0.0,
Sample_Name == "Standard 90" ~ 0.0222,
Sample_Name == "Standard 91" ~ 0.05,
Sample_Name == "Standard 92" ~ 0.1,
Sample_Name == "Standard 93" ~ 0.25,
Sample_Name == "Standard 94" ~ 0.5,
Sample_Name == "Standard 95" ~ 0.75,
Sample_Name == "Standard 96" ~ 1.0,
TRUE ~ `Conc`),  # leave unchanged if no match
Run_Date = format(mdy_hm(Run_Time), "%m-%d-%Y")) %>%
filter(!Sample_Name %in% c("Nitrate Standard" , "Nitrite Standard"))
#ID x and y
x1 <- stds_NOx$Absorbance
y1 <- stds_NOx$Conc
stds_NH3 <- stds_NH3 %>%
mutate(`Conc` = case_when(
Sample_Name == "Standard 0" ~  0.0,
Sample_Name == "Standard 1" ~  0.0,
Sample_Name == "Standard .0389" ~ 0.0389,
Sample_Name == "Standard .1000" ~ 0.1,
Sample_Name == "Standard .2000" ~ 0.2,
Sample_Name == "Standard .5000" ~ 0.5,
Sample_Name == "Standard 1.0000" ~ 1.0,
Sample_Name == "Standard 1.5000" ~ 1.5,
Sample_Name == "Standard 2.0000" ~ 2,
TRUE ~ `Conc`),  # leave unchanged if no match
Run_Date = format(mdy_hm(Run_Time), "%m-%d-%Y"))
# identifying x and y
x2 <- stds_NH3$Absorbance
y2 <- stds_NH3$Conc
stds_PO4 <- stds_PO4 %>%
mutate(`Conc` = case_when(
Sample_Name == "Standard 0" ~  0.0,
Sample_Name == "Standard 1" ~  0.0,
Sample_Name == "Standard 90" ~ 0.0060,
Sample_Name == "Standard 91" ~ 0.0150,
Sample_Name == "Standard 92" ~ 0.0300,
Sample_Name == "Standard 93" ~ 0.0750,
Sample_Name == "Standard 94" ~ 0.1500,
Sample_Name == "Standard 95" ~ 0.2250,
Sample_Name == "Standard 96" ~ 0.3000,
TRUE ~ `Conc`),  # leave unchanged if no match
Run_Date = format(mdy_hm(Run_Time), "%m-%d-%Y"))
#ID x and y
x3 <- stds_PO4$Absorbance
y3 <- stds_PO4$Conc
#generating line of best fit aka standard curves
#NOx
# Fit the quadratic model
quad_reg_NOx <- lm(y1 ~ x1 + I(x1^2))
#quad_reg_NOx <- lm(Absorbance ~ Conc + Conc2, data = stds_NOx)
NOx_coefs <- coef(quad_reg_NOx)                        # intercept and slopes
NOx_r2 <- summary(quad_reg_NOx)$r.squared              # R squared
# Store in a clean dataframe
std_curve_NOx_1 <- data.frame(
Test = "NOx",
Intercept = NOx_coefs["(Intercept)"],
Slope = NOx_coefs[2],
#Conc2_Coeff = NOx_coefs["Conc2"],
R_squared = NOx_r2
)
#NH3
lin_reg_NH3 <- lm(y2 ~ x2)
NH3_coefs <- coef(lin_reg_NH3)                        # intercept and slope
NH3_r2 <- summary(lin_reg_NH3)$r.squared              # R squared
# Store in a clean dataframe
std_curve_NH3_1 <- data.frame(
Test = "NH3",
Intercept = NH3_coefs["(Intercept)"],
Slope = NH3_coefs[2],
R_squared = NH3_r2
)
#PO4
lin_reg_PO4 <- lm(y3 ~ x3)
PO4_coefs <- coef(lin_reg_PO4)                        # intercept and slope
PO4_r2 <- summary(lin_reg_PO4)$r.squared              # R squared
# Store in a clean dataframe
std_curve_PO4_1 <- data.frame(
Test = "PO4",
Intercept = PO4_coefs["(Intercept)"],
Slope = PO4_coefs[2],
R_squared = PO4_r2
)
#Combining standards & curve data frames
stds_combo <- bind_rows(stds_NOx, stds_NH3, stds_PO4)
std_curve_combo <- bind_rows(std_curve_NOx_1, std_curve_NH3_1, std_curve_PO4_1)
cat("Assess Standard Curves")
#Plot standard Curve or Curves
#v-Nox
std_curve_NOx <- ggplot(stds_NOx, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", formula = y ~ poly(x, 2, raw = TRUE), color = "coral") +
theme_bw() +
facet_wrap(~Run_Date) +
labs(x="Concentration (ppm)",
y="Absorbance",
title = "NOx Standard Curve")+
annotate("text", x = 0.75, y = max(stds_NOx$Absorbance),
label = paste("r^2 =", round(NOx_r2, 4)),
color = "black", size = 4)
print(std_curve_NOx)
#NH3
std_curve_NH3 <- ggplot(stds_NH3, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkgoldenrod") +
theme_bw() +
facet_wrap(~Run_Date) +
labs(x="Concentration (ppm)",
y="Absorbance",
title = "NH3 Standard Curve")+
annotate("text", x = 1.7, y = max(stds_NH3$Absorbance), label = paste("r^2 =", round(NH3_r2, 4)),
color = "black", size = 4)
print(std_curve_NH3)
#PO4
std_curve_PO4 <- ggplot(stds_PO4, aes(x = Conc, y = Absorbance)) +
geom_point(size=3) +
geom_smooth(method = "lm", se = FALSE, color = "darkkhaki") +
facet_wrap(~Run_Date) +
theme_bw() +
labs(x="Concentration (ppm)",
y="Absorbance",
title = "PO4 Standard Curve")+
annotate("text", x = 0.25, y = max(stds_PO4$Absorbance), label = paste("r^2 =", round(PO4_r2, 4)),
color = "black", size = 4)
print(std_curve_PO4)
############## Report on Cutoffs
#Report out a flag if the run has an R2 lower than appropriate
#Write out to the user whether or not the r2 is above the cutoff of 0.98
ifelse(std_curve_NOx_1$R_squared <= r2_cutoff,
"NOx Curve r2 is below cutoff! - REASSESS",
"NOx Curve r2 GOOD - PROCEED")
ifelse(std_curve_NH3_1$R_squared <= r2_cutoff,
"NH3 Curve r2 is below cutoff! - REASSESS",
"NH3 Curve r2 GOOD - PROCEED")
ifelse(std_curve_PO4_1$R_squared <= r2_cutoff,
"PO4 Curve r2 is below cutoff! - REASSESS",
"PO4 Curve r2 GOOD - PROCEED")
#check for the a slope QAQC file, if there is not one, make one
if (file.exists(file_path)) {
# If it exists, read it back into R
stds_log <- read.csv(file_path)
print("QAQC log file exists and has been read into the code.")
} else {
# If it does not exist, create the CSV file
stds_log <- as.data.frame(matrix(ncol = 6, nrow = 0))
colnames(stds_log) <- c("Test", "Intercept", "Slope", "R_squared", "run_date", "user")
#maybe add here the last slopes from the previous year
# Write add_log to CSV
write.csv(stds_log, file = file_path, row.names = FALSE)
print("QAQC log file does not exist. It has been created.")
}
#getting rid of first "X" column
stds_log <- stds_log %>%
select(-1)
#create a dataframe that can be appended to the QAQC log
add_log <- as.data.frame(bind_rows(std_curve_NOx_1, std_curve_NH3_1, std_curve_PO4_1))
add_log$run_date <- run_date
add_log$user <- user
#compare slopes to previous runs (from log) in order to assess drift
stds_log$run_date <- as.character(stds_log$run_date)
add_log$run_date  <- as.character(add_log$run_date)     #getting the run_date column in the same type to merge
# Filter to only rows in Slopes that are NOT already in log (by run_date + analyte)
new_rows <- anti_join(add_log, stds_log, by = c("run_date", "Test"))
# Append the new, non-duplicate rows to log
log <- bind_rows(stds_log, new_rows)
#Plot the slopes through time
Slopes_chk <- ggplot(log, aes(run_date, Slope, col=Test)) +
geom_point(size=4) +
geom_line(aes(group = Test)) +
theme_bw() +
labs(title="Slope Drift Assessment", x="Run Date", y="Slope") +
scale_color_manual(values=c("coral", "darkgoldenrod", "darkkhaki"))
Slopes_chk
#averaging the slopes from the log and printing
avg_slopes <- log %>%
group_by(Test) %>%
summarise(avg_slope = mean(Slope, na.rm = TRUE))
knitr::kable(avg_slopes, caption = "Average Slope by Analyte", digits = 3)
#write out the log file with the added lines for this run date
write.csv(log, file_path)
#write out a flag to the sample dataframe if the r2 is above the cutoff of 0.98
df_all <- df_all %>%
mutate(
NOx_flag = if (NOx_r2 <= r2_cutoff) {
"NOx r2 low"
} else {
""
},
NH3_flag = if (NH3_r2 <= r2_cutoff) {
"NH3 r2 low"
} else {
""
},
PO4_flag = if (PO4_r2 <= r2_cutoff) {
"PO4 r2 low"
} else {
""
}
)
cat("Removing Data from Bad Curve on 01/11/24")
df_all_may <- df_all_stds %>%
filter(!str_detect(Sample_Name, "202304"))
df_all <- df_all_may %>%
mutate(Run_Time = mdy_hm(Run_Time),
Run_Date = as_date(Run_Time)) %>%
filter(!(Run_Date == as_date("2024-01-11") & Test %in% c("o-PHOS 0.3", "Ammonia 2")))
cat("Dilution Corrections")
#Calculate the concentration when accounting for the dilution factor if applicable
#checking to see if duplicate samples (mislabeled/wrongly inputted/reran)
df_duplicates <- df_all %>%
filter(!if_all(everything(), is.na)) %>%                    #removing NA rows
filter(!(Sample_Name %in% c("Duplicate",                    #removing duplicates, autospikes, ccv/bs
"CCV", "CCB", "1mg/L ammonia", "DI",
"Blank", "Auto Spike",
"0.15 mg/L")) &
!startsWith(Sample_Name, "Standard ") &               #removing standards
!startsWith(Sample_Name, "Nitrite ") &                #removing standards
!startsWith(Sample_Name, "Nitrate ") &                #removing standards
!startsWith(Sample_Name, "Abs_Chk_") &                #removing abs_chks
!startsWith(Sample_Name, "peChk_") &                 #removing peChks
!grepl("^\\d{1,2}/\\d{1,2}/\\d{4}\\s+\\d{1,2}:\\d{2}$", Sample_Name) & #removing naything that is a date
!grepl("^\\d+$", Sample_Name)) %>%                    #removing any rows with just numbers in the sample name column
group_by(Test, Sample_Name)%>%                              #grouping by test & sample name to avoid fake dups
filter(n() > 1) %>%                                         #count how many times each Sample_Name appears
ungroup()
#pulling them out into a string and printing them out
dil_dups_string <- df_duplicates %>%
distinct(Sample_Name) %>%
pull(Sample_Name) %>%
paste(collapse = ", ")
#Report duplicated samples
if (dil_dups_string != "") {
message("Duplicated samples: ", dil_dups_string)
bad_dups <- df_duplicates %>%
group_by(Sample_Name) %>%
summarise(
n_dups = n(),
has_dilution = any(Dilution > 0, na.rm = TRUE)
) %>%
filter(n_dups > 1 & !has_dilution) %>%
pull(Sample_Name)
if (length(bad_dups) > 0) {
message("Duplicated samples with NO dilution present (possible input error): ",
paste(bad_dups, collapse = ", "))
} else {
message("All duplicated samples have valid dilutions. No naming issues detected.")
}
} else {
message("No duplicated samples.")
}
df_all_cor <- df_all %>%
arrange(row_number()) %>%
mutate(
Dilution = case_when(
Dilution == "0" ~ 1.0,
TRUE ~ as.numeric(Dilution)),     #ensure it's numeric for math
Pair_ID = ifelse(                   # Assign a unique ID for pairing
grepl("Duplicate", Sample_Name, ignore.case = TRUE),
lag(Sample_Name), Sample_Name),  # Preserve original sample name for QA
Original_Name = Sample_Name) %>%
group_by(Pair_ID, Test) %>%          #groups by the name and test
filter(
Dilution == ifelse(any(Dilution > 1, na.rm = TRUE), max(Dilution, na.rm = TRUE), 1)
) %>%                                #keeps the higher dilution value
ungroup()
#repeated because of reruns
#replacements <- c("GCW_202305_SW_A", "GCW_202305_SW_B", "GCW_202305_SW_C", "GCW_202305_TR_LysA_20cm", "#GCW_202305_TR_LysA_45cm", "GCW_202305_TR_LysB_10cm", "GCW_202305_TR_LysB_20cm", "GCW_202305_UP_LysA_20cm", "GCW_202305_UP_LysB_10cm", "GCW_202305_UP_LysB_20cm", "GCW_202305_WC_LysA_10cm", "GCW_202305_WC_LysA_20cm", "GCW_202305_WC_LysA_45cm", "GCW_202305_WC_LysB_10cm", "GCW_202305_WC_LysB_20cm", "GCW_202305_WC_LysB_45cm", "GCW_202305_WC_LysC_10cm", "GCW_202305_WC_LysC_20cm", "GCW_202305_WC_LysC_45cm")
#df_replacements_latest <- df_all_cor %>%
# filter(Sample_Name %in% replacements) %>%
#group_by(Sample_Name) %>%
#slice_max(order_by = Run_Time, n = 1, with_ties = FALSE) %>%
#ungroup()
# Everything else stays untouched
#df_not_replacements <- df_all_cor %>%
# filter(!Sample_Name %in% replacements)
# Combine
#df_all_cor <- bind_rows(df_not_replacements, df_replacements_latest)
NOx_peChk <- df_all_cor %>%
filter(Test == "Vanadium NOx",
str_detect(Sample_Name, "peChk_NOx")) %>%
summarise(
Mean_Chk_Conc = mean(Conc, na.rm = TRUE),
SD_Chk_Conc = sd(Conc, na.rm = TRUE),
CV = (SD_Chk_Conc / Mean_Chk_Conc) * 100,
pct_diff_Chk = abs(Mean_Chk_Conc - NOx_pe) / NOx_pe,
NOx_pe_flag = ifelse(abs(pct_diff_Chk) < 10, "YES, Pass", "NO, rerun"))
NH3_peChk <- df_all_cor %>%
filter(Test == "Ammonia 2",
str_detect(Sample_Name, "peChk_NH3")) %>%
summarise(
Mean_Chk_Conc = mean(Conc, na.rm = TRUE),
SD_Chk_Conc = sd(Conc, na.rm = TRUE),
CV = (SD_Chk_Conc / Mean_Chk_Conc) * 100,
pct_diff_Chk = abs(Mean_Chk_Conc - NH3_pe) / NH3_pe,
NH3_pe_flag = ifelse(abs(pct_diff_Chk) < 10, "YES, Pass", "NO, rerun"))
PO4_peChk <- df_all_cor %>%
filter(Test == "o-PHOS 0.3",
str_detect(Sample_Name, "peChk_NH3")) %>%
summarise(
Mean_Chk_Conc = mean(Conc, na.rm = TRUE),
SD_Chk_Conc = sd(Conc, na.rm = TRUE),
CV = (SD_Chk_Conc / Mean_Chk_Conc) * 100,
pct_diff_Chk = abs(Mean_Chk_Conc - PO4_pe) / PO4_pe,
PO4_pe_flag = ifelse(abs(pct_diff_Chk) < 10, "YES, Pass", "NO, rerun"))
#flag write out
ifelse(NOx_peChk$pct_diff_Chk >= chk_flag,
"NOx pe Check has a % Difference >10% - REASSESS",
"NOx pe Check has a % Difference <10% - PROCEED")
cat("Run mean =", NOx_peChk$Mean_Chk_Conc, "\n")
cat("Expected  =", NOx_pe, "\n\n")
ifelse(NH3_peChk$pct_diff_Chk >= chk_flag,
"NH3 pe Check has a % Difference >10% - REASSESS",
"NH3 pe Check has a % Difference <10% - PROCEED")
cat("Run mean =", NH3_peChk$Mean_Chk_Conc, "\n")
cat("Expected  =", NH3_pe, "\n\n")
ifelse(PO4_peChk$pct_diff_Chk >= chk_flag,
"PO4 pe Check has a % Difference >10% - REASSESS",
"PO4 pe Check has a % Difference <10% - PROCEED")
cat("Run mean =", PO4_peChk$Mean_Chk_Conc, "\n")
cat("Expected  =", PO4_pe, "\n")
cat("Assess Reduction Efficiency")
#check on the reduction efficiency for the NOx test:
#Pull out test stds
red_eff <- df_all %>%
filter(Sample_Name %in% c("Nitrate Standard", "Nitrite Standard"))
red_eff$Eff_Percent <- (red_eff$Conc / 0.5)*100
red_eff$Eff_flag <-  ifelse(red_eff$Eff_Percent >= 90, 'YES', 'NO, rerun')
red_eff <- red_eff %>%
mutate(row_num = row_number())
#Plot these efficencies
red_effs_plot <-  ggplot(data = red_eff, aes(x = factor(row_num), y = Conc, fill=Eff_flag)) +
geom_bar(stat = 'identity') +
facet_wrap(~Sample_Name) +
scale_fill_manual(values = c("YES" = "midnightblue", "NO, rerun" = "darkred")) +
theme_classic() + labs(x= " ", y="NOx (mg/L)", title=" ") +
theme(legend.position="bottom") +
geom_hline(yintercept= 0.5, linetype="dashed",
color = "black", linewidth=1)  +
guides(fill=guide_legend(title="Reduction Efficiency >90%"))+
scale_x_discrete(drop = TRUE)
print(red_effs_plot)
#create a flag
NOx_mean_red_eff <- mean(red_eff$Eff_Percent)
#report out if flags indicate need for rerun and what the value is
ifelse(NOx_mean_red_eff >= 95,
"Mean NOx Reduction Efficiency >95% - PROCEED",
"Mean NOx Reduction Efficiency <95% - REASSESS")
print(NOx_mean_red_eff)
#write out a flag to the sample dataframe if red eff is low
if (NOx_mean_red_eff <= 95) {
df_all$NOx_flag <- ifelse(
df_all$NOx_flag != "",
paste0(df_all$NOx_flag, "; NOx reduction efficiency low"),
"NOx reduction efficiency low")}
View(NH3_peChk)
View(df_all)
