---
title: "Synoptic CB: Porewater SO4/Cl"
author: "July 2025 Samples"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true

output_dir: "To Be Reviewed/PDF"
---

\newpage

##Notes
#Alia needs to add a way to look at previous slopes
#We currently need to remove bad reps manually (check cv). Alia is working on automating this. 

#Files Required: 
#csv of samples and standard absorbance (1st box), IDs (second box), and dilutions (third box): see template

##Add Required Packages
```{r packages, include=FALSE}

#Packages that are required 
lapply(c(
  "dplyr", "ggplot2", "ggpubr", "stringr",
  "purrr", "tidyverse", "here", "broom", "tibble",
  "googledrive", "googlesheets4", "data.table", 
  "matrixStats", "gridExtra", "grid", "tidyverse", "knitr", 
  "plater", "raster", "readxl"), 
  library, character.only = TRUE)

```

## Run Information
```{r Run Information, echo=TRUE, message=FALSE, warning=FALSE}

###### Run information - PLEASE CHANGE
  Date_Run = "2025-05-29"  #Date that instrument was run
  Run_by = "Melanie Giessner"  #Instrument user 
  Script_run_by = "Zoe Read" #Code user 
  Project = "COMPASS"
  run_notes = "
  " #any notes from the run
  samples <- c("GCW", "GWI", "MSM", "SWH") #whatever identifies your samples within the same names 
  samples_pattern <- paste(samples, collapse = "|") 
    #samples_pattern <- "GCW" #use this instead of the line above if you have only one site code 

###### File Names - PLEASE CHANGE 
#file path and name for raw summary data file 
  raw_file_name_id_dil = "Raw Data/2025May_H2S_Datasheets.xlsx"
  Sheet = "Plate 1"
  raw_file_name_data = "Raw Data/20250529_COMPASS_H2S_Plate1.xlsx"

#file path and name of processed data file 
  tidy_file_name = "Tidy Data/20250529_COMPASS_H2S_Plate1_tidy.csv" 
  processed_file_name = "Processed Data/COMPASS_SynopticCB_PW_Processed_H2S_Plate1.csv" 

###### Log Files - PLEASE CHECK 
#downloaded metadata csv - downloaded from Google drive as csv for this year
  Raw_Metadata = "Raw Data/COMPASS_SynopticCB_PW_SampleLog_2025.csv"
  Raw_IDs = "Raw Data/20250529_COMPASS_H2S_IDs.xlsx"

#qaqc log file path for this year 
  Log_path = "Raw Data/Sulfide_STD_QAQC.csv"

```

#Set Up Code - constants and QAQC cutoffs
```{r Constants and QAQC cutoffs, include=FALSE}

# #Link to the protocol used for analysis 
#   #steph will add this soon 
# 
# #Coefficients / constants that are needed for calculations 
#   cl_mw <- 35.45     #molecular weight of Chloride, g/mol
#   s_mw <- 32.06      #molecular weight of sulfur, g/mol
#   Con1 <- 1000000    #conversion factor value for spike volumes (uL -> L)
# 
  #Flag cutoffs
  r2_cutoff = 0.98            #this is the level below which we want to rerun or consider a curve 
#   chk_flag_std_s = 10         #this is the maximum cv allowed for sulfate check standards
#   chk_flag_std_cl = 5         #this is the maximum cv allowed for chloride check standards
#   chk_flag_std_perc = 15      #this is the maximum perc diff allowed for check standards
#   chk_flag_dups = 10          #this is the maximum cv allowed for duplicates
#   high_recovery_cutoff = 120  #this is the maximum percent recovery of SO4 allowed in spiked samples
#   low_recovery_cutoff = 80    #this is the minimum percent recovery of SO4 allowed in spiked samples
#   chks_flag = 80              #if less than this percent of samples pass a check, a flag is added
# 
#Standard concentrations - Update if running different standard curve:
  standards <- tibble(
    IDs = c("Std 0", "Std 1", "Std 2", "Std 3", "Std 4", "Std 5"),
    Conc = c(0, 5.0, 12.5, 25.0, 50.0, 100.0))  #uM

#
# 
# #Spike concentration calc 
#   #spike for these samples was 10uL of the 250 Âµg/mL standard
#   spk_std <- (250/s_mw)         # mM of SO4 calculated from 250 ug/mL SO4 spike solution
#   spkvol <- 10                  # uL volume of spike added
#   spkvol <- spkvol/Con1         # L volume of spike added
#   spk_Conc <- (spk_std)*spkvol  # mmoles of SO4 added to each spiked sample
#    
#Top standard Concentrations- Update if running different standard curve: 
   Top_STD = 100

#Set time zone 
  common_tz = "Etc/GMT+5"
  Sys.setenv(TZ = "America/New_York")


```


## Tidy data
```{r Tidy data, include=FALSE}

# Read in file with IDs and dilutions
raw_id_dil <- read_excel(raw_file_name_id_dil, sheet = Sheet, col_names = FALSE, range = cell_rows(1:19))

# Read in file with data values
raw_data <- read_excel(raw_file_name_data, col_names = FALSE, range = "A1:M10")

# Combine IDs/dilutions with data
raw_tidy <- rbind(raw_data, raw_id_dil)

# Write tidy data to new folder
write.table(raw_tidy, tidy_file_name, sep = ",", col.names = FALSE, row.names = FALSE, na = "", quote = FALSE)


```



## Read in data
```{r Read in tidy data, include=FALSE}

# Read tidy plate file
dat <- stds <- read_plate(
  file = tidy_file_name,             # full path to the .csv file
  well_ids_column = "Wells",    # name to give column of well IDs (optional)
  sep = ","                     # separator used in the csv file (optional)
)
str(dat)
head(dat)

#Change the headers
colnames(dat) <- c("Wells", "Abs", "IDs", "Dilution")

#subset by H2s Stds
H2S_stds_all <- dat %>%  
  filter(str_detect(dat$IDs, "Std"))  
head(H2S_stds_all)

# Make a concentration column and add H2S std concentrations to std dataframe
H2S_stds_all <- left_join(H2S_stds_all, standards, by = "IDs")

#Remove check standards which have NAs in the Conc column
H2S_stds <- na.omit(H2S_stds_all)

```

## Plot standards 

#TO DO for the Standard Curve:
  1. Need to calculate CVs for each of the standards
  2. If CV is high (>10) figure out an automated way to remove one point and find the lowest CV
  3. Export the slope, yint, R2, etc into a QAQC file
```{r}

####manually remove high cv std replicates####
#hash for first run
#H2S_stds <- subset(H2S_stds, !(Wells %in% c("D01","D02" ,"D03","E01")))
#H2S_stds <- subset(H2S_stds, !(IDs %in% c("Std 3")))

#Plot stds and calculate the slope, intercept, and R2 
H2S <- ggplot(H2S_stds, aes(Conc, Abs)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
H2S

H2S_lm <- lm(H2S_stds$Abs ~ H2S_stds$Conc)
summary(H2S_lm)
cf <- coef(H2S_lm)

#Create data frame with 1 rows and 0 columns
Slopes <- data.frame(matrix(ncol = 0, nrow = 1))
Slopes$Curve <- "H2S"
Slopes$R2 <- summary(H2S_lm)$adj.r.squared
Slopes$Slope <- cf[2]
Slopes$Intercept <- cf[1]
head(Slopes)

#Calculate cv for STDs
H2S_stds1 <- H2S_stds %>%
  group_by(IDs) %>%
  summarise(H2S_mean_Abs = mean(Abs), H2S_sd = sd(Abs), H2S_cv = cv(Abs), 
            Dilution = first(Dilution))

head(H2S_stds1)

#Flag high cvs
H2S_stds1$H2S_cv_flag <- ifelse(H2S_stds1$H2S_cv > 10, 'High CV', 'within range')
head(H2S_stds1)


#Was the standard curve ok for this plate? Do we need to rerun the plate? Continue means plate is ok. Rerun plate means the R2 is too low.
if(Slopes$R2 > r2_cutoff){
print("Continue")
} else {
print("Rerun plate")
}

```

# Check STD Data against QAQC file
```{r}
#read in datafile with all the slopes
qlogO <- read.csv(Log_path)
colnames(qlogO) <- c("Row_Number", "Date", "Project", "R2", "Slope", "Intercept", "Top_STD")
qlogO$Row_Number <- NULL
head(qlogO)

#create data frame 
Date <- Date_Run
R2 <-summary(H2S_lm)$adj.r.squared
Slope <- cf[2]
Intercept <- cf[1]
qplate <- data.frame(Date, Project , R2, Slope, Intercept, Top_STD, row.names = NULL)
head(qplate)

#add data to file
qlog <- rbind(qplate, qlogO)
head(qlog)

# pull date run
qlog_Date <- subset(qlog, Date == Date_Run) 

##plot the slopes to make sure there are no crazy outliers 
slope1 <- ggplot(data=qlog, aes(x=Date, y=Slope)) +
           geom_hline(yintercept= (mean(qlog$Slope)+ (2*sd(qlog$Slope))), linetype="dashed", color = "red", size=2)+
            geom_hline(yintercept= (mean(qlog$Slope)- (2*sd(qlog$Slope))), linetype="dashed", color = "red", size=2)+
            geom_point(aes(size=3)) + 
            theme_classic() + ylim(0, 0.02) + 
           theme(legend.position="none") + 
           ggtitle("Sulfide Slopes")+
  guides(x = guide_axis(angle = 70))
  
slope1


if (qlog_Date$Slope[1] > (mean(qlog$Slope)+ (2*sd(qlog$Slope)))| qlog_Date$Slope[1] < (mean(qlog$Slope)- (2*sd(qlog$Slope)))) {
print("rerun Plate")
} else {
print("continue")
}

#Rerun if outside of red lines

##plot the intercepts to make sure there are no crazy outliers 
int1 <- ggplot(data=qlog, aes(x=Date, y=Intercept)) +
  geom_hline(yintercept= (mean(qlog$Intercept)+ (2*sd(qlog$Intercept))), linetype="dashed", color = "red", size=2)+
  geom_hline(yintercept= (mean(qlog$Intercept)- (2*sd(qlog$Intercept))), linetype="dashed", color = "red", size=2)+
  geom_point(aes(size=3)) + 
  theme_classic() + ylim(0.05,0.125) + 
  theme(legend.position="none")+ 
  ggtitle("Sulfide Intercepts")+
  guides(x = guide_axis(angle = 70))

int1

if (qlog_Date$Intercept[1] > (mean(qlog$Intercept)+ (2*sd(qlog$Intercept)))| qlog_Date$Intercept[1] < (mean(qlog$Intercept)- (2*sd(qlog$Intercept)))) {
print("rerun Plate")
} else {
print("continue")
}

#plot the R2s to make sure there are no crazy outliers 
Rsq1 <- ggplot(data=qlog, aes(x=Date, y=R2)) +
  geom_hline(yintercept= (0.98), linetype="dashed", color = "red", size=2)+
  geom_point(aes(size=3)) + 
  theme_classic() + ylim(0.96, 1.01) + 
  theme(legend.position="none")+ 
  ggtitle("Sulfide R2s")+
  guides(x = guide_axis(angle = 70))

Rsq1
if (qlog_Date$R2[1] < (0.985)) {
print("rerun Plate")
} else {
print("continue")
}

```

## Check standards QAQC
```{r}
#Make sure check standards are not different from standard concentration
#subset Check Standards from the rest of the dataset
H2S_std_Chk <- H2S_stds_all %>%  filter(!IDs =='Std 0') %>%  filter(!IDs =='Std 1') %>%  filter(!IDs =='Std 2') %>%  filter(!IDs =='Std 3') %>%  filter(!IDs =='Std 4') %>%  filter(!IDs =='Std 5')
head(H2S_std_Chk)

#Calculate Check standard Concentration
H2S_std_Chk$Conc <- (H2S_std_Chk$Abs-cf[1])/cf[2]

#Are the check standards significantly different from the standards?
#subset datasets for comparison
std0 <- subset(H2S_stds, IDs == "Std 0")
Chkstd0 <- subset(H2S_std_Chk, IDs == "ChkStd 0")
std3 <- subset(H2S_stds, IDs == "Std 3")
Chkstd3 <- subset(H2S_std_Chk, IDs == "ChkStd 3")
std4 <- subset(H2S_stds, IDs == "Std 4")
Chkstd4 <- subset(H2S_std_Chk, IDs == "ChkStd 4")

t.test.std0 <- t.test(std0$Abs,Chkstd0$Abs,var.equal = T)
t.test.std0

if(t.test.std0$p.value > 0.05){
print("Continue")
} else {
print("Rerun plate")
}

#t.test.std3 <- t.test(std3$Abs,Chkstd3$Abs,var.equal = T)
#t.test.std3

#if(t.test.std3$p.value > 0.05){
#print("Continue")
#} else {
#print("Rerun plate")
#}

t.test.std4 <- t.test(std4$Abs,Chkstd4$Abs,var.equal = T)
t.test.std4

if(t.test.std4$p.value > 0.05){
print("Continue")
} else {
print("Rerun plate")
}
```

## Matrix Check QAQC
#Uncomment MC20 if you used the 20 ppt matrix check
```{r}
std5 <- subset(H2S_stds, IDs == "Std 5")
MC10 <- subset(dat, IDs == "MC: 10ppt S5")
MC20 <- subset(dat, IDs == "MC: 20ppt S5")

#10ppt matrix check
t.test.MC10 <- t.test(std5$Abs,MC10$Abs,var.equal = T)
t.test.MC10

if(t.test.MC10$p.value > 0.05){
print("Continue")
} else {
print("Rerun plate")
}

#20ppt matrix check
 t.test.MC20 <- t.test(std5$Abs,MC20$Abs,var.equal = T)
 t.test.MC20
 
 if(t.test.MC20$p.value > 0.05){
 print("Continue")
 } else {
 print("Rerun plate")
 }


```


## Read in Sample Data subset 
```{r}

dat <- dat %>%  filter(!IDs =='Std 0') %>%  filter(!IDs =='Std 1') %>%  filter(!IDs =='Std 2') %>%  filter(!IDs =='Std 3') %>%  filter(!IDs =='Std 4') %>%  filter(!IDs =='Std 5') %>%  filter(!IDs =='ChkStd 0') %>%  filter(!IDs =='ChkStd 3') %>%  filter(!IDs =='ChkStd 4') %>%  filter(!IDs =='MC: 10ppt S5') %>%  filter(!IDs =='MC: 20ppt S5') 
head(dat)
Std5_ABS=mean(std5$Abs)
#Now flag any Abs that are above the 100uM standard absorbance (1.55)
dat$H2S_Dilute <- ifelse(dat$Abs > Std5_ABS , "Dilute", "Ok")
head(dat)

```


## Subset Data and Calculate Concentrations 
```{r}
#Calculate concentrations of Sulfide
dat$Conc <- ((dat$Abs-cf[1])/cf[2])*(dat$Dilution)
head(dat)

#Use ifelse to make any negative values equal to zero
dat$H2S_Conc_Final <- ifelse(dat$Conc <0, 0, dat$Conc)

#Use ifelse to mark samples that were below detection limit and changed to zero
dat$H2S_info <- ifelse(dat$Conc <0, 'bdl', ifelse(dat$Abs > mean(std5$Abs), "adl", "Within_Range"))

head(dat)
```


## Calculate Averages across wells and std. dev.  
```{r}
head(dat)

#summarize by sampleID so that we can calculate the mean and std. dev. of the three wells 
dat1 <- dat %>%
  group_by(IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
            H2S_flag = first(H2S_info), 
            Dilution = first(Dilution))

head(dat1)

#Flag high cvs 
dat1$H2S_cv_flag <- ifelse(dat1$H2S_cv > 10, 'High CV', 'within range')
## to flag high dilutions 
#dat1$H2S_Dilute <- ifelse(dat1$H2S_mean/dat1$Dilution > ((Std5_ABS-cf[1])/cf[2])*(1) , "Dilute", "Ok")

head(dat1)


#plot data and sd's just to check and see what they look like - just a quick first look 
H2S <- ggplot(dat1, aes(x=IDs, y=H2S_mean))+ 
  geom_point(size=4) +  theme_classic() + 
  labs(y="Sulfide (uM)", x="Sample ID") + 
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))+
  guides(x = guide_axis(angle = 70))
H2S

#If CVs are high manually go in and remove the one rep that you think is causing the issue (see below). Alia will automate this eventually

```

## Remove bad reps
```{r}

#auto remove bad reps
# filter High CV Samples
dat1_HCV <- dat1 %>%  
  filter(str_detect(H2S_cv_flag, "High CV")) 
head(dat1_HCV)

dat_HCV <- subset(dat, (IDs %in% dat1_HCV$IDs ))
head(dat_HCV)

#Columns 4 7 10
Column1= c("A04", "A07","A10", "B04","B10" ,"C04" , "C07", "C10",
"D04","D10", "E04","E07", "E10" ,"F04" , "F07" ,"G04","G07" ,"G10" ,"H01","H04","H07")

#Columns 5,8,11
Column2= c("A05", "A08","A11", "B05","B11" ,"C05" , "C08", "C11",
"D05","D11", "E05","E08", "E11" ,"F05" , "F08" ,"G05","G08" ,"G11" ,"H02","H05","H08")

#Columns 6,9,12
Column3= c("A06", "A09","A12", "B06" ,"B12" ,"C06" , "C09", "C12",
"D06","D12", "E06","E09", "E12" ,"F06" , "F09" ,"G06","G09" ,"G12" ,"H03","H06","H09")


#delete Column one
dat_HCV1 <- subset(dat_HCV, !(Wells %in% Column1 ))
#delete Column two
dat_HCV2 <- subset(dat_HCV, !(Wells %in% Column2 ))
#delete Column three
dat_HCV3 <- subset(dat_HCV, !(Wells %in% Column3 ))

##Find CV for each Data set
#W/out column1
dat_HCV1 <- dat_HCV1 %>%
  group_by(IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
            H2S_flag = first(H2S_info), 
            Dilution = first(Dilution))
dat_HCV1$H2S_cv_flag <- ifelse(dat_HCV1$H2S_cv > 10, 'High CV', 'within range')
#W/out column2
dat_HCV2 <- dat_HCV2 %>%
  group_by(IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
            H2S_flag = first(H2S_info), 
            Dilution = first(Dilution))
dat_HCV2$H2S_cv_flag <- ifelse(dat_HCV2$H2S_cv > 10, 'High CV', 'within range')
#W/out column3
dat_HCV3 <- dat_HCV3 %>%
  group_by(IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
            H2S_flag = first(H2S_info), 
            Dilution = first(Dilution))
dat_HCV3$H2S_cv_flag <- ifelse(dat_HCV3$H2S_cv > 10, 'High CV', 'within range')
#find lowest CVs

dat_HCV1_1 <- subset(dat_HCV1, dat_HCV1$H2S_cv < dat_HCV2$H2S_cv & dat_HCV1$H2S_cv < dat_HCV3$H2S_cv)
head(dat_HCV1_1)

dat_HCV2_2 <- subset(dat_HCV2, dat_HCV2$H2S_cv < dat_HCV1$H2S_cv & dat_HCV2$H2S_cv < dat_HCV3$H2S_cv)
head(dat_HCV1_1)

dat_HCV3_3 <- subset(dat_HCV3, dat_HCV3$H2S_cv < dat_HCV2$H2S_cv & dat_HCV3$H2S_cv < dat_HCV1$H2S_cv)
head(dat_HCV3_3)


#recombine data frames
dat2_HCV <- rbind(dat_HCV1_1,dat_HCV2_2,dat_HCV3_3)
head(dat2_HCV)

dat1 <- subset(dat1, (!IDs %in% dat2_HCV$IDs ))
dat1 <- rbind(dat2_HCV, dat1)
head(dat1)

#dat1$H2S_mean <- ifelse(dat1$H2S_cv_flag == 'High CV',  dat2_HCV$H2S_mean, dat1$H2S_mean)
#dat1$H2S_H2S_sd <- ifelse(dat1$H2S_cv_flag == 'High CV',  dat2_HCV$H2S_H2S_sd, dat1$H2S_H2S_sd)
#dat1$H2S_H2S_cv <- ifelse(dat1$H2S_cv_flag == 'High CV',  dat2_HCV$H2S_H2S_cv, dat1$H2S_H2S_cv)
#dat1$H2S_H2S_cv_flag <- ifelse(dat1$H2S_cv_flag == 'High CV',  dat2_HCV$H2S_H2S_cv_flag, dat1$H2S_H2S_cv_flag)



###Manually Remove bad reps by row number in original dataframe
#dat <- dat[-c(10,13,16,21,28,31,37,40,48,52,60),]
#dat <- subset(dat, !(Wells %in% c("B04," )))


#rerun lines 231-253
#head(dat)

#summarize by sampleID so that we can calculate the mean and std. dev. of the three wells 
#dat1 <- dat %>%
#  group_by(IDs) %>%
 # summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
           # H2S_flag = first(H2S_info), 
           # Dilution = first(Dilution))

#head(dat1)

#Flag high cvs
#dat1$H2S_cv_flag <- ifelse(dat1$H2S_cv > 10, 'High CV rerun', 'within range')

#head(dat1)

#plot data and sd's just to check and see what they look like - just a quick first look 
H2S <- ggplot(dat1, aes(x=IDs, y=H2S_mean))+ 
  geom_point(size=4) +  theme_classic() + 
  labs(y="Sulfide (uM)", x="Sample ID") + 
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))+
  guides(x = guide_axis(angle = 70))
H2S

#If samples with cv >10 rerun those samples 
H2S_HighCVRerun <- subset(dat1, H2S_cv_flag == "High CV")
H2S_DiluteRerun <- subset(dat1, H2S_flag == "adl")
H2S_bdl <- subset(dat1,H2S_flag == "bdl")
print(H2S_HighCVRerun)
print(H2S_DiluteRerun)
print(H2S_bdl)

```



## Check the dups for QAQC 
```{r}

#Show me the data that we have from the calculations 
head(dat1)

#pull out any rows that have "Dup" in the ID column
dups <- dat1 %>%  
  filter(str_detect(IDs, "Dup")) 
head(dups)

#remove these from dat1
dat2 <- dat1 %>%  
  filter(!str_detect(IDs, "Dup")) %>%  
  filter(!str_detect(IDs, "Spike")) 
head(dat2)

#remove the dup from these IDs so we will have duplicates 
dups$IDs<-gsub(" Dup",'',dups$IDs)
head(dups)
colnames(dups) <- c('IDs', 'mean_dup')


#put it back together with the old data set and look for duplicates 
QAdups <- merge(dat2, dups)
head(QAdups)

QAdups$dups_chk <- ((abs(QAdups$H2S_mean-QAdups$mean_dup))/((QAdups$H2S_mean+QAdups$mean_dup)/2))*100
QAdups$dups_flag <-  ifelse(QAdups$dups_chk <15.5, 'OK', 'Rerun')

head(QAdups)

#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this 
dupsbar <- ggplot(data = QAdups, aes(x = IDs, y = dups_chk, fill=dups_chk)) +
       geom_bar(stat = 'identity') + 
        scale_fill_gradient2(low='darkgreen', mid='darkgreen', high='darkgrey', space='Lab') + 
        theme_classic() + labs(x= "Sample ID", y="Difference Between Duplicates (%)") + 
        theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", 
                color = "black", size=1)

dupsbar

Baddups <- subset(QAdups, dups_flag == "Rerun")
#check for any one's that would warrant reruns 
```


## Check the spks for QAQC 
```{r}

#Show me the data that we have from the calculations 
head(dat1)

#pull out any rows that have "d" in the SampleID column
spks <- dat1 %>%  
  filter(str_detect(IDs, "Spike")) 
head(spks)

#remove these from dat1
dat2 <- dat1 %>%  
  filter(!str_detect(IDs, "Dup")) %>%  
  filter(!str_detect(IDs, "Spike")) 
head(dat2)

#remove the Spike from these IDs so we will have duplicates 
spks$IDs<-gsub(" Spike","",spks$IDs)
head(spks)
colnames(spks) <- c('IDs', 'mean_spk')


#put it back together with the old data set and look for duplicates 
QAspks <- merge(dat2, spks)
head(QAspks)

#now we need to calculate the spike concentration and calculate the spike recovery 
#spike for these samples was 50 uL of the 100uM standard
QAspks$spk_Conc <- (100*(50/1000000))                        # this would be in umoles of S2- in the spk 
QAspks$unspkd <- (QAspks$H2S_mean/QAspks$Dilution)*(250/1000000) #gives us the total S2- in the sample in umoles
QAspks$spkd <-    (QAspks$mean_spk/QAspks$Dilution)*((250+50)/1000000)        ##total fe in spiked sample in umoles 
QAspks$expctd_spkd <-  (QAspks$unspkd + QAspks$spk_Conc)
QAspks$spk_recovery <-    (QAspks$spkd/QAspks$expctd_spkd)*100
QAspks$spks_flag <-  ifelse(QAspks$spk_recovery >80 & QAspks$spk_recovery <120, 'OK', 'NO, rerun')   

head(QAspks)

#plot spk recoveries output as a bar graph to easily check - want any over 10% to be red need to work on this 
spksbar <- ggplot(data = QAspks, aes(x = IDs, y = spk_recovery)) +
        geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("gray48")) + 
        theme_classic() + labs(x= "Sample ID", y="Spike Recovery (%)") + 
        theme(legend.position="none") +  
        geom_hline(yintercept=80, linetype="dashed", color = "black", size=1) + 
        geom_hline(yintercept=120, linetype="dashed", color = "black", size=1)

spksbar

Badspks <- subset(QAspks, spks_flag == "NO, rerun")
#check for any no's that would warrant reruns! 
```



## Export full data then just final data 
```{r}
dat_IDs <- dat1

#Read out the summarized data
head(dat_IDs)
  write.csv(dat_IDs, file="S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd Data/COMPASS_H2S_20250529_Plate4_Summary_Data.csv")

#Read out all the data in dat 
head(dat)
write.csv(dat, file="S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd Data/COMPASS_H2S_20250529_Plate4_Data.csv")

#Now take out the absorbance and stuff 
head(dat)
dat3 <- dat[ ,-(1:2)]
dat3 <- dat3[ ,-(2:5)]
head(dat3)
write.csv(dat3, file="S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd data/COMPASS_H2S_20250529_Plate4_short_Data.csv")

# write flagged data to file
#write.csv(H2S_DiluteRerun, "S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd Data/COMPASS_H2S_20250529_Plate4_SamplesNeedDilution.csv")
#write.csv(H2S_HighCVRerun, "S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd Data/COMPASS_H2S_20250529_Plate4_SamplesNeedRerun.csv")
write.csv(Baddups, "S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd Data/COMPASS_H2S_20250529_Plate4_baddups.csv")
write.csv(Badspks, "S:/GlobalChangeEco/Porewater_R scripts & Data/Data/Sulfide Microplate/COMPASS/2025 May/QAQC'd Data/COMPASS_H2S_20250529_Plate4_badspks.csv")

#save to QAQC Data to QAQC File
write.csv(qlog, file="S:/GlobalChangeEco/porewater_r scripts & Data/BGC_GCE_PorewaterAnalysis/Sulfide/Sulfide_STD_QAQC.csv")




```

### END





