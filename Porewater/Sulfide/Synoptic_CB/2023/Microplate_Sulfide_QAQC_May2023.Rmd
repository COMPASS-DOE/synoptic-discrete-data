---
title: "Microplate_Sulfide_Code"
author: "Alia Al-Haj & Stephanie J. Wilson"
date: "`r Sys.Date()`"
output: html_document
---

## Information
```{r}
#######################################################################################
####### COMPASS FME
####### Synoptic: Chesapeake Bay
####### Data Analysis Code: Porewater sulfide 
####### MONTH: May 2023
##############################################################################################


############################# Information ##################################
#Author: Alia N. Al-Haj & Stephanie J. Wilson
#Edited: 20230906

#Samples taken from porewater lysimeters
#Samples were filtered with 0.45 uM, kept on ice and then in fridge until analysis
#Field Protocol: 
#Samples Analyzed using compass sulfide microplate method (Cline's method)
#Lab Protocol: 
```

## Code Set up 
```{r}
Date_Run = "5/24/23"
Run_by = "Stephanie J. Wilson"

#packages:
library(ggplot2)
library(dplyr)
library(data.table)
library(matrixStats)
library(gridExtra)
library(ggpubr)
library(grid)
library(stringr)
library(plater)
library(raster)


```

##Notes

#Samples are run in triplicate so that if there is pipetting error (indicated by high cv) 
#one samples can be removed from analysis
#Any reps to be removed must be removed manually by selecting the row number  

#Files Required: 
#csv of samples and standard absorbance (1st box), IDs (second box), and dilutions (third box): see template

## Read in data
```{r}
#set working directory 
setwd("S:/Biogeochemistry/People/Wilson (Steph)/Data/MicroPlate Data/Sulfide/Raw Files/2023")

# Read in raw data csv
dat <- stds <- read_plate(
  file = "COMPASS_Synoptic_CB_H2S_May2023_Plate1.csv",             # full path to the .csv file
  well_ids_column = "Wells",    # name to give column of well IDs (optional)
  sep = ","                     # separator used in the csv file (optional)
)
str(dat)
head(dat)

#Change the headers
colnames(dat) <- c("Wells", "Abs", "IDs", "Dilution")

#subset by H2s Stds
H2S_stds_all <- dat %>%  
  filter(str_detect(dat$IDs, "Std"))  
head(H2S_stds_all)

#Add H2S std concentrations to std dataframe
#Std 0 = 0 uM
#Std 1 = 5.0 uM
#Std 2 = 12.5 uM
#Std 3 = 25.0 uM
#Std 4 = 50.0 uM
#Std 5 = 100.0 uM
#make a concentration column
H2S_stds_all$Conc = NA
H2S_stds_all$Conc[H2S_stds_all$IDs == 'Std 0'] <- 0
H2S_stds_all$Conc[H2S_stds_all$IDs == 'Std 1'] <- 5.0
H2S_stds_all$Conc[H2S_stds_all$IDs == 'Std 2'] <- 12.5
H2S_stds_all$Conc[H2S_stds_all$IDs == 'Std 3'] <- 25.0
H2S_stds_all$Conc[H2S_stds_all$IDs == 'Std 4'] <- 50.0
H2S_stds_all$Conc[H2S_stds_all$IDs == 'Std 5'] <- 100.0

#Remove check standards which have NAs in the Conc column
H2S_stds <- na.omit(H2S_stds_all)
```

## Plot standards 
```{r}
#Plot stds and calculate the slope, intercept, and R2 
H2S <- ggplot(H2S_stds, aes(Conc, Abs)) + geom_point() + geom_smooth(method = "lm", se = FALSE)
H2S

H2S_lm <- lm(H2S_stds$Abs ~ H2S_stds$Conc)
summary(H2S_lm)
cf <- coef(H2S_lm)

#create data frame with 1 rows and 0 columns
Slopes <- data.frame(matrix(ncol = 0, nrow = 1))
Slopes$Curve <- "H2S"
Slopes$R2 <- summary(H2S_lm)$adj.r.squared
Slopes$Slope <- cf[2]
Slopes$Intercept <- cf[1]
head(Slopes)

#Was the standard curve ok for this plate? Do we need to rerun the plate? Continue means plate is ok. Rerun plate means the R2 is too low.
if(Slopes$R2 > 0.98){
print("Continue")
} else {
print("Rerun plate")
}
```

## Matrix Check QAQC
#Uncomment MC20 if you used the 20 ppt matrix check
```{r}
std5 <- subset(H2S_stds, IDs == "Std 5")
MC10 <- subset(dat, IDs == "MC: 10ppt")
MC20 <- subset(dat, IDs == "MC: 20ppt")

#10ppt matrix check
t.test.MC10 <- t.test(std5$Abs,MC10$Abs,var.equal = T)
t.test.MC10

if(t.test.MC10$p.value > 0.05){
print("Continue")
} else {
print("Rerun plate")
}

#20ppt matrix check
t.test.MC20 <- t.test(std5$Abs,MC20$Abs,var.equal = T)
t.test.MC20
 
if(t.test.MC20$p.value > 0.05){
print("Continue")
} else {
print("Rerun plate")
}

#choosing to accept this matrix check because p-val is 0.041 
```


## Read in Sample Data subset 
```{r}

dat <- dat %>%  filter(!IDs =='Std 0') %>%  filter(!IDs =='Std 1') %>%  filter(!IDs =='Std 2') %>%  filter(!IDs =='Std 3') %>%  filter(!IDs =='Std 4') %>%  filter(!IDs =='Std 5') %>%  filter(!IDs =='ChkStd 0') %>%  filter(!IDs =='ChkStd 3') %>%  filter(!IDs =='ChkStd 4') %>%  filter(!IDs =='MC: 10ppt') %>%  filter(!IDs =='MC: 20ppt') 
head(dat)

#Now flag any Abs that are above the 100uM standard absorbance (1.55)
dat$H2S_Dilute <- ifelse(dat$Abs >1.55, "Dilute", "Ok")
head(dat)
```


## Subset Data and Calculate Concentrations 
```{r}
#Calculate concentrations of Sulfide
dat$Conc <- ((dat$Abs-cf[1])/cf[2])*(dat$Dilution)
head(dat)

#Use ifelse to make any negative values equal to zero
dat$H2S_Conc_Final <- ifelse(dat$Conc <0, 0, dat$Conc)

#Use ifelse to mark samples that were below detection limit and changed to zero
dat$H2S_info <- ifelse(dat$Conc <0, 'bdl', 'within range')

head(dat)
```


## Calculate Averages across wells and std. dev.  
```{r}
head(dat)

#summarize by sampleID so that we can calculate the mean and std. dev. of the three wells 
dat1 <- dat %>%
  group_by(IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
            H2S_flag = first(H2S_info), 
            Dilution = first(Dilution))

head(dat1)

#Flag high cvs
dat1$H2S_cv_flag <- ifelse(dat1$H2S_cv > 10, 'High CV', 'within range')

head(dat1)

#plot data and sd's just to check and see what they look like - just a quick first look 
H2S <- ggplot(dat1, aes(x=IDs, y=H2S_mean))+ 
  geom_point(size=4) +  theme_classic() + 
  labs(y="Sulfide (uM)", x="Sample ID") + 
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))
H2S

#If CVs are high manually go in and remove the one rep that you think is causing the issue (see below). Alia will automate this eventually

```

## Manually remove bad reps
```{r}
#Remove bad reps by row number in original dataframe
dat <- dat[-c(7, 58, 68),]

#rerun lines 231-253
head(dat)

#summarize by sampleID so that we can calculate the mean and std. dev. of the three wells 
dat1 <- dat %>%
  group_by(IDs) %>%
  summarise(H2S_mean = mean(H2S_Conc_Final), H2S_sd = sd(H2S_Conc_Final), H2S_cv = cv(H2S_Conc_Final), 
            H2S_flag = first(H2S_info), 
            Dilution = first(Dilution))

head(dat1)

#Flag high cvs
dat1$H2S_cv_flag <- ifelse(dat1$H2S_cv > 10, 'High CV rerun', 'within range')

head(dat1)

#plot data and sd's just to check and see what they look like - just a quick first look 
H2S <- ggplot(dat1, aes(x=IDs, y=H2S_mean))+ 
  geom_point(size=4) +  theme_classic() + 
  labs(y="Sulfide (uM)", x="Sample ID") + 
  geom_errorbar(aes(ymin=H2S_mean-H2S_sd,
                    ymax=H2S_mean+H2S_sd),width=0.3,position=position_dodge(.1))
H2S

#If samples with cv >10 rerun those samples 
```



## Check the dups for QAQC 
```{r}

#Show me the data that we have from the calculations 
head(dat1)

#pull out any rows that have "Dup" in the ID column
dups <- dat1 %>%  
  filter(str_detect(IDs, "dup")) 
head(dups)

#remove these from dat1
dat2 <- dat1 %>%  
  filter(!str_detect(IDs, "dup")) %>%  
  filter(!str_detect(IDs, "spk")) 
head(dat2)

#remove the dup from these IDs so we will have duplicates 
dups$IDs<-gsub("_dup","",dups$IDs)
head(dups)
colnames(dups) <- c('IDs', 'mean_dup')


#put it back together with the old data set and look for duplicates 
QAdups <- merge(dat2, dups)
head(QAdups)

QAdups$dups_chk <- ((abs(QAdups$mean-QAdups$mean_dup))/((QAdups$mean+QAdups$mean_dup)/2))*100
QAdups$dups_flag <-  ifelse(QAdups$dups_chk <15.5, 'OK', 'Rerun')

head(QAdups)

#plot dups output as a bar graph to easily check - want any over 10% to be red need to work on this 
dupsbar <- ggplot(data = QAdups, aes(x = IDs, y = dups_chk, fill=dups_chk)) +
       geom_bar(stat = 'identity') + 
        scale_fill_gradient2(low='darkgreen', mid='darkgreen', high='darkgrey', space='Lab') + 
        theme_classic() + labs(x= "Sample ID", y="Difference Between Duplicates (%)") + 
        theme(legend.position="none") +  geom_hline(yintercept=10, linetype="dashed", 
                color = "black", size=1)

dupsbar

#check for any no's that would warrant reruns 

#dups on this run were both zero, so they were within range 
```


## Check the spks for QAQC 
```{r}

#Show me the data that we have from the calculations 
head(dat1)

#pull out any rows that have "d" in the SampleID column
spks <- dat1 %>%  
  filter(str_detect(IDs, "_spk")) 
head(spks)

#remove these from dat1
dat2 <- dat1 %>%  
  filter(!str_detect(IDs, "_dup")) %>%  
  filter(!str_detect(IDs, "_spk")) 
head(dat2)

#remove the Spike from these IDs so we will have duplicates 
spks$IDs<-gsub("_spk","",spks$IDs)
head(spks)
colnames(spks) <- c('IDs', 'mean_spk')


#put it back together with the old data set and look for duplicates 
QAspks <- merge(dat2, spks)
head(QAspks)

#now we need to calculate the spike concentration and calculate the spike recovery 
#spike for these samples was 10 uL of the 100uM standard
QAspks$spk_Conc <- (100*(10/1000000))                        # this would be in umoles of S2- in the spk 
QAspks$unspkd <- (QAspks$H2S_mean/QAspks$Dilution)*(250/1000000) #gives us the total S2- in the sample in umoles
QAspks$spkd <-    (QAspks$mean_spk/QAspks$Dilution)*((250+10)/1000000)        ##total fe in spiked sample in umoles 
QAspks$expctd_spkd <-  (QAspks$unspkd + QAspks$spk_Conc)
QAspks$spk_recovery <-    (QAspks$spkd/QAspks$expctd_spkd)*100
QAspks$spks_flag <-  ifelse(QAspks$spk_recovery >80, 'OK', 'NO, rerun')   

head(QAspks)

#plot spk recoveries output as a bar graph to easily check - want any over 10% to be red need to work on this 
spksbar <- ggplot(data = QAspks, aes(x = IDs, y = spk_recovery)) +
        geom_bar(stat = 'identity') + 
        scale_fill_manual(values=c("gray48")) + 
        theme_classic() + labs(x= "Sample ID", y="Spike Recovery (%)") + 
        theme(legend.position="none") +  
        geom_hline(yintercept=80, linetype="dashed", color = "black", size=1) + 
        geom_hline(yintercept=120, linetype="dashed", color = "black", size=1)

spksbar

#check for any no's that would warrant reruns! 
```



## Export full data then just final data 
```{r}
dat_IDs <- dat1

setwd("S:/Biogeochemistry/People/Wilson (Steph)/Data/MicroPlate Data/Sulfide/Processed Data Files")

#Read out the summarized data
colnames(dat_IDs) <- c('IDs', 'H2S_mean_uM', 'H2S_sd', 'H2S_cv', 'H2S_flag', 'Dilution', 'H2S_cv_flag')
head(dat_IDs)
write.csv(dat_IDs, file="COMPASS_Synoptic_H2S_202305_Plate1_Summary_Data.csv")

#Read out all the data in dat 
colnames(dat) <- c('Wells', 'Abs', 'IDs','Dilution', 'H2S_Dilute', 'Conc_uM', 'H2S_Conc_Final_uM', 'H2S_info') 
head(dat)
write.csv(dat, file="COMPASS_Synoptic_H2S_202305_Plate1_Data.csv")

#Now take out the absorbance and stuff 
head(dat)
dat3 <- dat[ ,-(1:2)]
dat3 <- dat3[ ,-(2:4)]
colnames(dat) <- c( 'IDs', 'H2S_Conc_Final_uM', 'H2S_info')
head(dat3)
write.csv(dat3, file="COMPASS_Synoptic_H2S_202305_Plate1_short_Data.csv")

```

### END


